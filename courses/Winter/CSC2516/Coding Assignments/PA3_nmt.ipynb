{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasudev-sharma/Brain-Tumor-Segmentation/blob/master/courses/Winter/CSC2516/Coding%20Assignments/PA3_nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjPTaRB4mpCd"
      },
      "source": [
        "# Colab FAQ\n",
        "\n",
        "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
        "\n",
        "You need to use the colab GPU for this assignment by selecting:\n",
        "\n",
        "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9IS9B9-yUU5"
      },
      "source": [
        "# Setup PyTorch\n",
        "\n",
        "All files will be stored at /content/csc421/a3/ folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-6MQhMOlHXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8526c4-cfeb-4d85-9d66-0f1fc52c3f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "/content/content/csc421/a3\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "# Setup python environment and change the current working directory\n",
        "######################################################################\n",
        "!pip install Pillow\n",
        "%mkdir -p ./content/csc421/a3/\n",
        "%cd ./content/csc421/a3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaTdRNuUra7"
      },
      "source": [
        "# Helper code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BIpGwANoQOg"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-UJHBYZkh7f"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "import pickle as pkl\n",
        "from pathlib import Path\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "import tarfile\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "\n",
        "def get_file(\n",
        "    fname, origin, untar=False, extract=False, archive_format=\"auto\", cache_dir=\"data\"\n",
        "):\n",
        "    datadir = os.path.join(cache_dir)\n",
        "    if not os.path.exists(datadir):\n",
        "        os.makedirs(datadir)\n",
        "\n",
        "    if untar:\n",
        "        untar_fpath = os.path.join(datadir, fname)\n",
        "        fpath = untar_fpath + \".tar.gz\"\n",
        "    else:\n",
        "        fpath = os.path.join(datadir, fname)\n",
        "\n",
        "    print(fpath)\n",
        "    if not os.path.exists(fpath):\n",
        "        print(\"Downloading data from\", origin)\n",
        "\n",
        "        error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
        "        try:\n",
        "            try:\n",
        "                urlretrieve(origin, fpath)\n",
        "            except URLError as e:\n",
        "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
        "            except HTTPError as e:\n",
        "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
        "        except (Exception, KeyboardInterrupt) as e:\n",
        "            if os.path.exists(fpath):\n",
        "                os.remove(fpath)\n",
        "            raise\n",
        "\n",
        "    if untar:\n",
        "        if not os.path.exists(untar_fpath):\n",
        "            print(\"Extracting file.\")\n",
        "            with tarfile.open(fpath) as archive:\n",
        "                archive.extractall(datadir)\n",
        "        return untar_fpath\n",
        "\n",
        "    if extract:\n",
        "        _extract_archive(fpath, datadir, archive_format)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "    Arguments:\n",
        "        tensor: A Tensor object.\n",
        "        cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "    Returns:\n",
        "        A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title(\"BS={}, nhid={}\".format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel(\"Epochs\", fontsize=16)\n",
        "    plt.ylabel(\"Loss\", fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, \"loss_plot.pdf\"))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_gru(l1, l2, o1, o2, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and val loss curves from GRU runs.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    ax[0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0].title.set_text(\"Train Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n",
        "    ax[1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n",
        "    ax[1].title.set_text(\"Val Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n",
        "\n",
        "    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "    ax[0].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "    ax[1].set_ylabel(\"Loss\", fontsize=10)\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"GRU Performance by Dataset\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.85)\n",
        "    plt.legend()\n",
        "\n",
        "    plt_path = \"./loss_plot_{}.pdf\".format(fn)\n",
        "    plt.savefig(plt_path)\n",
        "    print(f\"Plot saved to: {Path(plt_path).resolve()}\")\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by dataset while holding hidden size constant.\n",
        "\n",
        "    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0][0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0][0].title.set_text(\n",
        "        \"Train Loss | Model Hidden Size = {}\".format(o1.hidden_size)\n",
        "    )\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n",
        "    ax[0][1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n",
        "    ax[0][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o1.hidden_size))\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l3)), mean_l3, label=\"ds=\" + o3.data_file_name)\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"ds=\" + o4.data_file_name)\n",
        "    ax[1][0].title.set_text(\n",
        "        \"Train Loss | Model Hidden Size = {}\".format(o3.hidden_size)\n",
        "    )\n",
        "\n",
        "    ax[1][1].plot(range(len(l3[1])), l3[1], label=\"ds=\" + o3.data_file_name)\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"ds=\" + o4.data_file_name)\n",
        "    ax[1][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o4.hidden_size))\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
        "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
        "    runs in Part 3, comparing by hidden size while holding dataset constant.\n",
        "\n",
        "    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n",
        "\n",
        "    Arguments:\n",
        "        l1: Tuple of lists containing training / val losses for model 1.\n",
        "        l2: Tuple of lists containing training / val losses for model 2.\n",
        "        l3: Tuple of lists containing training / val losses for model 3.\n",
        "        l4: Tuple of lists containing training / val losses for model 4.\n",
        "        o1: Options for model 1.\n",
        "        o2: Options for model 2.\n",
        "        o3: Options for model 3.\n",
        "        o4: Options for model 4.\n",
        "        fn: Output file name.\n",
        "        s: Number of training iterations to average over.\n",
        "    \"\"\"\n",
        "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
        "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
        "    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n",
        "    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n",
        "\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"hid_size=\" + str(o1.hidden_size))\n",
        "    ax[0][0].plot(range(len(mean_l3)), mean_l3, label=\"hid_size=\" + str(o3.hidden_size))\n",
        "    ax[0][0].title.set_text(\"Train Loss | Dataset = \" + o1.data_file_name)\n",
        "\n",
        "    # Validation losses are assumed to be by epoch\n",
        "    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"hid_size=\" + str(o1.hidden_size))\n",
        "    ax[0][1].plot(range(len(l3[1])), l3[1], label=\"hid_size=\" + str(o3.hidden_size))\n",
        "    ax[0][1].title.set_text(\"Val Loss | Dataset = \" + o1.data_file_name)\n",
        "\n",
        "    ax[1][0].plot(range(len(mean_l2)), mean_l2, label=\"hid_size=\" + str(o2.hidden_size))\n",
        "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"hid_size=\" + str(o4.hidden_size))\n",
        "    ax[1][0].title.set_text(\"Train Loss | Dataset = \" + o3.data_file_name)\n",
        "\n",
        "    ax[1][1].plot(range(len(l2[1])), l2[1], label=\"hid_size=\" + str(o2.hidden_size))\n",
        "    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"hid_size=\" + str(o4.hidden_size))\n",
        "    ax[1][1].title.set_text(\"Val Loss | Dataset = \" + o4.data_file_name)\n",
        "\n",
        "    for i in range(2):\n",
        "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
        "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
        "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
        "        ax[i][0].legend(loc=\"upper right\")\n",
        "        ax[i][1].legend(loc=\"upper right\")\n",
        "\n",
        "    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(top=0.9)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, \"encoder.pt\"), \"wb\") as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, \"decoder.pt\"), \"wb\") as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, \"idx_dict.pkl\"), \"wb\") as f:\n",
        "        pkl.dump(idx_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbvpn4MaV0I1"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVT4TNTOV3Eg"
      },
      "outputs": [],
      "source": [
        "def read_lines(filename):\n",
        "    \"\"\"Read a file and split it into lines.\"\"\"\n",
        "    lines = open(filename).read().strip().lower().split(\"\\n\")\n",
        "    return lines\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\"\"\"\n",
        "    return all(c.isalpha() or c == \"-\" for c in s)\n",
        "\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data(file_name):\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\"\"\"\n",
        "    path = \"./data/{}.txt\".format(file_name)\n",
        "    source_lines, target_lines = read_pairs(path)\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(\"\".join(source_lines)) | set(\"\".join(target_lines))\n",
        "\n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = {\n",
        "        char: index for (index, char) in enumerate(sorted(list(all_characters)))\n",
        "    }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index[\"SOS\"] = start_token\n",
        "    char_to_index[\"EOS\"] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = {index: char for (char, index) in char_to_index.items()}\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = {\n",
        "        \"char_to_index\": char_to_index,\n",
        "        \"index_to_char\": index_to_char,\n",
        "        \"start_token\": start_token,\n",
        "        \"end_token\": end_token,\n",
        "    }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s, t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s, t))\n",
        "\n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRWfRdmVVjUl"
      },
      "source": [
        "## Training and evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa5-onJhoSeM"
      },
      "outputs": [],
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\"\"\"\n",
        "    return [char_to_index[char] for char in s] + [\n",
        "        end_token\n",
        "    ]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    return \" \".join(\n",
        "        [translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()]\n",
        "    )\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\"\"\"\n",
        "\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "    index_to_char = idx_dict[\"index_to_char\"]\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = \"\"\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(\n",
        "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
        "    )  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(\n",
        "            decoder_inputs, encoder_annotations, decoder_hidden\n",
        "        )\n",
        "\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1]  # latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [\n",
        "                    index_to_char[int(item)]\n",
        "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\"\"\"\n",
        "    if idx_dict is None:\n",
        "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "    index_to_char = idx_dict[\"index_to_char\"]\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = \"\"\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(\n",
        "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
        "    )  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    produced_end_token = False\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "        ## slow decoding, recompute everything at each time\n",
        "        decoder_outputs, attention_weights = decoder(\n",
        "            decoder_inputs, encoder_annotations, decoder_hidden\n",
        "        )\n",
        "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "        ni = ni[-1]  # latest output token\n",
        "\n",
        "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "\n",
        "        if ni == end_token:\n",
        "            break\n",
        "        else:\n",
        "            gen_string = \"\".join(\n",
        "                [\n",
        "                    index_to_char[int(item)]\n",
        "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    if isinstance(attention_weights, tuple):\n",
        "        ## transformer's attention mweights\n",
        "        attention_weights, self_attention_weights = attention_weights\n",
        "\n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
        "\n",
        "    for i in range(len(all_attention_weights)):\n",
        "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111)\n",
        "        cax = ax.matshow(attention_weights_matrix, cmap=\"bone\")\n",
        "        fig.colorbar(cax)\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_yticklabels([\"\"] + list(input_string) + [\"EOS\"], rotation=90)\n",
        "        ax.set_xticklabels(\n",
        "            [\"\"] + list(gen_string) + ([\"EOS\"] if produced_end_token else [])\n",
        "        )\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        # Add title\n",
        "        plt.xlabel(\"Attention weights to the source sentence in layer {}\".format(i + 1))\n",
        "        plt.tight_layout()\n",
        "        plt.grid(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    \"\"\"Train/Evaluate the model on a dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        mean_loss: The average loss over all batches from data_dict.\n",
        "    \"\"\"\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in input_strings\n",
        "        ]\n",
        "        target_tensors = [\n",
        "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
        "            for s in target_strings\n",
        "        ]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "\n",
        "            # The batch size may be different in each epoch\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "\n",
        "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = (\n",
        "                torch.ones(BS).long().unsqueeze(1) * start_token\n",
        "            )  # BS x 1 --> 16x1  CHECKED\n",
        "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
        "\n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
        "\n",
        "            decoder_inputs = torch.cat(\n",
        "                [decoder_input, targets[:, 0:-1]], dim=1\n",
        "            )  # Gets decoder inputs by shifting the targets to the right\n",
        "\n",
        "            decoder_outputs, attention_weights = decoder(\n",
        "                decoder_inputs, encoder_annotations, decoder_hidden\n",
        "            )\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            ## training if an optimizer is provided\n",
        "            if optimizer:\n",
        "                # Zero gradients\n",
        "                optimizer.zero_grad()\n",
        "                # Compute gradients\n",
        "                loss.backward()\n",
        "                # Update the parameters of the encoder and decoder\n",
        "                optimizer.step()\n",
        "\n",
        "    return losses\n",
        "\n",
        "\n",
        "def training_loop(\n",
        "    train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
        "):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "        * Returns loss curves for comparison\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "\n",
        "    Returns:\n",
        "        losses: Lists containing training and validation loss curves.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict[\"start_token\"]\n",
        "    end_token = idx_dict[\"end_token\"]\n",
        "    char_to_index = idx_dict[\"char_to_index\"]\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, \"loss_log.txt\"), \"w\")\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    mean_train_losses = []\n",
        "    mean_val_losses = []\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0][\"lr\"] *= opts.lr_decay\n",
        "\n",
        "        train_loss = compute_loss(\n",
        "            train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts\n",
        "        )\n",
        "        val_loss = compute_loss(\n",
        "            val_dict, encoder, decoder, idx_dict, criterion, None, opts\n",
        "        )\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_val_loss = np.mean(val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "            best_val_loss = mean_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            print(\n",
        "                \"Validation loss has not improved in {} epochs, stopping early\".format(\n",
        "                    opts.early_stopping_patience\n",
        "                )\n",
        "            )\n",
        "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return (train_losses, mean_val_losses)\n",
        "\n",
        "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
        "        print(\n",
        "            \"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(\n",
        "                epoch, mean_train_loss, mean_val_loss, gen_string\n",
        "            )\n",
        "        )\n",
        "\n",
        "        loss_log.write(\"{} {} {}\\n\".format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses += train_loss\n",
        "        val_losses += val_loss\n",
        "\n",
        "        mean_train_losses.append(mean_train_loss)\n",
        "        mean_val_losses.append(mean_val_loss)\n",
        "\n",
        "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
        "\n",
        "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "    return (train_losses, mean_val_losses)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Data Stats\".center(80))\n",
        "    print(\"-\" * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print(\"Num unique word pairs: {}\".format(len(line_pairs)))\n",
        "    print(\"Vocabulary: {}\".format(idx_dict[\"char_to_index\"].keys()))\n",
        "    print(\"Vocab size: {}\".format(vocab_size))\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    if opts.encoder_type == \"rnn\":\n",
        "        encoder = GRUEncoder(\n",
        "            vocab_size=vocab_size, hidden_size=opts.hidden_size, opts=opts\n",
        "        )\n",
        "    elif opts.encoder_type == \"transformer\":\n",
        "        encoder = TransformerEncoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            num_layers=opts.num_transformer_layers,\n",
        "            opts=opts,\n",
        "        )\n",
        "    elif opts.encoder_type == \"attention\":\n",
        "      encoder = AttentionEncoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            opts=opts,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    if opts.decoder_type == \"rnn\":\n",
        "        decoder = RNNDecoder(vocab_size=vocab_size, hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == \"rnn_attention\":\n",
        "        decoder = RNNAttentionDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            attention_type=opts.attention_type,\n",
        "        )\n",
        "    elif opts.decoder_type == \"transformer\":\n",
        "        decoder = TransformerDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "            num_layers=opts.num_transformer_layers,\n",
        "        )\n",
        "    elif opts.encoder_type == \"attention\":\n",
        "      decoder = AttentionDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=opts.hidden_size,\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    #### setup checkpoint path\n",
        "    model_name = \"h{}-bs{}-{}-{}\".format(\n",
        "        opts.hidden_size, opts.batch_size, opts.decoder_type, opts.data_file_name\n",
        "    )\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(\n",
        "        list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        losses = training_loop(\n",
        "            train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
        "        )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Exiting early from training.\")\n",
        "        return encoder, decoder, losses\n",
        "\n",
        "    return encoder, decoder, losses\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Opts\".center(80))\n",
        "    print(\"-\" * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print(\"{:>30}: {:<30}\".format(key, opts.__dict__[key]).center(80))\n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yh08KhgnA30"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aROU2xZanDKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e1dd88-3696-4bfb-ef66-4cdafae7d147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/pig_latin_small.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_small.txt\n",
            "data/pig_latin_large.txt\n",
            "Downloading data from http://www.cs.toronto.edu/~jba/pig_latin_large.txt\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "# Download Translation datasets\n",
        "######################################################################\n",
        "data_fpath = get_file(\n",
        "    fname=\"pig_latin_small.txt\",\n",
        "    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_small.txt\",\n",
        "    untar=False,\n",
        ")\n",
        "\n",
        "data_fpath = get_file(\n",
        "    fname=\"pig_latin_large.txt\",\n",
        "    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_large.txt\",\n",
        "    untar=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYMr7NclZdw"
      },
      "source": [
        "# Part 1: Neural machine translation (NMT)\n",
        "\n",
        "In this section, you will implement a Gated Recurrent Unit (GRU) cell, a common type of recurrent neural network (RNN). The GRU cell is a simplification of the Long Short-Term Memory cell. Therefore, we have provided you with an implemented LSTM cell (`MyLSTMCell`), which you can reference when completing `MyGRUCell`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOnALRQkkjDO"
      },
      "outputs": [],
      "source": [
        "class MyLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Wif = nn.Linear(input_size, hidden_size)\n",
        "        self.Whf = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wii = nn.Linear(input_size, hidden_size)\n",
        "        self.Whi = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wic = nn.Linear(input_size, hidden_size)\n",
        "        self.Whc = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.Wio = nn.Linear(input_size, hidden_size)\n",
        "        self.Who = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, h_prev, c_prev):\n",
        "        \"\"\"Forward pass of the LSTM computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "            c_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "            c_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        f = torch.sigmoid(self.Wif(x) + self.Whf(h_prev))\n",
        "        i = torch.sigmoid(self.Wii(x) + self.Whi(h_prev))\n",
        "\n",
        "        c = torch.tanh(self.Wic(x) + self.Whc(h_prev))\n",
        "        o = torch.sigmoid(self.Wio(x) + self.Who(h_prev))\n",
        "\n",
        "        c_new = f * c_prev + i * c\n",
        "        h_new = o * torch.tanh(c_new)\n",
        "\n",
        "        return h_new, c_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCae1mOUlZrC"
      },
      "source": [
        "## Step 1: GRU Cell\n",
        "Please implement the `MyGRUCell` class defined in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGyxqZIQzTJH"
      },
      "outputs": [],
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        # Input linear layers\n",
        "        self.Wiz = nn.Linear(input_size, hidden_size)\n",
        "        self.Wir = nn.Linear(input_size, hidden_size)\n",
        "        self.Wih = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Hidden linear layers\n",
        "        self.Whz = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whr = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        z = torch.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
        "        r = torch.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
        "        g = torch.tanh(self.Wih(x) + self.Whh(r * h_prev))\n",
        "        h_new = (1 - z) * h_prev + z * g\n",
        "        return h_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEq4TP2lZ4Z"
      },
      "source": [
        "## Step 2: GRU Encoder\n",
        "\n",
        "The following cells use your `MyGRUCell` implementation to build a recurrent encoder and decoder. Please read the implementations to understand what they do and run the cells before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jDNim2fmVJV"
      },
      "outputs": [],
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = MyGRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:, i, :]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
        "        of a batch of sequences.\n",
        "\n",
        "        Arguments:\n",
        "            bs: The batch size for the initial hidden state.\n",
        "\n",
        "        Returns:\n",
        "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvwizYM9ma4p"
      },
      "outputs": [],
      "source": [
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
        "            annotations: This is not used here. It just maintains consistency with the\n",
        "                    interface used by the AttentionDecoder class.\n",
        "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            None\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = embed[\n",
        "                :, i, :\n",
        "            ]  # Get the current time step input tokens, across the whole batch\n",
        "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSDTbsydlaGI"
      },
      "source": [
        "## Step 3: Training and Analysis\n",
        "\n",
        "Train the encoder-decoder model to perform English --> Pig Latin translation. We will start by training on the smaller dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmVuXTozTPF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fedd0dd9-1272-41c0-b834-643bde55527f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 20                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('injure', 'injureway')\n",
            "('revenge', 'evengeray')\n",
            "('resist', 'esistray')\n",
            "('robert', 'obertray')\n",
            "('sedateness', 'edatenesssay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.346 | Val loss: 2.024 | Gen: ingay ay illeday-ay ingay ingay\n",
            "Epoch:   1 | Train loss: 1.901 | Val loss: 1.864 | Gen: ongay-ingway-ingway- anday-ingway-ingway- ingsay-ingway-ingway illlay-andway-ingway ingsay-ingway-ingway\n",
            "Epoch:   2 | Train loss: 1.743 | Val loss: 1.768 | Gen: endway-ingway-ingway answay-ingway-ingway oonssay-ingway-ingwa illlay-ingway-ingway oustingway-ingway-in\n",
            "Epoch:   3 | Train loss: 1.622 | Val loss: 1.676 | Gen: entway arsay-ingway oonsay-ingsay-ingway istionsay-ingway oustay\n",
            "Epoch:   4 | Train loss: 1.521 | Val loss: 1.612 | Gen: eway arsay onsingway isway oustay\n",
            "Epoch:   5 | Train loss: 1.436 | Val loss: 1.541 | Gen: eway ansay-ingway ongsay-ingsay isway oringway\n",
            "Epoch:   6 | Train loss: 1.359 | Val loss: 1.507 | Gen: eway arsay-ingway onsingway iway oodgay\n",
            "Epoch:   7 | Train loss: 1.299 | Val loss: 1.519 | Gen: eway away-ingway oningsay-ingsay-andw iway onsuringway\n",
            "Epoch:   8 | Train loss: 1.275 | Val loss: 1.498 | Gen: eway aray-ingway ongingway iway ortingway\n",
            "Epoch:   9 | Train loss: 1.210 | Val loss: 1.487 | Gen: eway aray-ingway oningsay-ingway iway ortingway\n",
            "Epoch:  10 | Train loss: 1.166 | Val loss: 1.513 | Gen: eway aray oningstay-ingsay iway-ingsay oringshay-oday\n",
            "Epoch:  11 | Train loss: 1.129 | Val loss: 1.496 | Gen: eway aray-inway-ay-edway oninishingway iway-axtay oningshay-ortionday\n",
            "Epoch:  12 | Train loss: 1.117 | Val loss: 1.424 | Gen: eway aiway oningscay-ingway iway ortingway\n",
            "Epoch:  13 | Train loss: 1.079 | Val loss: 1.459 | Gen: ehay arway oningsay-ingsay iway onrantionday\n",
            "Epoch:  14 | Train loss: 1.055 | Val loss: 1.446 | Gen: ehay aringway inconshay iway oringway\n",
            "Epoch:  15 | Train loss: 1.029 | Val loss: 1.384 | Gen: ehay aringway oningscay-inway-awla iway ortifymay\n",
            "Epoch:  16 | Train loss: 0.985 | Val loss: 1.397 | Gen: ehay ailay-otay-otay-oday oningscay-inway-adwa iway ortingway\n",
            "Epoch:  17 | Train loss: 0.958 | Val loss: 1.389 | Gen: ehay aringway ontingshay-oringway isway ortifingway\n",
            "Epoch:  18 | Train loss: 0.934 | Val loss: 1.362 | Gen: ehay aitay-otay-oday oningscay-inway-axta iway ortifymay\n",
            "Epoch:  19 | Train loss: 0.910 | Val loss: 1.382 | Gen: ehay aitay-oteway ontingshay-inway-adw iway ortifymay\n",
            "Epoch:  20 | Train loss: 0.902 | Val loss: 1.416 | Gen: ehay aitay oningscay iway ortifymay\n",
            "Epoch:  21 | Train loss: 0.895 | Val loss: 1.419 | Gen: ehay ailay ondingspray isway ortifymay\n",
            "Epoch:  22 | Train loss: 0.883 | Val loss: 1.401 | Gen: ehay aitay oningshay-inway-away iway ortifingsay\n",
            "Epoch:  23 | Train loss: 0.858 | Val loss: 1.362 | Gen: ehay aringway ontingshay-inway isway ortingray\n",
            "Epoch:  24 | Train loss: 0.851 | Val loss: 1.357 | Gen: ehay aithway ontingcay isway ortifymay\n",
            "Epoch:  25 | Train loss: 0.839 | Val loss: 1.366 | Gen: ehay aringway oningshay-oundsay isway ortifymay\n",
            "Epoch:  26 | Train loss: 0.822 | Val loss: 1.418 | Gen: ehay aitay oningshingway isway ortifymay\n",
            "Epoch:  27 | Train loss: 0.805 | Val loss: 1.336 | Gen: ehay aringway ontingspray iway ortifymay\n",
            "Epoch:  28 | Train loss: 0.792 | Val loss: 1.387 | Gen: ehay aritay ontingshingway isway ortifymay\n",
            "Epoch:  29 | Train loss: 0.783 | Val loss: 1.330 | Gen: ehay aritay ontingspray isway ortifymay\n",
            "Epoch:  30 | Train loss: 0.777 | Val loss: 1.324 | Gen: ehay aritay onitingway isway ortifymay\n",
            "Epoch:  31 | Train loss: 0.749 | Val loss: 1.321 | Gen: ehay aiway ontingspray-inway isway ortifymay\n",
            "Epoch:  32 | Train loss: 0.737 | Val loss: 1.359 | Gen: ehay aiway ontininciongray isway ortifymay\n",
            "Epoch:  33 | Train loss: 0.737 | Val loss: 1.347 | Gen: ehay aringay-ybay ontioningsray isway ortifymay\n",
            "Epoch:  34 | Train loss: 0.727 | Val loss: 1.346 | Gen: ehay aiway ontinishionspay isway ortifymay\n",
            "Epoch:  35 | Train loss: 0.712 | Val loss: 1.304 | Gen: ehay aiway ontingsprishay isway ortifymay\n",
            "Epoch:  36 | Train loss: 0.731 | Val loss: 1.351 | Gen: ehay aiway ontininciontay isway ortifinecay\n",
            "Epoch:  37 | Train loss: 0.749 | Val loss: 1.429 | Gen: ehay aiway ontingcay isway ortifymay\n",
            "Epoch:  38 | Train loss: 0.757 | Val loss: 1.368 | Gen: ehay aiway ontingsibleway isway ortifinecay\n",
            "Epoch:  39 | Train loss: 0.713 | Val loss: 1.353 | Gen: ehay ailay ontingspray-inway-ad isway ortifymay\n",
            "Epoch:  40 | Train loss: 0.687 | Val loss: 1.408 | Gen: ehay ailay-omprehentway ontinginingthay isway ortifinecay\n",
            "Epoch:  41 | Train loss: 0.680 | Val loss: 1.387 | Gen: ehay airay ontinivilitypay isway ortifymationsday\n",
            "Epoch:  42 | Train loss: 0.666 | Val loss: 1.364 | Gen: ehay ailay ontinitionspay isway ortifymay\n",
            "Epoch:  43 | Train loss: 0.663 | Val loss: 1.378 | Gen: ethay airway ontinitioncay-inway- isway ortifymay\n",
            "Epoch:  44 | Train loss: 0.659 | Val loss: 1.427 | Gen: ehay ailay-oundway ontinitioncay isway ortifymay\n",
            "Epoch:  45 | Train loss: 0.659 | Val loss: 1.380 | Gen: ehay aiway ontinitioncay isway ortifymay\n",
            "Epoch:  46 | Train loss: 0.643 | Val loss: 1.365 | Gen: ehay airway ontinitiongingway isway ortifymay\n",
            "Epoch:  47 | Train loss: 0.633 | Val loss: 1.455 | Gen: ethay ailay-acketjay ontingspicityway isway ortifymay\n",
            "Epoch:  48 | Train loss: 0.731 | Val loss: 1.429 | Gen: ethay aiway ontinginghay isway orkinscay\n",
            "Epoch:  49 | Train loss: 0.718 | Val loss: 1.421 | Gen: ehay aiway ontionininsway isway ortifoasiousway\n",
            "Obtained lowest validation loss of: 1.3041225642692753\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehay aiway ontionininsway isway ortifoasiousway\n"
          ]
        }
      ],
      "source": [
        "%timeit\n",
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "rnn_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n",
        "    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n",
        "    \"attention_type\": \"\",   # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_s.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_s)\n",
        "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mR97V_NtER6"
      },
      "source": [
        "Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \n",
        "\n",
        "For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a quick and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3YLrAjsmx_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e7fd8b-81f1-4974-c57b-e583923041c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn                                    \n",
            "                         attention_type:                                        \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('empirical', 'empiricalway')\n",
            "('screams', 'eamsscray')\n",
            "('fate', 'atefay')\n",
            "('johnson', 'ohnsonjay')\n",
            "('hammond', 'ammondhay')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.339 | Val loss: 2.028 | Gen: ay-ay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ontay-ay-ay-ay-ay-ay insay-ay-ay-ay-ay-ay ontay-ay-ay-ay-ay-ay\n",
            "Epoch:   1 | Train loss: 1.950 | Val loss: 1.898 | Gen: aresay-ay-aray-aray- aray-aray-aray-aray- ontay-ontay-ay-ay-ay illlllllay-ay-aray-a ontay-ontay-ay-ay-ar\n",
            "Epoch:   2 | Train loss: 1.775 | Val loss: 1.766 | Gen: ay-ay-away-ay-away-a away-ayday ontay-ay-ay-ay-ay-ay illllllay-ay-arsay-a ontay-ay-ayday\n",
            "Epoch:   3 | Train loss: 1.639 | Val loss: 1.675 | Gen: enay-ayday away-away ontay-ay-away-ayday issay-ayday ontay-ayday\n",
            "Epoch:   4 | Train loss: 1.533 | Val loss: 1.635 | Gen: etay-ayday away-away ontay-ayday issay ongay-ayday\n",
            "Epoch:   5 | Train loss: 1.454 | Val loss: 1.604 | Gen: otay-ayday away-awlay ontay-ay-ateray-ayda issay ortionsay\n",
            "Epoch:   6 | Train loss: 1.403 | Val loss: 1.619 | Gen: etay-ay-ayday arway-ayday ontay-ay-ay-ayday-ay istersay intay-ayday\n",
            "Epoch:   7 | Train loss: 1.366 | Val loss: 1.542 | Gen: etay-ay araray-ay-awlay otay-ontay-ay-ayday issay oodgay-ayday\n",
            "Epoch:   8 | Train loss: 1.304 | Val loss: 1.506 | Gen: oway aray-ay-ay ontay-ay-ay-ay-ay-ay ishedway oodgay-ay\n",
            "Epoch:   9 | Train loss: 1.259 | Val loss: 1.503 | Gen: etay aray-ay ontiontay-ayday ishedway ortionsway\n",
            "Epoch:  10 | Train loss: 1.207 | Val loss: 1.434 | Gen: etay away-awlay ontinglay-ayday ishedway orghay-ayday\n",
            "Epoch:  11 | Train loss: 1.165 | Val loss: 1.449 | Gen: etay-ayday arway-awlay ontinglay-ingway-ayd issway orysay-inway-awlay\n",
            "Epoch:  12 | Train loss: 1.141 | Val loss: 1.413 | Gen: etay arway-ayday ontinglay-ingway isshay orghay-ayday\n",
            "Epoch:  13 | Train loss: 1.102 | Val loss: 1.415 | Gen: athedway ariplway ontintiontay-ayday istedway orybay-ybay-ay\n",
            "Epoch:  14 | Train loss: 1.067 | Val loss: 1.389 | Gen: etay ariraryway ondintionay-ayday ishedway orybay-ybay\n",
            "Epoch:  15 | Train loss: 1.035 | Val loss: 1.326 | Gen: attay ariplway ondglooticationway issistmay orybay-ybay\n",
            "Epoch:  16 | Train loss: 1.006 | Val loss: 1.451 | Gen: etay ailway-away-ayday ondoninglationcay istay oorybay-oadgay-ayday\n",
            "Epoch:  17 | Train loss: 1.008 | Val loss: 1.352 | Gen: etay ariray ondingfay-oadway ishedway orygray\n",
            "Epoch:  18 | Train loss: 0.970 | Val loss: 1.307 | Gen: atetjay arilway ontingcay-ondway isway onglay-inway\n",
            "Epoch:  19 | Train loss: 0.953 | Val loss: 1.354 | Gen: ethay airalway onciontioncay istsway oroorybay-ybay\n",
            "Epoch:  20 | Train loss: 0.951 | Val loss: 1.420 | Gen: atetjay airalway ontintingtay-ingpay- isway-awlay orningray-awlay\n",
            "Epoch:  21 | Train loss: 0.931 | Val loss: 1.260 | Gen: etay ariralway ondictioncay isshedway orysray\n",
            "Epoch:  22 | Train loss: 0.879 | Val loss: 1.292 | Gen: ethay aillway-yearay oringorationcay issay orymay-ybay\n",
            "Epoch:  23 | Train loss: 0.878 | Val loss: 1.310 | Gen: etay airilway ongelay-ondentay-ayp ishedway oringingsay\n",
            "Epoch:  24 | Train loss: 0.881 | Val loss: 1.256 | Gen: ethay airlway ondinationcay isway orybay\n",
            "Epoch:  25 | Train loss: 0.840 | Val loss: 1.243 | Gen: atway ariralway oringcay-ondedway-yb issay orymay-ybay\n",
            "Epoch:  26 | Train loss: 0.818 | Val loss: 1.249 | Gen: ethay ailway-ilway onciontiontay-inway- isway orymay-ingray\n",
            "Epoch:  27 | Train loss: 0.844 | Val loss: 1.269 | Gen: htay arway oniontationcay ishway ororygray\n",
            "Epoch:  28 | Train loss: 0.832 | Val loss: 1.257 | Gen: athay airilway ondingfay-ompronedwa ishedway orymingway-ybay\n",
            "Epoch:  29 | Train loss: 0.805 | Val loss: 1.221 | Gen: htay airilway ondgingcorypay ishway orkningsray\n",
            "Epoch:  30 | Train loss: 0.780 | Val loss: 1.178 | Gen: atway airilway ondingcorypay ishway oringray\n",
            "Epoch:  31 | Train loss: 0.764 | Val loss: 1.180 | Gen: ethay ariralway ondingcay-oryday ishway oringray\n",
            "Epoch:  32 | Train loss: 0.739 | Val loss: 1.157 | Gen: atedway airilway ondingcomentway ishway ornindgray\n",
            "Epoch:  33 | Train loss: 0.721 | Val loss: 1.205 | Gen: atedsay airilway oningcorypay isway oringray\n",
            "Epoch:  34 | Train loss: 0.734 | Val loss: 1.205 | Gen: ethay ariralway oningcay-ouredway issway orindgray\n",
            "Epoch:  35 | Train loss: 0.728 | Val loss: 1.184 | Gen: ethay airiway ondingcorypay isway orrygray\n",
            "Epoch:  36 | Train loss: 0.728 | Val loss: 1.236 | Gen: ethay ariralway oningday ishway orrinkway\n",
            "Epoch:  37 | Train loss: 0.753 | Val loss: 1.176 | Gen: tay ariway ontingcay isway orysay\n",
            "Epoch:  38 | Train loss: 0.716 | Val loss: 1.159 | Gen: ethay airlway-yaray oningcomontay isway oringray\n",
            "Epoch:  39 | Train loss: 0.709 | Val loss: 1.173 | Gen: ethay ariraway ondgray-ouducentway isway ornkway\n",
            "Epoch:  40 | Train loss: 0.682 | Val loss: 1.172 | Gen: ethay ariray oningcay isway orinkway\n",
            "Epoch:  41 | Train loss: 0.649 | Val loss: 1.108 | Gen: ethay ariway ondinticationway isway orysmay\n",
            "Epoch:  42 | Train loss: 0.639 | Val loss: 1.093 | Gen: ethay ariway ondingcay ishway orrikngray\n",
            "Epoch:  43 | Train loss: 0.628 | Val loss: 1.119 | Gen: ethay airalway oninticationway isway orinkway\n",
            "Epoch:  44 | Train loss: 0.618 | Val loss: 1.121 | Gen: ethay ariway ondingcay-oryday isway orniknay\n",
            "Epoch:  45 | Train loss: 0.618 | Val loss: 1.197 | Gen: ethay ariway oncicationcay isway orinsday\n",
            "Epoch:  46 | Train loss: 0.635 | Val loss: 1.114 | Gen: ethay airalway ondingcorypay isway ornismay\n",
            "Epoch:  47 | Train loss: 0.636 | Val loss: 1.196 | Gen: ethay arlay oninfway-ondentalway isway orinkway\n",
            "Epoch:  48 | Train loss: 0.627 | Val loss: 1.212 | Gen: ethay ariway ontiationcay issay orniknay\n",
            "Epoch:  49 | Train loss: 0.628 | Val loss: 1.141 | Gen: ethay ariway ondingcay-ybay isway orysnimay\n",
            "Obtained lowest validation loss of: 1.0925906855686038\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay ariway ondingcay-ybay isway orysnimay\n"
          ]
        }
      ],
      "source": [
        "%timeit\n",
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "rnn_args_l = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_large\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"early_stopping_patience\": 10,\n",
        "    \"batch_size\": 512,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n",
        "    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n",
        "    \"attention_type\": \"\",   # options: additive / scaled_dot\n",
        "}\n",
        "rnn_args_l.update(args_dict)\n",
        "\n",
        "print_opts(rnn_args_l)\n",
        "rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01HsZ6EItc56"
      },
      "source": [
        "The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Are there significant differences in the validation performance of each model? (see follow-up questions in handout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qyk_9-Fwtekj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "5cb5cb67-c0bd-43b0-ebd1-9daad2909d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plot saved to: /content/content/csc421/a3/loss_plot_gru.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAEdCAYAAAARsJF3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3hVVfbw8e9KCIQSeq8JvYcWWnAAK4yIDRQFFUH9WdAZZ3R0HBUUecc2M8ogolIVREGwYC9jQ5oBQgcVCBAD0iFAgISs9499ApeYntzclPV5nvvAPWefc9a9N1lZd5999hFVxRhjjDHGGOMEBToAY4wxxhhjihIrkI0xxhhjjPFhBbIxxhhjjDE+rEA2xhhjjDHGhxXIxhhjjDHG+LAC2RhjjDHGGB9WIBtjSjQRiRaRtSJyWkS+CXQ8RYWIjBSRY4GOwxhjiiIrkI0pxUSkjoj8R0R+FpGTIrJXRJaIyL0iUsmnXZyIqPdIEpHNIvKgiIhPm37e+poZHCdORB7IIo5xPvs/IyK7RGSqiNQqgJf5IrAGaAZcUwD7M1kQkZk+n2Wy9zP1tYjcIyIhudxXpj9T/iQi4d5xuxXmcY0xRUeZQAdgjAkMEQkHfgCOAo8Ba4EkoB1wG3AAeNNnkyeBl4FQ4GLv/0eBVwoopC1APyAY6AxMAxoAA/OyMxEpq6qngebAS6q6K6+B+ezL5MyXwE24z7IWcCHwBHCTiFykqscDGZwxxmTHepCNKb1eBlKBbqr6lqpuVNXtqvqhql4FzE3XPlFV96hqnKpOxRXUlxZgPCne/n9V1Q+BicClIlIeQERuFZGNXk/3TyJyv4iczWFej989IrJQRI4Db4qIAlWA6d76kV7bP4jIcm9fv3m96GV99vWNiLwsIs+LyD7gB5/ezIEistLrSf9eRBqKSF8RWSMix0TkQxGp4bOvKBH5XET2i8hREVksIr18X7i33ztEZL6IHBeRbSIyIl2b+iIyR0QOiMgJEYkVkf4+66/w4jopIttFZILva8qMt91P3nZfi0hTb3m4iKSm70UVkdu915LVvk/5fJaxqvpv3JefLsDffPY1QkR+FJFEr6d5vog0SDs+8LXXdJ/3Hs301g3w3vtDInJQRD4TkTbp4nxcRHaIyCkR2SMir/usExH5m4hs9T7Hdene7+3evz96x/0mu/fRGFOyWIFsTCnkFXCX4XpWM+zN00zuQ+8VF/2ANkCy34J0vdlBQBkRuR34f8Dj3nH/CjwE3J1um7HAx0AHb3094ATwZ+//b3sF2CfAalxP9WjgBuCf6fY1AhDgAuBmn+VPePvrAVQD3vbiugNXBLYDxvm0DwPe8PbTHYgFPvYtoj2PA+8Dkd4+p4tIYwARqQh8C4QDV3mv78m0DUXkMmAOMMk7/ihgiPeeZaUc7j27FeiF6/FdKCKiqnHAF96+fI0C3shtj7qqrgc+Ba71WVzWO34kMAioybkvZrt82rbDfX5/8p5XBF7AvZ/9gCPAorSiXUSuBR7A/Xy08Pa9wue4T+E+93uAtrjP/hURudxb3937d4B3XBuaY0xpo6r2sIc9StkDV9wpcHW65fHAMe8xxWd5HHDKW37a2zYJ6O3Tpp+3vGYGx4sDHsginnHAep/nrYGfgeXe853ATem2+TOw0ee5Av/NYN/HgJE+zyd4+w7yWTbSe30VvOffAGvT7Sft9V3ms2yMt6xLZq8lg3gE2A2MSBf7P32el8EV9iO857cDiRm9t97674DH0i27ynvtksk2I73jRvssawKcAS72ng8BDgGh3vM23jbts3h9M4EPM1n3NHAii21be/tvmN3PVLrtKnpx9/Ge/wU3ZCckk7ZJwAXplr8AfOz9P9w7brfC+H20hz3sUfQe1oNsjPF1AdAJ19sWmm7dv711fXGnvp9Q1SUFeOw23hCFJGAjrgdxuLgL9RrheviOpT1wxVazdPuIyclxgGWqmuqzbDGuN7O5z7KVmWy/1uf/v3n/rku3rHbaExGpLSKveMMYjuAK3dpA48z2q6opwD6f/XTGFez7M4mpK/CPdO/Pm7hisG4m24AbYnO2Z1VVdwAJuF5VcD3apznXgzoKWKGuNzgvBFd4uiciXUTkfW8oRCLnPr/07835OxFpJiJvekMkjuLe8yCf7ebjfn63i8g0ERkqIuW8dW29dZ+me7/u4vc/T8aYUsou0jOmdPoFV6i09l2oqtsBROREBtscUNVfgF+8U9g/i8hyVU0bJ3rU+7cKkL6Qq4o7DZ6VrcAfcT2BCap6youljrf+TiC7gjy/F3/5DivJbF++w0pc969q+mW+nQ+zgDrA/Zzrif8KV5Bntt+M9pOVINzQj/kZrNuXzbYZDqUB97q8sbujRGQe7sK7x3MYU0baAtvg7LCRzzh3Qd9e3BCL7/n9e5Peh7izHf8H/Aqk4L5UlfXi3iUirYCLcBeU/gsYKyI9OPeeXoE7M+HLn0OGjDHFiBXIxpRCqnpARD4HxojIf1U1V/PhquohEZkE/EdEOquq4oYtpOJ6M7emtfUu+qqCO+WdldNeAZ7+WL+JSALQTFVfz2C73NoEXCciQT69yH1wPaVbM98sz/oA96nqR3C24K+Xy32sxs0AUTOTXuRVQOuM3r9sBOHG2y7xYmsM1Me9R2mm4orPu3Hjqd/K5THw9t0eN6b3KW9Ra1xB/IjPF7P0Y33TxjkH++ynhrft3WlfzkSkC+n+nqnqSeAj4CMReRrYA0QDS3FfUpqo6v8yCfd3xzXGlC5WIBtTet2Nm+ZtpYiMw80VnIIrcCOBz7PZfjLuQrihwDxVTRSRqcBzInIKN2SgEfAMsAzXM5hXY4H/ishh3EV4IbgZERqoavqL67IzGTd+ebKIvAg0xQ3XmKSqGfWc59dPwAgRWY4b8vAs5wqwnHoTeBh4X0QexvWatsfNLPI17oK9D0VkBzAP9zm2B7qr6t8y2SdeuxdE5E+4cbn/ATbgenUBUNUtIrIYeA54S1WPZrin85UTkbq4ArwWrif3Edywlee9NjtxheoYEXkJN/RlfLr97MD1cF8uIou8GA/hzlDcLiK7cFMBPue9FsDdBAX39205bhz29bje4Z+9n9PngedFRHDjtysBPYFUVX0V15udBFwmInHASVXN7gyIMaYEsTHIxpRSqroNN7b1U1xhshrXE/kXzhWRWW2/Fzc7wzg5N93an4DpuIJzA254wTrgCq+XOa+xTsWNf70JV8h/j5s1YntW22Wyr19xcyt3xs0oMR03c8IjeY0vG6NwBdhKXO/rdNxQixxTN9NIX9ywgkXAetyQirQhHp8BlwP9cWOKV+AK6vRDCNI7hbto8XVcMRkEXJPBZzUNN3xhWg5Dvhh3IeJO3HCSwbiLF//gvRZUdR9wC+5iwo24L0F/Sfe6f/WWT8CNM57k9fpfD3T03oeXcPN4n/LZ9DBulorvvTbXeq8r7eflMS+eB3A/p194bbZ7x00B7sPNB56AG4ttjClFJB9/s4wxxpQCIvIQMFpVWwY6FmOMKQw2xMIYY0yGxN1uvAnuzMCEAIdjjDGFxoZYGGOMycwk3LCbHyi4W4obY0yRZ0MsjDHGGGOM8WE9yMYYY4wxxviwAtkYY4wxxhgfViAbY4wxxhjjwwpkY4wxxhhjfFiBbIwxxhhjjA8rkI0xxhhjjPFhBbIxxhhjjDE+rEAuIUTkExG5JdBx5IaI9BORbwIdR15k9X6LSLiIqIhkeKdKERknIrP9G2GGxy12PyPGFFdeDmge6Dh8ebkpLtBx5IWITBGRx7JYn+n7LSIjRWSx/6LLNKYsYzZFmxXIASQix3weqSKS5PN8eG72paoDVXVWHuOIE5GL87Ktv4nIJSLytYgkisgBEYkVkYdEJNRbP05Ekr337LCILBGRXj7bZ5gYs3rNIvKNiNyWblk/EYlPe56f99ufROQREdnuvR/xIvJ22rqiFLP3me4TkaMiskZErvRZd7mILPY+zz0iMlVEwgIZryl9RORTEXkyg+VXej+XGX4BzuG+f5djigoR6SYiH4rIIe93cKOITBCRat76kSJyxssxab+/g3y2Py9X+izP9DWLyEwReSrdsvM6GlT1TlUdX7CvNv9EZLSIbPb+Rv0mIh+n5auiFLOIzBaR3d5n9pPvZyEiPUXkCxE56OXl+SJSL5DxFgVWIAeQqlZKewA7gSt8ls1Ja5efRFycichQ4B3gTaCJqtYArgcaAo18mr7tvYc1ga+B+YUda1Hg9Q7fBFzsvR/dgK8CG1Wm/gTUU9XKwB3AbJ+EXAV4CqgPtAEaAM8FJEpTms0CRoiIpFt+EzBHVVMCEJNfiUhv4BvcrcVbq2pVYACQAkT6NF3q5ZiqwGTgLRGpWsjhBpyI9AX+H3CDqobh8tXbWW8VMP8Ewr2cOxh4SkS6euuqAa8C4UATIBGYEYggixIrkIugtG/gXk/pHmCGiFTzvtXv877ZfygiDX22OfvtPK3XVESe99puF5GBeYijnIi8ICIJ3uMFESnnravpxXDY+9b5vYgEeeseEpFfvW/UW0TkojwcW4B/A0+q6muqehBAVbeo6r2q+nP6bbw/WHOABiJSK7fHzGV8vu93sPde7xeRbcDl6dpGiMi33vvxBa6Q913fU1zP92GvN6ZfuuOMF5EfvO0/F5HztvcRBXymqlsBVHWPqr6aScxr5PwzGJp23KziKSiqutanwFAgBO9Lj6q+qaqfquoJVT0EvAZEF3QMxmTjPaAGcEHaAq8XdRDwuoh0F5Gl3u/JbhGZJCJl83NAEQkSkUdFZIeI7BWR10Wkircu1OsFPOAd80cRqeOtGyki27wcsV1yeQbSx7PADFX9p6r+BqCqO1V1rKp+k76xqqYCbwAVgRZ5PGaOSLpeZhF50HvfE0RkVLq2NUTkA6+3dAXQLN361j49pltE5Lp0x3lJRD7y3s/lInLe9j6icF8WVgOo6kFVnaWqieljFpFF8vuzxiOzi6egqOoGVT2V9tR7NPPWfaKq81X1qKqeACZhOdcK5CKsLlAd923uDtxnNcN73hhIwv0QZ6YHsAVXjD0LTPOKztz4B9AT6ITrPegOPOqt+ysQD9QC6gCPACoirYAxQJT3jfoyIC6XxwVohespXpDTDbw/TjcDB4BDeThmXt2O+6PZGddrOyTd+jeBlbjPYjxwdhywiDQAPsL1mFYHHgAWpCvwbwRuBWoDZb02GVkG3Oz94egmIsGZBayqkT5nL/6C+1lZlcN4zvL5kpTR48PMju+z7UlgOa7XKiaTpn8ANmS1L2MKmqomAfNwOSXNdcBmVV0DnAHux/1e9wIuAu7O52FHeo/+QFOgEufy/C24syuNcIX7nUCSiFQEJgIDvZzbG4jN7YG9/fQidzk3GJebkoEduT1mXonIAFxuugRXmKcfLvcScBKoB4zyHmnbVgS+wOXl2sAwYLKItPXZfhjwBK5n9RdgQiahLAcuE5EnRCRavA6kjKjqFT45dyiwB/gqh/H4vvbJWeTctZkd32fbE8BmYDfwcSZNLediBXJRlgqMVdVTqpqkqgdUdYHXq5aI+4Xtm8X2O7ye1zO4U4X1cIVsbgzH9eDuVdV9uIRxk7cu2dtnE1VNVtXvVVVxfzTKAW1FJERV49J6NHMprZd0T9oCEXnLSwInROQmn7bXichh3JeG24EhBXD6c6Jv4gGyKvauA15Q1V1eT/c/fWJujOtleMz7LL8DFvlsOwL4WFU/VtVUVf0CVyj+0afNDFX9yecPdqeMglDV2cC9uC8l3wJ7ReShrF6kiPTBFcODVfVoDuPxPeYgVa2ayWNQRtv4bguEefv+3OuNSh/fJbjC4PGs9mWMn8wChoh3zQOuWJ4FoKorVXWZqqaoahzwClnn5JwYDvxbVbep6jHg78AwccPsknGFcXNVPeMd/6i3XSrQXkTKq+puVc1LcVMNVxP45txnvRx4XEQe9Wnb08uLJ4HngRGqujcPx/T1QLqcm1Wxdx0uL65X1ePAOJ+Yg4FrgcdV9biqrsf7zDyDgDhVneF9dqtxXwqG+rR5V1VX+JyVzCznfg9cA3TBdSwcEJF/Z9U5ISItvXiuU9VdOYzH95h3Z5FzO2bxnqGqd+Ny7gXAQuBU+jYi0hGXbx/Mal+lgRXIRdc+VT2Z9kREKojIK96pt6PAd0DVLH4RzyY575QJuN6I3KjP+b0CO7xl4MaE/gJ87p3ae9g71i/An3EJa69X1NYn9w54/569UEBVh6kbE7cK8H3d87zldYD1QFefdSm40/fpheD+4GTmPt/Eg0timakP7PJ5viPdukNeEs9ofRNgaLo/DH3wed34fJbACbL4HFV1jqpejBsbeCcwXkQuy6itiDTCFdy3qOpPuYinwHhfrj4BLhWRweni64nrVRniE58xhUZVFwP7gau80+zdcT+TiEhL7yzIHi8n/z/SDZ/Kg4xybhlcbnsD+Aw33jfBK15DvNxyPe73fbc3NKB1Ho59CFdo++bcv3n5710vjjTLvOXVgA/wGYZC3nPu8+lyblbFXlY5t5YXa2brmwA90uW44biztmlyk3M/UdUrcGfcrsSdAcjsYsQqwPvAo97PVk7jKTDel6vFuDO0d6WLrznwCfAnr/gv1axALro03fO/4oYd9FA3yP4P3vLcDpvIjQTcL2+axt4yVDVRVf+qqk1xA/7/It5YY3VjSPt42yrwTB6OvQX4FfftPEdUdT9uOMo4OXfB106gse/wEhGpgDuVVVCnBHdz/kWDjdOtq+adRsto/S7gjXS9ABVV9en8BOQVnvNxvTDt068XkfK4MZYveAVqnuIRN3XcsUwen2S0TSbK4DNOUEQ64/7wjlLVonqhoSkdXsf1HI/AjfH/zVv+Mu5UdQsvJz9C/vNxRjk3BfjN+51+QlXb4oZRDPLiQlU/U9VLcMXtZty4/VzxCu3l5C7nHsMVWTd5v7Pgcm5NETlbVHr5twmFk3P34d6zzNbvAr5Nl+Mqqep5xWJueWfcvgL+R8Y5Nwj35epr9bk2JLfxiJs6LrOcm5szB+lzbhPgS2C8qr6Ri/2UWFYgFx9huCEEh0WkOjC2gPcfIu4ikLRHGWAu8KiI1BJ3YdjjwGwAERkkIs29xHcEN7QiVURaiciF3lisk17Mvzt1nh3vdPtfgbEicru4ixRFRFqQxVARVd2C62X5m7douRfHw97rqgg8jRs2UFDJeh5wn4g0FHcRz8M+8ezwjvWEiJT1hjRc4bPtbOAKEblM3MV+oeIu0mxILom7UOdyEQkTd7HPQKAd7j1IbzpuLOWz6ZbnKh51U8dVyuSR4YWh4i5IGSgi5UUkRERG4L7wfeutbw98Ctyrqosy2ocxheh13BjX2zn/VH0YcBQ45vXY5rbAKpMu54bgcu794i7srYTrlX5bVVNEpL+IdPDOGh7F9camikgdcVPPVcSdMj9GHnKu52/AKBF5WERqA3i/+xGZbaBuWNlUvGFQqroTl3OeEZFK3t+CB714l+UxrvTmASNFpK3X4XH276G6YYULcR0lFcSN5fWd//1DoKWI3OTlnxARiRKRNrkNwnvfh/n8feqOG2aT0eucgLuY8U/plucqHnVTx2WWc9tlEmdtL85KXl6/DLgBb5Yjcdee/A+YpKpTcvs+lFRWIBcfLwDlcaf7luEKiIL0Ma6YTXuMw41NjcH1Qq7DDW1Iu4q4Be7b5jFgKTBZVb/GjT9+2otzD66n9u95CUhV38aNNRuB+5a9H5cYXyXrqdyeA+4Qkdrqrtq9HOiHu6hwG+703HWqmr6XPq9ewxXla3Dv0cJ062/EXTR5EJfIX09b4Y1BuxLX+7QP9zofJG+/m0e9/ewEDuMuzrzL51Ser2HA1el6Hy4o4HgyI3hDcLxj/Am4XlVXeev/ijtNOi2PPSPGFBh144uX4IqbD3xWPYD73U7E5YDcTu/1Mufn3Bm4L65v4IbQbcd9ub/Xa18XN+3lUWAT7gvlG7jfzb/gep8P4gq0PPWGerniQtwX1p/Ene7/FHcR7X+z2PQF4I/ixq+CG/JRGzcM71fcBYyX+w4bzA/vrNcLuKLuF+9fX2NwwyL2ADPxmbJM3TU8l+JyYILX5hnc367cOoT74vQz7nOZDTynPtO0+rgBd9H7IZ+8NryA48mM4n4m4r2Ynwf+rKppP8+34S4KHef7N6EAj18sScHVCMbkjrjpw8apar8Ah2KMMSWeiIQD36hqeGAjMabosx5kY4wxxhhjfFiBbAIpDnf6yxhjjP8dxg1NMMZkw4ZYGGOMMcYY48N6kI0xxhhjjPFRJvsmRUvNmjU1PDw80GEYY0y+rFy5cr+qZngL7+LAcrExpiTILBcXuwI5PDycmJiYQIdhjDH5IiIFNQ93QFguNsaUBJnlYhtiYYwxxhhjjA8rkI0xxhhjjPFhBbIxxhhjjDE+it0YZGOKk+TkZOLj4zl5skDusGqKodDQUBo2bEhISEigQzGmVLI8bCD3udgKZGP8KD4+nrCwMMLDwxGRQIdjCpmqcuDAAeLj44mIiAh0OMaUSpaHTV5ysQ2xMMaPTp48SY0aNSwpl1IiQo0aNaznypgAsjxs8pKLS0eBvGwKvDMq0FGYUsqSculmn7/n5FF07jBS1swLdCSmFLLfQ5PbnwG/FcgiMl1E9orI+kzWVxGRRSKyRkQ2iMit/oqFlCRYvwD2ZBiKMcYYPztOeQ5tWczWZYsCHYoxxmTLnz3IM4EBWay/B9ioqpFAP+BfIlLWL5F0HQkhFWDZZL/s3hhjTNYqhoawUVpR9UBsoEMxxphs+a1AVtXvgINZNQHCxPV5V/LapvglmPLVoPMIWDsPEvf45RDGFAfjxo3j+eefL/D93nbbbWzcuNEv8bz33nvn7fvxxx/nyy+/zPWx/GHmzJmMGTMG8N97m18i0khEvhaRjd7Zuj9l0EZEZKKI/CIia0Wkiz9i2VOlA3VO74QTWf1pMKbks1xcsPyRiwM5i8Uk4AMgAQgDrlfV1IwaisgdwB0AjRs3ztvRetwJK15zj4sey9s+jMmHJxZtYGPC0QLdZ9v6lRl7RbsC3WdeTJ061W/7fu+99xg0aBBt27YF4Mknn/TbsUqoFOCvqrpKRMKAlSLyhar6/hUdCLTwHj2Al71/C1Ry3W5weAYpu36kTKvLCnr3xmSrJOdhsFxckAJ5kd5lQCxQH+gETBKRyhk1VNVXVbWbqnarVatW3o5Woxm0vhxipsHpE3mN2ZhiZ8KECbRs2ZI+ffqwZcsWACZOnEjbtm3p2LEjw4YNy9F+4uLiaN26NcOHD6dNmzYMGTKEEyfc71K/fv2IiYkBYNq0abRs2ZLu3btz++23n/1Wn53XXnuNqKgoIiMjufbaazlx4gRLlizhgw8+4MEHH6RTp05s3bqVkSNH8s477wAQHh7O2LFj6dKlCx06dGDz5s2Z7v/bb7+lU6dOdOrUic6dO5OYmMg333xD3759ufLKK2natCkPP/wwc+bMoXv37nTo0IGtW7cCsGjRInr06EHnzp25+OKL+e2333L0mooCVd2tqqu8/ycCm4AG6ZpdCbyuzjKgqojUK+hYwpp354wKh39aUtC7NqbIs1zsFJdcHMge5FuBp1VVgV9EZDvQGljhtyP2GgObP4Q1cyFqtN8OY0xGAtHDsHLlSt566y1iY2NJSUmhS5cudO3alaeffprt27dTrlw5Dh8+DMDXX3/N/fff/7t9VKhQgSVLXEGzZcsWpk2bRnR0NKNGjWLy5Mk88MADZ9smJCQwfvx4Vq1aRVhYGBdeeCGRkZE5ivWaa67h9ttvB+DRRx9l2rRp3HvvvQwePJhBgwYxZMiQDLerWbMmq1atYvLkyTz//POZ9qA8//zzvPTSS0RHR3Ps2DFCQ0MBWLNmDZs2baJ69eo0bdqU2267jRUrVvDiiy/y3//+lxdeeIE+ffqwbNkyRISpU6fy7LPP8q9//StHr6soEZFwoDOwPN2qBsAun+fx3rLdBXn8Fg3rsUUbU3On/9K8MVkJVE+v5eJziksuDmQP8k7gIgARqQO0Arb59YiNe0L9Lu5ivdQMR3MYU6J8//33XH311VSoUIHKlSszePBgADp27Mjw4cOZPXs2Zcq478n9+/cnNjb2d4+0hAzQqFEjoqOjARgxYgSLFy8+73grVqygb9++VK9enZCQEIYOHZrjWNevX88FF1xAhw4dmDNnDhs2bMjRdtdccw0AXbt2JS4uLtN20dHR/OUvf2HixIkcPnz47OuOioqiXr16lCtXjmbNmnHppZcC0KFDh7P7i4+P57LLLqNDhw4899xzOY6tKBGRSsAC4M+qmqdzzCJyh4jEiEjMvn37cr1901oVWaPNqXxwjeVgU6pYLj6nuORif07zNhdYCrQSkXgRGS0id4rInV6T8UBvEVkHfAU8pKr7/RWPFxT0ugcO/AI/f+7XQxlTlH300Ufcc889rFq1iqioKFJSUvj666/PnvbyffTu3fvsdunnkSzIuUVHjhzJpEmTWLduHWPHjs3xhO7lypUDIDg4mJSUzK/zffjhh5k6dSpJSUlER0efPQWYtj1AUFDQ2edBQUFn93fvvfcyZswY1q1bxyuvvFLsbvwhIiG44niOqi7MoMmvQCOf5w29ZefJ73C3kOAgEsI6EnrmOOzfkuvtjSlpLBcX3Vzsz1ksblDVeqoaoqoNVXWaqk5R1Sne+gRVvVRVO6hqe1Wd7a9YTiafYUPCEfek7ZVQuSEsneSvwxlTZPzhD3/gvffeIykpicTERBYtWkRqaiq7du2if//+PPPMMxw5coRjx47lqNdi586dLF26FIA333yTPn36nHe8qKgovv32Ww4dOkRKSgoLFizIcayJiYnUq1eP5ORk5syZc3Z5WFgYiYmJ+XwnYOvWrXTo0IGHHnqIqKioLMfIpXfkyBEaNHDDdmfNmpXvWAqTN1PQNGCTqv47k2YfADd7s1n0BI6oaoEOr0hzum5X959dNszClB6Wi88pLrm4VNxJ76/z1zByxo+cSjkDwSHQ4/8g7ntIsPk4TcnWpUsXrr/+eiIjIxk4cCBRUVGICCNGjKBDhw507tyZ++67j6pVq+Zof61ateKll16iTZs2HDp0iLvuuuvj6UUAACAASURBVOu89Q0aNOCRRx6he/fuREdHEx4eTpUqVXK07/Hjx9OjRw+io6Np3br12eXDhg3jueeeo3Pnzmcv1MiLF154gfbt29OxY0dCQkIYOHBgjrcdN24cQ4cOpWvXrtSsWTPPMQRINHATcKGIxHqPP6Y7o/cxbojbL8BrwN3+CqZGkzYc0kqcilvmr0MYU+RYLj6nuORicdfIFR/dunXTtCs0c+q7n/Zx8/QV/GtoJNd2bQgnj8C/27pZLa551U+RGgObNm2iTZs2gQ6jQMTFxTFo0CDWr8/6jpTHjh2jUqVKpKSkcPXVVzNq1CiuvvrqQoqyaMro50BEVqpqtwCFlG95ycXg8nHKG0PoWf0YFe5f6YfIjDlfScrDYLk4P3KTi0tFD/IFLWrSsk4lpi3ejqpCaBXocrO7/fTRhECHZ0yJMm7cODp16kT79u2JiIjgqquuCnRIpghpXTeMVaktqHDkF0g6HOhwjCmxLBfnTyCneSs0IsKo6AgeXriOZdsO0qtZDTfMYvkUWPEqXDwu0CEaU+SFh4dn22MBZHgHowkTJjB//vzzlg0dOpR//OMfBRZfmhkzZvDiiy+etyw6OpqXXnqpwI9lcq9WWDl+KdcGUoFfY6D5xYEOyZhixXJx4SgVQyzAXajX++n/0aVxVabeEuUWzrsZtn0D92+EcpUKNlBjKHmn9kze2BCL841+5Ste230tQX0fgv5/L+DIjDmf5WGTxoZYZCA0JJgRPRrz1ea9bN9/3C3sda8bjxz7ZmCDM8aYUqRJ/Xr8rI1Qm8nCGFNElZoCGWBEryaEBAUx44ftbkGjKGjY3btxyJnABmeMMaVE63phrDzTnNT4GLthiDGmSCpVBXLtsFAGd6rP/Jh4jpxIdgt73QOHtsOWTwIbnDHGlBJt6lZmtTYn+PRR2P9ToMMxxpjfKVUFMsCo6AiSks8w98edbkHrQVC1MSwtHoPGjcmPcePGZXjhRn7ddtttbNy40S/xvPfee+ft+/HHH+fLL7/M9bFmzpzJmDFjcr2dKXgt6lRitbZwT+JtmIUpfSwXF/1cXOoK5Lb1K9O7WQ1mLYkj+UwqBJeBHnfBziXwq83JaUxeTJ06lbZt2/pl3+mT8pNPPsnFF/t/5oOsbpVq8ic0JBiqN+d4UBjE/xjocIwpMSwXF5xSMc1beqP7RDB6VgyfrN/D4Mj60OUm+Oafrhd5yPRAh2dKqk8ehj3rCnafdTvAwKezbDJhwgRmzZpF7dq1adSoEV27dmXixIlMmTKFMmXK0LZtW956661sDxUXF8eAAQPo2rUrq1atol27drz++utUqFCBfv368fzzz9OtWzemTZvGM888Q9WqVYmMjKRcuXJMmpT9rd1fe+01Xn31VU6fPk3z5s154403iI2N5YMPPuDbb7/lqaeeYsGCBYwfP55BgwYxZMgQwsPDueWWW1i0aBHJycnMnz//vDs/ZWbRokU89dRTnD59mho1ajBnzhzq1KnDuHHj2Lp1K9u2baNx48ZMnDiRG2+8kYSEBHr16sUXX3zBypUrqVmzJrNnz2bixImcPn2aHj16MHnyZIKDg7M9tnFa1a/Kul9a0HOXFcimEAUoD4Pl4owU5Vxc6nqQAfq3qk3TmhWZ9v02d+OQcmHQ9RbY8B4c3hXo8IwpMCtXruStt94iNjaWjz/+mB9/dMXI008/zerVq1m7di1TpkwB4Ouvv6ZTp06/e/Tu3fvs/rZs2cLdd9/Npk2bqFy5MpMnTz7veAkJCYwfP55ly5bxww8/sHnz5hzHes011/Djjz+yZs0a2rRpw7Rp0+jduzeDBw/mueeeIzY2lmbNmv1uu5o1a7Jq1SruuuuuHJ+y7NOnD8uWLWP16tUMGzaMZ5999uy6jRs38uWXXzJ37lyeeOIJLrzwQjZs2MCQIUPYudMNzdq0aRNvv/02P/zwA7GxsQQHBzNnzpwcv1YDbeqGseRUU3TfZjebkDElmOXijBXlXFwqe5CDgoRbo8N57P0NrNxxiG7h1aH7/8HSybDiFbj0qUCHaEqiHPQwFLTvv/+eq6++mgoVKgAwePBgADp27Mjw4cO56qqrzt5dqX///sTGxma5v0aNGhEdHQ3AiBEjmDhxIg888MDZ9StWrKBv375Ur14dcBPQ//RTzi7CWr9+PY8++iiHDx/m2LFjXHbZZTna7pprrgGga9euLFy4MEfbxMfHc/3117N7925Onz5NRETE2XWDBw+mfPnyACxevJh3330XgAEDBlCtWjUAvvrqK1auXElUlJtTPSkpidq1a+fo2MZpXbcyM7UFgkJ8DDS/KNAhmdIgAHkYLBdnpijn4lLZgwxwbdeGVCkfwrTF3pRvVRtBu6tg5Sw4eTSwwRnjZx999BH33HMPq1atIioqipSUlBz1WojIeftJ/zw/Ro4cyaRJk1i3bh1jx47l5MmTOdquXLlyAAQHB+d4rNq9997LmDFjWLduHa+88sp5x6pYsWK226sqt9xyC7GxscTGxrJlyxbGjRuXo2Mbp3W9MNakNkMRVyAbUwpZLi66ubjUFsgVypbhhu6N+WzDHnYdPOEW9roHTh2F1bMDG5wxBeQPf/gD7733HklJSSQmJrJo0SJSU1PZtWsX/fv355lnnuHIkSMcO3bsbK9F+seSJUvO7m/nzp0sXboUgDfffJM+ffqcd7yoqCi+/fZbDh06REpKCgsWLMhxrImJidSrV4/k5OTzTpGFhYWRmJiYz3fifEeOHKFBgwYAzJo1K9N20dHRzJs3D4DPP/+cQ4cOAXDRRRfxzjvvsHfvXgAOHjzIjh07CjTGkq5B1fJQrjJ7Q8NtJgtT4lkuzlhRzsWltkAGuKV3E4JEmLkkzi1o0BUa94blL8MZu4LdFH9dunTh+uuvJzIykoEDBxIVFYWIMGLECDp06EDnzp257777qFq1ao7216pVK1566SXatGnDoUOHuOuuu85b36BBAx555BG6d+9OdHQ04eHhVKlSJUf7Hj9+PD169CA6Ovq8izuGDRvGc889R+fOndm6dWvOX3wWxo0bx9ChQ+natSs1a9bMtN3YsWP5/PPPad++PfPnz6du3bqEhYXRtm1bnnrqKS699FI6duzIJZdcwu7duwskttJCRGhdL4x10srNZGE3DDElmOXijBXpXKyqxerRtWtXLUj3zV2l7R7/VI8mnXYLNi5SHVtZdf27BXocUzpt3Lgx0CEUmO3bt2u7du2ybZeYmKiqqsnJyTpo0CBduHChv0Pzm5MnT2pycrKqqi5ZskQjIyPztJ+Mfg6AGC0COTWvj4LIxY++u04fe/xvLufu3Zzv/RmTkZKUh1UtFxdWLi7VPcjgpnw7diqFeTHxbkGrgVAtwm4cYkwejRs3jk6dOtG+fXsiIiLOXnhSHO3cuZOoqCgiIyO57777eO211wIdUonSul4YS043dU9sHLIxBcpycf6UylksfHVsWJWo8GrM+GE7I3uHExwUDD3vhk8ehF0roFH3QIdoTJEQHh7O+vXrs22X0fQ+EyZMYP78+ectGzp0KP/4xz8KLL40M2bM4MUXXzxvWXR0NC+9lPsvvS1atGD16tUFFZpJp3XdymzV+qSEVKJM/I/QeXigQzKmyLNcXDjE9S4XH926ddOYmILtafh0/W7unL2Kl4d3YWCHenDqGPynLTTtB9e9XqDHMqXLpk2baNOmTaDDMAGW0c+BiKxU1W4BCinfCiIXHzuVQvuxn/Fd3RdpXO4E3LW4gKIz5hzLwyZNbnJxqR9iAXBJ27o0ql7+3JRv5SpBt1GwaREcigtobKb4K25fQk3Bss8/c5XKlaFx9QqsD2oJeze4zglj/MB+D01ufwasQAaCg4SRvSOI2XGINbsOu4Xd7wAJguWvBDY4U6yFhoZy4MABS86llKpy4MABQkNDAx1KkdW6bhjfJ4WDpkKCDWcxBc/ysMlLLvbbGGQRmQ4MAvaqavtM2vQDXgBCgP2q2tdf8WTnum4N+c8XPzFt8XYm3tAZKteH9tfCqteh38MQmrPpUYzx1bBhQ+Lj49m3b1+gQzEBEhoaSsOGDQMdRpHVul5lZm9qyD/LAb/GQMQFgQ7JlDCWhw3kPhf78yK9mcAkIMNBvCJSFZgMDFDVnSIS0Pu0hoWGMCyqETOXxPH3P7amXpXy7mK9tW+7u+tF3xfI8EwxFRISct6tM40x52tbL4yDGsbJsHBCbSYL4weWh01e+G2Ihap+BxzMosmNwEJV3em13+uvWHLqlt7hpKoya4l3F5b6nSD8AjfM4kxyYIMzxpgSqFfTmgQHCVvLtXY3DLHT4MaYIiCQY5BbAtVE5BsRWSkiN2fWUETuEJEYEYnx5ymSRtUrMKB9Xeau2MmJ096d9HqNgaPxsPF9vx3XGGNKqyoVQogKr8bXx8Lh2G9wZFegQzLGmIAWyGWArsDlwGXAYyLSMqOGqvqqqnZT1W61atXya1Cj+0RwJCmZBSu9G4e0uBRqNIelk6xnwxhj/ODiNnX49Ig3NjD+x8AGY4wxBLZAjgc+U9Xjqrof+A6IDGA8AHRpXI3IRlWZ/kMcqakKQUFuLHLCati5NNDhGWNMiXNRmzps1sakBJWzO+oZY4qEQBbI7wN9RKSMiFQAegCbAhgPACLC6D4RbN9/nP9t9oZFR94A5avZ7aeNMcYPImpWpHHNKmwt08J6kI0xRYLfCmQRmQssBVqJSLyIjBaRO0XkTgBV3QR8CqwFVgBTVTX7eycWgoHt61KvSui5G4eUrQDdRsPmj+DA1sAGZ4wxJdBFbWrzfVI4unstpJwKdDjGmFLOn7NY3KCq9VQ1RFUbquo0VZ2iqlN82jynqm1Vtb2qvuCvWHIrJDiIkb3DWbrtABsSjriF3e+A4BBYPiXrjY0xxuTaRW3qEHOmOXLmFOwpEn0lxphSzO6kl4lh3RtToWww0xfHuQVhdaDDUFg9G5IOBTQ2Y4wpabo1qcbWsq3dExtmYYwJMCuQM1GlfAhDuzZk0ZoE9iaedAt73g3JJyBmRmCDM8aYEqZMcBBtW7dmDzVItQLZGBNgViBn4dboCJJTU5m91LtxSN320LQfrHgVUk4HMjRjjMkREZkuIntFJMNxCyJSRUQWicgaEdkgIrcWdoxpLmpTh5VnmpEctzxQIRhjDGAFcpbCa1bkotZ1mL18JyeTz7iFvcZA4m7Y8G5ggzPGmJyZCQzIYv09wEZVjQT6Af8SkbKFENfv9G1Zi7XagnLHdsGxgN9c1RhTilmBnI3RfSI4ePw0767+1S1ofjHUam03DjHGFAuq+h1wMKsmQJiICFDJa5tSGLGlV6V8CKfqdHFPbD5kY0wAWYGcjZ5Nq9O2XmWmL96OqoKIG4u8Zy3ELQ50eMYYk1+TgDZAArAO+JOqpmbUUETuEJEYEYnZt2+fX4Jp0qEXyRrM0V+W+GX/xhiTE1YgZyPtxiE/7z3Gdz/vdws7XgcVarpeZGOMKd4uA2KB+kAnYJKIVM6ooaq+qqrdVLVbrVq1/BJMv/bhbNLGHNtm45CNMYFjBXIOXBFZn9ph5c7dOCSkPETdBj99Cvt/DmxwxhiTP7cCC9X5BdgOtA5UMBE1K7KtXBuqHVoHqWcCFYYxppSzAjkHypYJ4uZeTfjup338/FuiWxh1GwSXg2WTAxucMcbkz07gIgARqQO0ArYFMiBpFEV5TeJ4vN0wxBgTGFYg59CNPZpQrkwQ03/wepEr1YLI6yF2Lhw/ENjgjDEmEyIyF1gKtBKReBEZLSJ3isidXpPxQG8RWQd8BTykqvsDFS9Ak459AdgW+00gwzDGlGJWIOdQ9YpluaZLQxau+pUDx065hT3vhpQkWDk9sMEZY0wmVPUGVa2nqiGq2lBVp6nqFFWd4q1PUNVLVbWDqrZX1dmBjrl9+04cIowT25YFOhRjTCllBXIujO4TzqmUVOYs3+kW1G7jpn1b8RqknApscMYYU0KUKRNMQsV21D4cS8qZDCfUMMYYv7ICORea1w6jb8tavL50B6dS0m4ccg8c+w3WvRPY4IwxpgSRZv2JIIF1a1cHOhRjTClkBXIuje4Twf5jp1i0Zrdb0LQ/1G4HS1+yG4cYY0wBCY++DoB9MQsCHIkxpjSyAjmXLmhRk5Z1KjHN98Yhve6BvRtg2zeBDs8YY0qECnWasqNsC+onfOFyrTHGFCIrkHMp7cYhm3YfZek2b/aKDkOgYm27cYgxxhSgoxEDaK8/sfmnnwIdijGmlLECOQ+u7NSAGhXLMj3txiFlykH3O+CXL2HvpsAGZ4wxJUSj3tcD8Osyu8bDGFO4rEDOg9CQYIb3bMJXm/eyff9xt7DbKChT3m4cYowxBaRq4/b8WqYh1Xd+FuhQjDGljBXIeXRTzyaEBAUxI+3GIRVrQKcbYM3bcGxfYIMzxpiSQIQDDS+lY8o64nbtCnQ0xphSxArkPKoVVo7BneozPyaewydOu4U974YzpyBmWmCDM8aYEqJuz6GUkVS2/WCzWRhjCo8VyPkwKjqCpOQzzF3h9WzUbAEtB7gbhyQnBTY4Y4wpAWq36sW+oJpU3PZJoEMxxpQifiuQRWS6iOwVkfXZtIsSkRQRGeKvWPylbf3K9G5Wg1lL4khOu9tTr3vgxH5YOy+wwRljTEkgQkLdi4g8tZLf9h8IdDTGmFLCnz3IM4EBWTUQkWDgGeBzP8bhV6P7RLDn6Ek+XufdOCT8AqjbwW4cYowxBaRGt2sJlWQ2f/9uoEMxxpQSfiuQVfU74GA2ze4FFgB7/RWHv/VvVZumNSsy/bwbh9wL+7fAL18FOjxjjCn2GkReyBHCKPPTh4EOxRhTSgRsDLKINACuBl7OQds7RCRGRGL27StaM0QEBQm39olgTfwRVu445Ba2uxrC6tmNQ4wxpgBIcAhxtfrR8cQyjiQeD3Q4xphSIJAX6b0APKSqqdk1VNVXVbWbqnarVatWIYSWO9d2aUCV8iFMO3vjkLLuxiHbvoY9WQ7BNsYYkwOVIq8iTJJY/8MHgQ7FGFMKBLJA7ga8JSJxwBBgsohcFcB48qxC2TLc2KMxn23Yw66DJ9zCriMhpILdOMQYYwpARPfLOU4oqRsWBToUY0wpELACWVUjVDVcVcOBd4C7VfW9QMWTX7f0CidIhBk/xLkFFapDp+FuNovEPQGNzRhjirugsuXZVjWaNkcXk3TydKDDMcaUcP6c5m0usBRoJSLxIjJaRO4UkTv9dcxAqlsllMs71mNezC4STya7hT3vgtQU+HFqYIMzxpgSIKTdYGrKEdYuK7YTHxljigl/zmJxg6rWU9UQVW2oqtNUdYqqTsmg7UhVfcdfsRSW0X0iOHYqhbd/9G4cUqMZtL4cfpwGp08ENjhjjCnmmkVfzSlCELtbqTHGz+xOegWoY8OqRIVXY+aSOM6kenMg97oHkg7CmrmBDc4YY4q5kApViGlwM92P/Y/9S+cEOhxjTAlmBXIBG92nKfGHkvh8gzfuuHEvqN/ZXayXmu2EHcYYY7LQfOiTrEptQaUvHoRDOwIdjjGmhLICuYBd0rYOjatXODflmwj0GgMHfoGfbdycMcbkR52qlfi4xXiSz6SS8s7tcCYl0CEZY0ogK5ALWHCQMLJ3ODE7DhG767Bb2PZKqNzQbhxijDEF4KoLe/No8q2U+XU5fP98oMMxxpRAViD7wXVRjQgrV+ZcL3JwCPT4P4j7HnavCWxwxhhTzLVvUIW94VfyaVBf9NtnYOeyQIdkjClhrED2g0rlynB9VCM+XrebhMNJbmGXm6FsJVj6UmCDM8aYEuC2CyJ44MRNnChfHxbcDiePBDokY0wJYgWyn9zSOxxVZdbSOLegfFXofBOsXwBHEwIZmjHGFHv9W9Wmds1ajAv5M3r0V/jwL6Aa6LCMMSWEFch+0qh6BQa0r8vc5Ts5fsq7iKTnnaCpsOS/gQ3OGGOKuaAgYVSfCOb/Vp9fO90H69+x6zyMMQXGCmQ/Gt2nKUdPprBgVbxbUC0cIm90U779b4L1dhhjTD5c26UhVSuE8NSRgdDuavj8UYiZEeiwjDElgBXIftS1STU6NarKjB/iSE27ccgVL0DnEfDds/D+PXAmObBBGmNMMVW+bDAjejThs0372dH3P9D8EvjwflhX7G/MaowJMCuQ/Wx0nwi27z/O/zbvdQuCQ2DwJOj3d4idA29eD6cSAxukMcYUUzf3akKZIGHGsgS47nVo0hve/T/Y8snvG6echpjp8EJH19tsjDGZsALZzwa2r0v9KqHnpnwDd/OQfg+7QnnbNzDjj5C4J2AxGmNKLhGZLiJ7RWR9Fm36iUisiGwQkW8LM778ql05lMGRDZgXs4sjKSFww1tQtwPMuwW2eS/lTAqsegMmdXU9zCcOQsxMOH08oLEbY4ouK5D9rExwELf0DmfptgNsSEg3DVGXm+DGt+HAVph6Cez7KTBBGmNKspnAgMxWikhVYDIwWFXbAUMLKa4CM6pPOCdOn+GdVfEQWhlGLITqTWHuDbD4P/BSFHwwBirUgOHvwA1z4XQibPow0KEbY4ooK5ALwbDujalQNvj8XuQ0LS6BkR9CShJMv9QmvDfGFChV/Q44mEWTG4GFqrrTa7+3UAIrQO3qV6FL46rMWb4DVYUK1eHm96BSbfhyHIRUhGFz4favXc5tEg1VG8OaNwMdujGmiMpRgSwiFUUkyPt/SxEZLCIh/g2t5KhSPoShXRuyaE0Ce4+e/H2DBl1g9BdQvjrMGgwbPyj8II0xRZ6fcnFLoJqIfCMiK0Xk5iyOf4eIxIhIzL59+/J52II1vEcTtu07ztJtB9yCsLow6jPXm/x/30HrP7rhbQBBQW5GoW3fwpH4wAVtjCmyctqD/B0QKiINgM+Bm3Cn7UwO3RodQUqq8sayHRk3qB7hiuR6kTDvZlj+SuEGaIwpDvyRi8sAXYHLgcuAx0SkZUYNVfVVVe2mqt1q1aqVz8MWrMs71qNK+RDmLN95bmFYHWh+kSuI04scBiiseavQYjTGFB85LZBFVU8A1wCTVXUo0M5/YZU84TUrcnGbOsxZvpOTyWcyblSxBtzyAbS+HD75G3z+GKSmFm6gxpiizB+5OB74TFWPq+p+XBEemc99FrrQkGCGdG3IZ+v3sC/xVPYbVI9wQy3WzLU56Y0xv5PjAllEegHDgY+8ZcH+CankGt0ngoPHT/Pu6l8zbxRS3k1VFHU7LJkIC2+DlBwke2NMaeCPXPw+0EdEyohIBaAHsCmf+wyIG3s0JiVVmRezK2cbRN4AB36B+B/9G5gxptjJaYH8Z+DvwLuqukFEmgJf+y+skqlHRHU6NKjC059s5se4LK6ZCQqGPz4HF4+D9Qtg9rWQdLiwwjTGFF25zsUiMhdYCrQSkXgRGS0id4rInQCqugn4FFgLrACmqmqmU8IVZc1qVaJX0xrMXbGTM6k56BVudxWEVIBYu1jPGHM+0VyeWvIuEKmkqkf9E1LWunXrpjExMYE4dIHYdfAEt8xYQfyhJCYO68SA9vWy3mDtPHjvbqjZAobPhyoNCydQY4xfichKVe2Wj+0tF2fgw7UJjHlzNTNujaJ/q9rZb7DwDtjyKTywxZ3BM8aUKpnl4pzOYvGmiFQWkYrAemCjiDxY0EGWBo2qV2DBnb1pX78yd81ZxcwfMpj6zVfH62DEO+5K66mXwG8bCidQY0yRY7k4e5e2rUvNSmWZs2xn9o0BOt0Ip47Alo/9G5gxpljJ6RCLtl4vxVXAJ0AE7urpTGV39yYRGS4ia0VknYgsEZFid1FIXlWrWJY3b+/JJW3qMG7RRv758SZSszod2LQf3OrdNnX6gHN3hzLGlDa5zsWlTdkyQVzXrRH/2/wbCYeTst8g/A9QuaENszDGnCenBXKIN9fmVcAHqpoMZDc2YyZZ3L0J2A70VdUOwHjg1RzGUiKEhgTz8oiu3NSzCa98t43758VyOiWLGSvqtofbvoDKDdyY5LXzCy9YY0xRkZdcXOrc0L0xCrz1Yw4u1gsKgsjrYev/4Ohuv8dmjCkeclogvwLEARWB70SkCZDluLfs7t6kqktU9ZD3dBlQ6gbXBgcJT17Zjr8NaMX7sQmMnLGCoyeTM9+gSkMY9Sk06uFmt1j8gk1PZEzpkutcXBo1ql6Bvi1r8daKnSSfycFUmZE3gqbCunn+D84YUyzkqEBW1Ymq2kBV/6jODqB/AcYxGne6sNQREe7u15x/XxfJiu0HuW7KUvYcyeBue2nKV4WbFkK7a+DLsW6+5NRM5lU2xpQohZCLS4zhPZqwN/EUX23KwZ2zazaHht3dMAvrdDDGkPOL9KqIyL/TbjEqIv/C9WDkm4j0xxXID2XRpsje3rSgXNOlITNujWLXwRNcM/kHfv4tMfPGZcrBtdOg1xhY8aq7815yDsbaGWOKNX/m4pKmf6ta1KsSyhvL4sjRbE2dboR9m2HnMv8HZ4wp8nI6xGI6kAhc5z2OAjPye3AR6QhMBa5U1QOZtSvKtzctSBe0qMW8O3uRnKpc+/ISlm/L9C1x4+YumwADnobNH8HrV8KJLOZWNsaUBH7JxSVRmeAgRvYO54dfDjBtcTazBQG0v9Zd47Hwdji+3/8BGmOKtJwWyM1UdayqbvMeTwBN83NgEWkMLARuUtWf8rOvkqRd/SosvKs3tcLKcdO0FXy0NpuLRnreBdfNgoRYmHYJHIorlDiNMQFR4Lm4JLv9gqYMaFeXCR9v4tP1e7JuHFoZrp8Nx/bC/JFwJqVQYjTGFE05LZCTRKRP2hMRiQayPKef3d2bgMeBGsBkEYkVkaI343yANKpegQV39aZjwyqMmbuK6dn1frS9Em5+3/V6TL0EElYXTqDGmMKW61xcmgUFCf+5vhMdG1blz2+vJnZXNnckbdAFrngR4r6HLx4rnCCNMUVSju6k581R/DpQxVt0CLhFVdf6MbYMFdW7N/nDyeQz/OmtvLilkQAAIABJREFU1Xy24TduvyCCvw9sQ1CQZL7Bvi0wewicOADXvQ4tLi68YI0xuZKXO+lZLs6bfYmnuHryD5xMPsO7d0fTqHqF37U5lXKGdfFHaN+gCqFfPgLLp8DVr0DksABEbP4/e/cdVtWVNXD4t+nSRDqIoCJWFAv23rvRRBONmkQTTTW955tkMplkMplJJs0ejSYmRmNJ7C2xd1ERsKIogiBFQXq7+/tjYyx0BC7gfp/nPsi955y7juK5i33WXlvTqso9raQnpQyWUgYAbYA2Usp2QL8KjlG7i5W5KbMmduDxrj7M3x3BS8uOk5VbTMcKl2aqV7JTY/j5YTi2pOqC1TSt0ulrcfm42FmyaEpHsnMNTF10mOSMW+00ryRl8J/Np+n2rz8ZO2c/o2fu5VzAW+DTA9a+VOY7cqWaEKhpWrVX2hILAKSUN/JXcQJ4tRLi0e5iaiL4+6hWvD20OWuDr/D4wkN3XNwLsHOHJzZAo17w+/Ow49+6bZGm1TL6Wlx2TVztmDO5AxcT03h2SRB7ziXw9I9H6PHvP5m94zztferxjwdaEZ+SxYhZB/nV959IG2f4ZRKklq570p5zCbT7aCvPLgniUmJaJZ+RpmmVqVQlFoXuKMRlKWWDCo6nRDXptl5F++1YNG+sCKaxsy2LpnbEo26dojfOy4E1L0Lwz9D+MRj+PzA1q7pgNU0rVnlKLIo4jr4Wl8GKoChe/zUYgHrW5ozv5M3Ezt541VNlF3Epmby2PJjd5xJ4qkky78W+gnBqAo37gK0r2Lqprzau6q6dqTkAa4Kv8Nry43g61CE+JYucPANPdGvIC339qGttbqSz1TStJEVdi+8lY9LDklVsdLv6uNhZ8vSPQTw4ax+LpnSimbtd4RubmsPoWVC3Puz6D6TEwtjvwdK2aoPWNK2y6WtxGYzt4IWZiSDPIBnexgMrc9M7Xne1s2LxlE4s2BPBZ5tPk1ZnBu+nL6fO4QWQe9d8yOYjYPxPLNgTwUfrTtKpkSPzHwskKyePz7ec5bs9EfwaFMXL/f2Y2MUHc9My3bTVNM2Iih1BFkKkUPjFVwB1pJRVPiRZU0ctKtLJKzd44vtDZOTkMf+xQLo0dip+hyMLYf1r4BEAjy5Xox+aphlVWUaQ9bXYOEKiknnxl2NEJKTR1qsuT3R0YWhDgWVmApxYDkHfM6/NMj45lMeQVu58Ob7tHQl32JVkPl5/in3nE2nmZseq57phY6nv5GladVKuSXpSSjsppX0hDztjXJA1paWnPaue64abvRWPLTjE2uArxe8QOBXG/wxxp1Wv5ITwqglU07QKoa/FxtHaqy7rX+zB+yNakpKVy8urw+k05yIfhThw3v8lcoU55kcXMrGzNzMnti8wGt3Ksy4/PdWZ/z0SwJmrKawp6VqtaVq1oe/31FBe9axZ8UxX2jZwYMbSY3y3+0LxOzQbCk+sh6wUlSRfPlw1gWqaptVg1hZmTO3RiG2v9ubnaZ3p0cSZxfsu0n/uSdbmduJRyz38c1hDTItowSmEYHTb+jR1s+WXQ5FVHL2maeWlE+QazMHagh+e7MSw1u78c/0pPlp3EoOhmHJErw7w5FawqguLR6glqjVN07QSCSHo5uvMzInt2fdOP94a0hynvs9jmZeGOLG8xH3Hd/QmOCqZk1duFLutpmnVg06Qazgrc1O+mdCeJ7o1ZMGeCGb8cozMnGJ6JTv5qiTZrRUsmwR7vgRDMdtrmqZpd3C1s+LZPr706jsM3NvA4e9KbKf5YPv6WJiZ8MthPYqsaTWBTpBrAVMTwQcjW/LusOasPxHDYwsPkZxeTK9kWxd4fK2agb3tA/h+GCSer7qANU3TagMhoNM0iDsJl/YVu6mDtQXD/N1ZfSyajGw9KKFp1Z1OkGsJIQTTe/ny1fi2HIu8zri5+7iSlFH0DhY2ajnqMXMh/hTM7g4H54LBUHVBa5qm1XT+Y8HKAQ7PL3HT8Z28ScnMZX1ITBUEpmnavdAJci3zQNv6LJ7SiZikTB6ctY/TscXUuwkBAePhuQPQsAdsfBN+GAXXL1ZZvJqmaTWahTW0mwSn1qp+88Xo3MiRxs42JU7WO3zxGtHFDXBomlbpdIJcC3Vr4szyZ7oCMG72fvadTyh+B3tPmPgrjPoGrhxXo8lHFuolqjVN00ojcCoYciFoUbGbCSEY36kBRy5d5+zVlEK3WX8ihnFz9tPrs+3MWHqME1FJlRCwplVvl6+lM3/XBbJzjXdXWyfItVQLD9Ur2cPBiscXHiq5/6YQaknq5/aDVyCsewV+HAPJUVUTsKZpWk3l5AtNBsCR7yGvkPkfMcFwYDbk5fJQey/MTQW/HLpcYLPQ6GRe+/U4HXzq8WSPRuw4Hceob/fy8Jz9bAmLJa+4LkV3WzEVgpfdw0lpmvH8b+tZPt5wiscXHiI5o5g5VZVIJ8i1mKdDHX59uhvtvOvx4tJjzNt1nuJWTgTAoQFM/g2GfwGXD8GsrnD0Rz2arGmaVpyO0yA1VpVa3JR4Hn6dAnN7waa3YdsHONlaMqiVO6uORd3RcSguJZNpPxzB0dqCOZM68O6wFux7px//N7wF0UkZTP8xiMFf7iIhNavkWG7EQOhKODSvEk5U0ypXdq6Bbaeu0szNjiOXrjFuzj6jlBzpBLmWq2ttzg9TOzG8tQefbDjNh2tPljwKIQR0fBKe3ataGK15AX5+RF10NU3TtIL8BoKDt2r5diMG1r4M33aEs5ug1xvQYQrs/xZO/MqEjt4kpeewOUzVLGfl5vHMj0Ekpecw//FAXOwsAbCzMuepno3Z+UYf/vdIAOfjU1m4J6LkWC4fUF+jgyCthBI7TatmDlxI5EZmLq8PbvbXnKoxM/cSdiW5SuPQCfJ9QPVKbsfU7o1YtO8iM5YeLb5X8k2OjVQ7uCH/hohdMKuzumWnR5M1TdPuZGIKgU/Cpb3wdVs4tkQNNLwUDP3+D4b9B3y6w5oX6GZ9GW9Ha5YeikRKyXurQzkamcTnDwfQyrNugUObmZowpp0XQ/3d+XH/JW5klnDLOTI/QUZC+LaKP1dNq0SbwmKxtjClp58z3Zo4s+LZbpiaCB6es5+dZ+OrLA6dIN8nTEwE749syf8Nb8GGkFgeW3CIpPTs0uwIXZ5Ro8kuzWH1dLXASGpc5QetaZpWk7R/DJyaQItR8MJhlRTbuqrXTM1h3GKwdsZk+WQeD7DlwIVr/H1NGCuConipvx/DWnsUe/hnevuSkpXLzwdLWGwk8oBKxm1c4dyWCjo5rbpIzcplS1hsySWTRiKlLN0gXCHyDJItYbH0beaKlbkpAM3c7Vj9XHe8nWyYuugwn285w5nYlEo/f50g32ee6tmYrye04/jlJMbO2c/la+ml29HJF6ZshIEfwbmtMLMzhK6q3GA1TdNqEmtHmBEED81Xd+DuZusC45dAWjyTL7+PpUkei/dfYqi/Oy/19yvx8G28HOjRxJkFeyKKTkCyUiE2BLy7qLKP8G2Ql3uPJ6ZVJ7N3hDP9xyD2hicaO5RC/e33UHp+tp3E0tTL3yXo0nUSUrMZ4u9+x/Puda349Zmu9G/uyjd/hjP4y130/s8OPlp3kv3nE8nNq/huFzpBvg+NCvBk8dROXL2RyeAvdzF/1wVySvPDZWIK3V+EZ3ZDvYawYgosfxzSqud/Uk3TtGrHsx2M/AqLqH0s9FxDoE89Pn84ABMTUardn+3jS3xKFquPRRe+QXQQyDxo0AX8BkFmMkQdrrj406+Rt2QcUedOcCjiGutOXGHhngg+3XiaL7aeLd2dyQpy8EJi8avG1kJ5Bsmqo+rfftaOcCNHU9CxyOssORBJfEoWH284Veb9N4XGYmFqQt/mrgVes7U0Y95jgRx8tz8fj/HH18WGHw9cYsL8AwR+vI3fivo/UU5mFXo0rcbo6uvEhhd78vc1YXy84RQrj0bx8ZjWdPCpV/LOLs3gya2w7yvY/i9VczfiS2gxovID1zRNq+kCxkNMMN0PzKJbRwfErq2QnapGf7NTITdLlWsUck3t5utEG6+6zN15nocDG2B6d2J9+aD62qAjCBMwMVNlFj5dKyT0yC3f4h2+hUWnXPgub/hfz1uYmpBrMPDTgUu8M6wFD7WvjxClS/rL40RUEo/MO8DkLj58NNq/0t6nutl3PoGY5Ew6NXRk3/lEjkVep513KT63q0CeQfL+72G42Vsy1N+DRfsuMraDF918nUu1v5SSzWGx9PRzxtay6PTUzd6KiZ19mNjZh7SsXHadjWfryat41atTUacC6BHk+1oDR2u+ezyQuZM7kJyRw0Oz9/HOqpDSjQCYmkHP1+DpnWDnAcsmwsppkH6t8gPXNK3UhBALhRBxQojQErbrKITIFUKMrarY7msDPwK/wYjD82HvV3BimZoMnXAW4k6puR77ZxbYTQjBs719uZiYzqbQQlbuizwAri2hTj2wqgveXSusDjkyLhmr4wsBmNwohR+f7MTml3tx7G8DOfPPIax/sSc+Tta8/msw4+cd4FwRi6HcKyklH69Xo5PrQ2JKdwe0llgZFIW9lRlzJnegbh1zZu04X67jZOca+OPU1Qr9u1t2+DIh0cm8O6wFbw1pTgPHOvzfb6Fk5ZauHjkkOpnopIwC5RXFsbE0Y2hrD754pC2BDR3LG3qhKi1BLumiLJSvhRDhQogTQoj2lRWLVjQhBINbubPt1d5M69mI5Ucu0//znawMiipdAbxbK5j2J/R5B8JWqb7JZzdXfuCappXWImBIcRsIIUyBfwN6RldVMTWDR5fBe7HwfiK8HQmvhsHzB+GFQ2r0ePO7sPFtMNyZYAxq5U4jZxtm7wy/8zptyFPlFA0633rObyBcDYXke7v9nJKZw5Lvv8KV6+RZ1sMnN4Kefi40c7ejno0FQghaeNiz4plu/OvB1pyOTWHoV7v5bNNpMrLLN2GrKNtOxXEw4hr9mrtyLS2bPeH3Ryu7lMwcNoXFMjLAE0cbC57o1pCtJ69yJrbsv4h8uz2cJxcfYdyc/UQmlnIuUjGup2Xz2ebTdGrkyKgAT+pYmPLRA/5ciE9j3s4LpTrGptBYTE0EA1q43XM8FaEyR5AXUfxFeSjgl/+YDsyuxFi0EthYmvHe8Jasm9EDHydrXvs1mAnzDxAeV4r/eKbm0OdteOoPsHaCnx+G355XtW+aphmVlHIXUNKtnRnASkC3p6lKQoB5HfX1duZ1VMeLzs/Cwdnw6+OQc2uhBFMTwdO9GhMafePO5DDuJGTdAO8uJKZmkZaVq+qQAcK3ljvMPIPk5V+OMzTtdzLsGmHaYTLEnyl01UATE8GETt78+VpvHmhbn1k7zjNpwcEKm0SVm2fg042naOxiw7ePtqNuHXPWHC9hpdhaYkNIDJk5BsZ28ALgiW4NsbYwZc7Oso0iJ6ZmsWD3Bdp41eVCfCrDvt7N6mP3tmru51vPkJKZy4ejWv1VWtOnmSvD23jwzfZwLiakFbu/lJJNobF0aexIPRuLe4qlolRaglyKi/IDwA9SOQA4CCGK73GjVbrbRwBOxagRgP9sLuUIgGdbmL4der4OwUvVaHL4H5UftKZp5SaEqA+MoRSDFEKI6UKII0KII/HxVdeP9L5kYgpDP4XBn8CpdfDDA3dMiB7Tvj6udpZ3Jkf5/Y//dsyGjh9vI+DDLYxdcY1kSw+uHV9Hdm75ktTPNp8m8cw+2pmEU6fHc2oBqbxsSDhX5D5OtpZ8/nAAXzwcQNCl63z1R9HblsUvhy9zPj6Nt4c0x9rCjGGt3dkcFkt6du3v1LEiKApfFxvaNnAAoJ6NBY928mZN8JXSd6QCZm4/T0ZOHl883JYNL/WkubsdrywL5pVlx0kpqcd2IUKjk/npYCSTu/jQwsP+jtfeH9ESS1MT/vZ7aLF3pc/FpXIhIY0h/tUnDTRmDXJ94PbF6KPynytAX5Sr1s0RgD9e682ogPrM3H6eQV/uZPvpUgwumVlC/7/BU1vBwhaWPKhWlMqqnFo0TdPu2ZfAW1LKErMnKeU8KWWglDLQxcWlCkLT6Po8jFsEV47DwkGQcR0ASzNTnurZiL3hiRyNvM66E1fY+cc64qQDv180Z1rPxkzv1ZgcCWvTW2EZuYvAD9cz5ftDJY7m3W5lUBRzd17gQ7ddYGkPbSeo0jqAq2El7v9gey/GdvDi2+3hHLhwbx2PUrNy+XLbWTo1dGRgS3UbflRAfdKz89h2qvjPpzyDVCPqNdTFhDQOX7zOQx287pj8+FTPxpgImLurdKPI0UkZLDlwibEdvGjiaotXPWt+md6Flwf48fvxaIZ/vYfjl5NKHZfBIHn/91CcbCx4ZWDTAq+72Vvx+uBm7D6XwNoTRa/Guyk0FiFgcMvqUV4BNWSSnr4oG4dz/gjA0mldsDA1Ycqiwzy7JIiY5FKsiV6/Azy9C7q9CEGLYHY3NQFF07TqJhD4RQhxERgLzBJCjDZuSNodWo2GSSsgMRwOL/jr6QmdvLG3MmPcnP288PMxmmWHke7ekf3vDOCdYS14c0hzfn++O2MenoKNyOL1Zgkcy++BfyrmRolvG3TpOu+sCmF4Q0mbGzug3WSwtANnPzAxV7XNpfDhqFY0dLLhlWXH76kN3Nyd50lIzebd4S3+ShI7NXLE3d6KNceLr7F+7qcgBn+5q9wLWBjbqqNRmAh4sJ3XHc+717VibAcvlh+JIi4ls8TjfL1NjeS/NOBWMmtmasLLA5qy/Omu5Bkk4+bsK3kxmnyrj0VzNDKJt4Y0p24d80K3mdTFhzZedfnH2pMkZxQ+Qr0pNJb23vVwtbcq1ftWBWMmyNFAg9u+98p/Tqtmuvo6sfGlXrwxuBl/no5jwOc7WbAnouSaMnMrGPQRTN2sLqaLR8KGNyC79KMXmqZVLillIyllQyllQ2AF8JyU8jcjh6XdrVEv8O0PB+dCjkqE7KzMeWtoc3o0ceaHsV64y3gatu2HzV0tsmya9QNTSx5zPsuvT3fF1AQembufo5HXC30rKSU/H4zksQUH8XCw4vNGRxCGPOg0TW1gaq5WVi3FCDKoOS5fj29HQmoWb608Ua4V0GKTM5m/+wIjAzz/KjEAVY89MsCDHWfiuZ5WePK982w8m8OuEnU9g59KmfhVJwaDZOXRaHr4ueBet2AC+XQvX3LzDCzYE1Hscc7Hp/Jr0GUmdvGmvkPBlmiBDR3Z8GJPuvk68+7qEN5ZdaLYDhTnrqbwr42naeftwEPtvYrcztRE8MmY1lxLy+KpxYcJjb5zflJkYjonY24wpFXpu1dUBWMmyGuAx/K7WXQBkqWURY+/a0ZlYWbC832bsPWV3nRs5MhH604y6tu9HCviAnsH787wzB414eTQPJjdHS7tr/ygNU1DCLEU2A80E0JECSGeFEI8I4R4xtixaWXUbQakxUHI8r+emtjZh8VTO9HLKv8Wu3fngvtZWEOjnnBuC35udqx4phv1bCyY9N1B9py7swNEdFIGjy08xLurQ2jr7cDPU9piFfwDNBt25+qAbq1KPYIM0NqrLm8Obs7msKv8fKjsSernW85gMMCbg5sVeO2BtvXJNUg2FtL2LjfPwEfrTuLjZE2Xxo7M3hFe4+qVD1xIJDopg4faF1qFSkNnG4a19mDJ/kvFLpzyxZazWJmb8nzfJkVuU9fanIVPdOS5Pr4sPXSZCfMOcPXGnSPTZ2JTeP7nowzKH5H/6AH/Ehe68a9fl08fakN4XCojvtnDi0uPcSlRDZZtClOpX1nau1WFymzzVtJFeQNwAQgH5gPPVVYsWsXxdrLm+yc6Mntie66lZfPg7H28tzqk5NWMLKzVhJMn1oM0wPdDYfN7d8zM1jSt4kkpJ0gpPaSU5lJKLynlAinlHCnlnEK2fUJKucIYcWql0LgPuLeGfd+A4a47eJcPgrm1mkBXGL9BqkQj8TwNHK359emuNKhnzdRFh9kUGouUkmWHIxn8v10EXbrOP0f7s+TJztSPXAfpidDlrt+n3P0hJaZMK6k+2aMRPf2c+WjdyTL1SD4Vc4MVR6N4vJsPDRytC7zeytMeXxcbfiukzOKng5GEx6Xy3rAWvDG4GQmp2fyw/1Kp37s6WHE0CjtLMwYXM8L6XJ8mpGXnMWfX+UJH6EOjk1kfEsNTPRrhbGtZ7PuZmgjeHNKcWRPbczo2hRHf7CHo0jVOxdzg2SWqVGXH6Tie6+PLrjf74l+/bqnO4+HABux8sy/P9/Vly8lY+n++kw9+D+X341do5Wlf6L+tMVXaSnpSygklvC6B5yvr/bXKI4RgaGsPejZ14X9bz7Jo30U2h8Xy3vAWjG5bwupJDXvAs/tg2wew/1vVM3nMHPAKrLoT0DRNq4mEgG4vwaqn4NxmaDb01muR+9XcD9PC60DxGwQb34RzW8HJF1d7K5Y93YUnvj/Mcz8FEdDAgWORSXRp7Mh/xgaoZEVKODgH3PyhYc87j3dzol5cmCr/KAUTE8HnDwcw7KvdzFh6jPmPBVLfoU6ho49xKZnsPBPPjjPx7Dobj72VOS/09Svir0XwQNv6fLH1LFeSMvDMLx9ISs/mf9vO0r2JEwNbuiGEoHdTF+buPM+kLj7FrtZWXaRm5bIxJJbR7epjZW5a5HYtPe0Z6u/O7B3n2RuewEv9/ejX3PWvz+PPNp/Bwdqcp3o1LvV7D2vtga+LLdN+OMIjcw+Qa5DYWpoxo18TnuzRCAfrsrdjs7cy543BzXmsa0O++uMcSw5GkmeQvFbIBD9jE+WpBTKmwMBAeeTIEWOHod0m7Eoy760O5fjlJLo2duKj0f40cbUtecfz22HNDLgRrSbz9X5LjTRr2n1ACBEkpayxvxnqa7GR5OXAV22hng9M2aCey0qFT72hxyuqi1BRvgkEB2+YvOqvp9Kycpn+4xGOXkri7aHNmdzF51bCGrEbFo+AUd+opa9vlxoH//WDIZ9Cl2fLdArbz8Qx5fvDgCrf83a0pqGTNT5ONliambAnPIETUapO1dXOkr7NXJnYxZs2Xg5FHvNiQhp9/ruDd4Y25+nevgD8fU0YP+y/mN/KTLUfC76cxAMz9/L6oKa80K/whLsspJR882c4OXkGJnTy/is5ryjLj1zmzRUnWPlsVzr4FL9SXE6egdVHo/lm+zkuX8ugdf26vNTfD1srM8bPO8C7w5ozvZdvmWNISs/mn+tP4elQh6ndG5YrMS7K+fhUVh+NZmqPRjgaqf9xUddinSBrFcJgkCw9HMm/N54mIyePZ3r78nzfJsX+xgtA5g3Y8h4c/UG1EPJ/UM2Urt+hYPN8TatFdIKsldv+mWqVvaf+BK8OcGGH6pM8cSX4DSh6v03vwuHv4K0IsLD562mDQZKek1dwRHXpBNVb+dWTavGSu/2nCfgNhtEFl8QuSWh0MiHRyVxMTONSQrr6mphOZm4e7Ro40K+5K32audLK0774u5K3eWDmXnJyDWx4qSfhcSkM/nI34zs24OMxre/Y7qnFRzgUkcjut/oV2XmhtH46eIn3VqtabFMTwcAWbjzWzYeujZ1KHXdxHpm7n7iULP58rXepj5eTZ2D1sWi+/TOcyGvpWJia4GhjwY43+pT8mXwfKupaXP3vL2g1gomJYGJnHwa1dOdfG07xzZ/h/H78Cv94oBV9mrkWvaOVvRqdaDtRtYMLXqa+ujRXzwWMB9ti9tc0TbvftH8Mdvwb9n0NDy+GyIOAgAYdi9+v2VA4MFMNSNw26mtiIgomxxd2wJkN0Pe9wpNjKPNEvdv5169boHZVSklWrqHcSdzotp58uFbVN3+84RTW5qa8Wsit+1cG+jH866ss2BNR6OulFRqdzIdrTtKrqQv/fMCfnw5dYtnhy2wKi8XP1ZaJnb3xcVaj4pZmpliamWBlboKrvRX2ViUn5ieikjgYcY03BjcrU7JtbmrCw4ENGNOuPr8di2bx/os809tXJ8dlpEeQtUqxLzyB//s9lAvxaQxv7cH7I1viVpr+hpk3IGwVHPsJog6BiZkaoWg3CfwGFl1fp2k1jB5B1u7J1g9UgjzjKKx/FVKuwnP7it9HSvhpHFzaB88fUOUWhcnJVL3rkfDsftWyszCb31Mj0u9Eg6nxx9viUjLp8skfdPCpx+GL13lvWAumFVFz++ySIHafS2DPW33LVTKQnJHDyG/2kJ0/Yn2zPCAzJ4+1wVf4Yf8lQu5qZ3aTnaUZv7/QncYuRZci5hkkY2btJSY5kz9e612qhForHz2CrFWpbk2c2fhST+btvMC328PZeTae1wY1ZXIXH8xMi2meYmUPHZ5Qj/gzcGwJBP8CZ9aDjYsaUW47CVybV9WpaJqmVT+dn1GlFvu+gcuHoc24kvcRAkZ8ATO7wLpXYOKKwkvZ9n4F187DpFVFJ8egJu/lZsK1C+Bi/ElWrnZWdG/izO5zCTR0subxbg2L3PblAU3ZFBbLvF0XeHNI2T5PpJS8uSKYK0kZLHu6yx21s1bmpowLbMDYDl5EJKSRnJFDVq5BPXLySM/O44M1YbyyPJiVz3Qt8vPw54OXOBGVzFfj2+rk2EhqxEp6Ws1kaWbKjP5+bHmlFx186vHh2pM8MHNv6ZexdGmmFhp59SRM+AUadIYDs2FWZ5jfH458D5mF/4auaZpWq9l7QJuHIeh7yE6BBl1Kt5+DN/R/H8K3QcivBV9PPA+7P4dWD0KT/sUf668lp8tXZlEZxrRTvYLfG94SC7OiU5xm7naMaOPJon0XSUzNKtN7LNgTweawq7w1pHmRE+eEEDR2saWddz26NHaid1MXBrVyZ3S7+vxztD/Bl5OYtaPw5aHjUjL5bPMZujdxYlSAZ5li0yqOTpC1SufjZMOiKR2Z+Wh7ElKzGDNrL3/7LbTIJScLMDVXtXPjf4JXT8Ogj9VqfOtehv82g1XT1TLWd/cF1TRNq826zVB95QG8S5kgg1oRz6sjbHwL0m5bKERK2PA6mFnC4E9KPo5W5CvQAAAgAElEQVRLMxCm1S5B3vJKLwa2dCtx25f6+5GZk8fgL3fx9I9HmLfrPEGXrhW7HHXQpWt8uvE0A1u68VTPRkVuV5yRAZ6MCvDk6z/OERJVcJDnk/WnyMox8NED/hUy0U8rH12DrFWplMwcvth6lsX7LuJoY8nfRrRgVIBn2S8CUsKVo6oEI2QFZN0ABx81sa/to+DQoORjaJoR6RpkrUL8PF71In7pRNk6/8Sdgjk9odUYeGi+ei50FayYAkM/g85Pl+44M7uolnOPLit77NXAtpNXWR8Sw9HI61xKTAfAwtSEFp72eDnUwcXO8q+Ho7UFf/s9FDNTwboZPe+pA0Zyeg6Dv9yFjaUp61/s+dcEun3nE3h0/kFe7NeEVwcVXDVQq3i6zZtWrYRGJ/Pe6hCCo5Lp3sSJjx7wL3bCQrFyMuDUOjj2I0TsBIRacardJGg+ovgaOk0zEp0gaxUi84a6o2bvUfZ9t/8Ldn6qapEbdIZvO4KdO0z7E0xK2fFgxZNqFb9Xqs8ocnnFp2RxNPI6RyOvExKVTOyNTOJTskjJvLU0tYWpCSuf7UZrr9KtHlec3efimbzgEFO6N+SDka3IzjUw9Ktd5ORJtrzSS3edqCJ6kp5WrfjXr8uq57rz86FIPtt0miFf7la3ndp60t3XqfiJfHczr6MmqLQZB9cvQfBS1QVj5ZNgVRdaj1PJskdb3VtZ07TaxcpePcqj56tw8jdY+zL49oXUqzBhaemTY1BLToeugIwkqFP0Qh4FSFntrscudpYMbuVeYEnnjOw8ElKziEvJwtXOssKWRO7p58LjXX34fu9FBrRw4/jlJM7Hp/H9lI46Oa4G9AiyZnRxKZn8b+s51gVfISUrF0cbC4a1dmdkG086NnQsdBnSEhkMcHGXSpRPrVEzrV1bqUS5zcNg41zxJ6JpZaBHkLVq4fIhWDAIkNBxGgz/b9n2P7cVfhoLUzaCT7eSt8/LhQOzYOdnMOw/0HZCucKuLTKy8xj+zW7SsnJJzsihbzNXZk/qYOyw7iu6xEKr9jJz8th5Np61wVfYduoqmTkG3O2tGNHGg1FtPWldv275JixkJEHoSjj+E0QHgYk5NBuiVuzz7V8t+ndq9x+dIGvVxrYP4eTvqrSiLKPAADeuwBctYOh/oPP04re9cgzWvAixJ8DCTq3m9+IxsKiYEdma6vjlJB6avQ8rMxO2vdYbj7oVu1y1VjydIGs1SlpWLttOXWVt8BV2no0nJ0/S0MmakQGejAzwpKmbXfkOfPWkSpSDf4H0BLB1V72V200CZ7+KPQlNK4ZOkLVqxWAAk3I0tpISPmsELUbBqK8L3yY7DbZ/okaObVzVyLGNC3w/BPp/oEo97nObQmOxtjClV1MXY4dy39EJslZjJafnsCkshrXBMew7n4BBQnN3O5Ust/HE26kcow+52XBui+qCcW4LyDzVR7TdRDWr27KcCbimlZJOkLVaY9EINVl62h8FX7uwE35/AZIjocMUGPD3W6PUPz8CkfvhpWCoU68qI9a0v+gEWasV4lOy2BASw9rgKxy5dB2AgAYOjGzjwcgAz9ItZ323lKtw4heVLCecBXNraDYMWo8F336qJ6imVTCdIGu1xsa34ehiteT07aPQF3aq+mQHHxj1Dfh0vXO/q2Ewuzt0fxEG/qNqY9a0fDpB1mqdqOvprD8Rw5rgK4RduYEQ0LmRIyMDPBnq73HH8p+lIiVEHVElGCd/h4xrqgtGi5Hg/xA07KXrlbUKoxNkrdY4+iOseQFmHAUnX/VcdBAsHqVW7ntiPVgXvuIcq55WnTRePAb2etU4rerpBFmr1c7Hp7I2+Aprgq9wIT4NMxNBDz9nRrbxZFArN+zKupZ9Xg5c2KEm951ap5ZytXGBlqNVstygc/nq9TQtn06QtVoj+ijM7wsP/wAtH4D4M7BwiCpVm7q5+B7N1y/CN4GqvG3kV1UWsqbdpBNk7b4gpeRkzA3WBqsyjOikDCzMTOjXzJVRbT3p19y17P0lczJVnXLoSji7SbWMs6+vapVbj9X9lbVy0QmyVmtkp8O/6kOvN1R3oIWD1SDD1E23RpSLs+FNOPwdPH+wYiZLV8Mey1r1pRNk7b4jpeRoZBJrg6+w7kQMCalZ2FiYMqiVOyMDPOjRxAULszKOAmelwJlNKlkO3waGHHBsrEaV/R8C1xaVczJaraMTZK1W+SYQbF0hNU49pmxQi4iURmo8fBUAfgPh4cXljyEnA7b8DY7+oI7VaRo06q2TZa1YOkHW7mt5BsmBC4msDb7CxtBYkjNycLA2Z6i/OyMDPOncyAnTsi5Ikn4NTq9TyXLELpAGtRiJ/4Pq4di4ck5GqxV0gqzVKr8+AWGrwawOTF5dcEJeSbZ/Ajv/DdO2Q/32ZX//qyfV6qlxJ6H5CNUdIz0RnJtCp+mqnafuTqQVQifImpYvO9fA7nPxrAm+wtaTV0nPzsPVzpLh+Z0w2jVwKPuCJKlxEPabSpYvH1DPebbPH1l+UE8+0QrQCbJWq+z7Brb9HcYvhaaDyr5/5g34ui24tYKJK0rfPUhKVZ6x+T215PboOeA3QJXGha2GQ3PVAiUWdtDtBej9lh5R1u5glARZCDEE+AowBb6TUn561+vewGLAIX+bt6WUG4o7pr4oaxUpIzuPP0/HsSY4mu1n4snONeBVrw7DW3swtLUHAV7lWL0v6bK6MIeugJhgQKglWP0fVJP89DLXGjpB1mqZvBxIi7+3wYBD82HD62DtDIFTIHBq8cdLS4Tfn4ezG6HJQBg9S5V53C0qCPZ8oe74Df8cOj5V/hi1WqfKE2QhhClwFhgIRAGHgQlSypO3bTMPOCalnC2EaAlskFI2LO64+qKsVZYbmTlsCVOr9+0NTyDXIPGsa8UQfw+GtXanvXc9TMpahpEQDmGrIGQFJJwBYQqNe4P/WGg+vOzLumq1hk6QNe0uUqruQYfmwZmNYGKqVujr/DS4t4G4U3A1RPVPjg1VS1bnZaseyp2eLr6zkMEASx+B83/C4+vKXgKi1VrGSJC7An+XUg7O//4dACnlv27bZi5wQUr57/ztP5dSdivuuPqirFWF5PQctp26ysbQGHadTSA7z4CrnSVD/N0Z6u9Bp0aOZatZllLVxoWsUGUYSZfA1EKNevg/CM2GgoVN5Z2QVu3oBFnTinEtQpVOHP0RspLvfM3CVpViuPmrkWb31qU7ZkaSakeXnQbTdxbffk67bxgjQR4LDJFSPpX//WSgs5Tyhdu28QC2APUAG2CAlDKokGNNB6YDeHt7d7h06VKlxKxphUnJzOHP03FsDIll+5k4snINONtaMKiVO8P8Pejc2BFz0zJ0w5BS9Q0NXalGl1Ni8lfvG6pqlpsM0Kv33Qd0gqxppZCdBiG/QkrsraTYwaf8feivnoTvBqhjPbGuel9rpVQrvDYbqkvzKlF1TZBfzY/h8/wR5AWAv5TSUNRx9UVZM6a0rFx2nIlnQ2gM20/HkZ6dh4O1OYNaujG0tQfdfZ3L1jrOYIDIffnJ8m9q9T7LutBihBpZbtRHr95XS+kEWdOMJGy16rrRYQqM/NLY0RQtdCWsmKp6Sz/wrbGjqbWKuhZX5idvNNDgtu+98p+73ZPAEAAp5X4hhBXgDMRVYlyaVm42lmYMb+PB8DYeZObksfNsPBtDYtgYEsvyI1HYWZkxsIVKlnv6OZe8KImJCTTsoR5DP4OInRC6Ck6tVUte13FUNcuNeqmlrp189QxsTdO0e9FqDFw5Dnu/BM920OFxY0dUUG6W6goCEPyL6r7h0KDYXbSKVZkJ8mHATwjRCJUYjwcevWubSKA/sEgI0QKwAuIrMSZNqzBW5qYMbuXO4FbuZOXmsTc8gQ0hsWwJi2XVsWhsLEzp38KNYa3d6d3UlToWJSTLpuaqvKLJABj+hVqI5NRa1WM5bLXaxs5TJcuNeqqvDt6Vf6Kapmm1Tf/3VZehDa+rcguvanYz59A8SIqEB2bC2pdh39cw7D/Gjuq+Utlt3oYBX6JauC2UUn4shPgHcERKuSa/c8V8wBaQwJtSyi3FHVPf1tOqu+xcA/svJLIpNIbNYVe5lpZNHXNT+jZ3Yai/B32bu2JrWYbfTaWExPNwcZdKliN2Q3qCes3BJz9h7q2SZjv3yjkprcLpEgtNM7L0azC3typje2YvWFgbOyIl/ZrqCe3VCSatgDUzIHgZvBwCdm7Gjq7W0QuFaJoR5OYZOBRxjQ2hMWwKvUpCahYWZib0burCsNbu9G/hhr2VedkOKqVqdxSxCy7uVo/M/Fnezk3zyzF6qoeNU8WflFYhqipBFkIsBEYAcVLKAmv/CiEmAm8BAkgBnpVSBpd0XH0t1mqFiF2weCR0eQ6G/Kvk7avCpnfg4ByVtLu1hGsX4JsO0PV5GPRPY0dnPCmxsOu/asQ/cEqFHVYnyJpmZHkGSdCl62wIiWFTaCyxNzIxNxX09HNhiL87g1q64WBtUfYDG/JUP9CI3epiH7kfslPVa27++SPMvdRiJVZ1K/aktHKrwgS5F5AK/FBEgtwNOCWlvC6EGIpqz9m5pOPqa7FWa6x/DQ4vgCkb1HWyIhgMamGS1KsgDWpgQxoACbZu0OrBwjtxJJ6HmZ2h7aMw6utbz6+cBqfXwyuhYO1YMTHWFLlZcGCWSo6zU8HSHl47XWGtUXWCrGnViMEgOXY5iU2hMWwIiSU6KQMzE0FXXyeGtfZgUEs3nGzL2X4oL0ctrRqxUyXNlw9CbiYIE/Boe6t+2bur7r1sRFVZYiGEaAisKyxBvmu7ekColLJ+ScfU12Kt1shKhdnd1DXy2b0Vc10MWgRrXyr6dd/+MGYu2Lrc+fzyx+DcNnjx6J0lc3GnYVZn6PUm9Hvv3uOrCaSEMxvUMuLXI6DZMLVwzG/PwKhvoP1jFfI2OkHWtGpKSklIdDIbQ2PZEBLDpcR0TAR0buTEwJZueDta42RrgbOtJc62liVP9rtbTiZEHValGBG7IOoIGHLAxBzqd7g16c+rE5hbVc5JagVU0wT5daD5zfachbyue9JrtVPEblg8Ajo/A0P/fW/HSr+mSiJcmsPDP6gVAUEl4EKo9m2b3lF39B6cB437qNcjD8LCQdDnXejzVsHjLpsMF3bCKyE1827g5vdUeUTbu/s1FOLGFfjtWbWyoktzVf7i208lzbO7qf7V03dUSFg6Qda0GkBKyamYFDaGxrAhJIbz8WkFtrG2MMXZ1hInWwucbCxxsVNfbybRtyfTDnXMCy6PnZ0GkQdu1TBfOaZu/Zlagndn1U6uUS+o31511tAqRXVLkIUQfYFZQA8pZWJJx9TXYq3W2fCG6h7xxAZo2L38x1n7Mhz9AZ7ZrRLCwlwNU72YE85Bz9egz9vw/VBIjoIZQYWPYscEw9xe0O9v0Ov18sdnDNcuwNftVOell06U3K709+fVyrMDP4LAqXeuB3Bovuo+Mm27+py6RzpB1rQaKCY5g7gbWSSkZpGYmk1CWv7X1FtfE1KzuZaWhaGQ/8omAhxtLHG+LXl2srHE2c4C5/yk2tUiC4+kYzhc3Y/ZpT1wNUTtbG4DPl1vTfrzCLg1EqLds+qUIAsh2gCrgaFSyrOlOaa+Fmu1TnaaGp0EeHZf+Uotoo/C/H7Q5dmSJ/1lp8GGN+H4EnD0hWvnVVu3dpOK3uenceou4CuhNatE7s9/wq78NnVPboUGnYreNicT/usHzYfDmDkFX89Mhs+bq5VnK2ABFWMsFKJp2j3yqFsHj7p1StzOYJAkZeTkJ8wqeU7MT54T09TXhNQsIiPTSUzNIi07764jmAE9sbXsS2ObTHqYnaYjYbS8HIxb+DYAcsztoEEnzBt1B+9u6jf36rxMq1YqQghvYBUwubTJsabVShY2KkFdNFwt0tH3XchIgozrkJmk/mznXvREPoNBTfizcVEjwqV5v9Ez1SDE+lfBrTUETCh+n15vwIKBqsa56/NlPUPjMOTB8aXQoIu6YxmyovgE+dxmyLoBrccV/rpVXWg9Vh1n0D+hjkOlhK0TZE2rBUxMBI42FjjaWNDUza7E7TOy81QinXYzkc5PpvMT6eA0N/5M7UpCXhamWVfpLE7RJfckHcNP43fhDwAMJhZQvz0mPt3UB0aDTjWzLq6WE0IsBfoAzkKIKOADwBxASjkHeB9wAmYJddsztyb3Z9a0e9Kwh6pDPjhHlVsUpstzMPAfBUvQjv0AV47CmHlluxYGPKLqa03NSr5L16CTSqj3fgWN+6o2cCXJyQSZB+bWxlmJNWIn3IiCQf+AsN/UwleDP7mzbOJ2J5aDjavq71+UwKmqjOXEcug8vVLC1gmypt2H6liY0sDRmgaOJTfGzzNIktKziUhIY314AsFnzmNx5TAdxGk6R56m1eWvMNvzBRIBbq0QPt3Au4saZbb3qIKz0YojpSx2SCp/Ql6hk/I07b404O/g1ER1BKrjAFYOt74e/UG1HLtyDMYtutVpIv0abPtQXffaPFz297y7m0VJ8S15COb2VMl677fA0rbgdilX1Qp8hxdAbgYgwMJWbWthA3YeMG5x5ffLP/aT+oWh2XAwMYNTa9T8F9++BbfNuA7ntkDgk0Un0KCWCPdsD0cWQqdplZL46wRZ07RimZoInGwtcbK1JLChIwxoSkrmAA5euMbq8ATePXsZ+2sn6CRO0y3uHG3jfsDy5shLvYaqnZx3VzXK7NTEOCMYmqZppWVeRyVdhRn6qVqWes0MNWFu3GI1V+OPf6ja2OH/rfxrXP0O8EIQbPtAJcChK2HIp9BipHrvGzFqhDnoe5Xktx4Hri1UD+HsNMhKUY+Tv8Hh+aUrBymvjCTVD7rdJNUlyW8QWNip8ojCEuSTayAvG9oUUV5xu8CpsOYFNencp2uFh64n6Wmads9ikjPYcy6BPeEJHDgXi1v6OTqanKFPnXDacxqb3CS1obWzGl2+OcrsHlD8KEEtppea1rQa7OpJWDYJki5Bp+lwYHbpJuZVtMiDqn75aig0GQCOjSFoMRhyVT1zz1fBybfwfX8eD1GH4JUw9UtBZTi8QMV3e8eJ1c/A6Q3wxrmC81gWjVAt3mYElfyLRnYafN4Cmg6Ch74rd4h6kp6maZXGo24dxgU2YFxgAwwGyenY7uwJj2d+eCKHIhLwzI2mi+kZBhku0PbiMRxOr1M7mttAg47qtqR3F/DqCBYll31omqYZlVtLmL4dVj+rSi5sXCt3JLYo3p1h+k5VL739E9U3OGCCah3n2Kj4fbvNgEXDIHipGo2tDMd/AtdWqiTiptZj1Xue2wotRtx6PjkaLu5Rf4+lGYW3sIGA8WqkfMinYONcoaHrBFnTtAplYiJo6WlPS097pvfyJTMnj6OXrrM7PIHPzyUQeiUZV3mNnpbhDKsTQdv409S78C8EUtWnebS9bZS56/23rKqmaTWDVV14ZIlKAp2bGm+SsqkZdH0O2k5QJRW2rqXbz6ebquPd9y20f6Lwpa/vRdxpiA5SE/JuT3gb9VF3E0NX3Jkgh64EZNHdKwoTOAUOzVX/Bt2LWbmwHHSCrGlapbIyN6VbE2e6NXHmrSFwPS2bfecT2RMewPvnEoi6noE9aQyyu8SwuhcJyDyF46F5iP35/S1dmt+a9OfTFeo20HXMmqZVDyYm0H6ysaNQ6tQr2/ZCqFHkFVPg7EbVd7giHV+iBj3aPHLn86Zm0Gq0mryXlQKW+Z2XQpar+uqiSkIK49pCfTYc+R66zqjQJF8nyJqmVal6NhYMb+PB8DYeSCm5lJjOnvAE9pzz5eXzCdzIHIYl2YxyucrwuhdpYzhJvdCViKBF6gD29cGnu5rg0biv7pShaZpWXi1GqdXt9n1TsQlyXg4EL4OmQwovffAfC4e/U7XIAY+o0ebYEBhSjmW+A6fCqqcgYodql1dBdIKsaZrRCCFo6GxDQ2cbJnXxIc8gORGVlD/hz51pEd7k5PWkjhmM8UxmaN0IWuedou6F7YiQ5eogri3VRdG3rxpJ0DXMmqZppWNqBl2eh01vweXDak5IRQjfBmlx0HZi4a836Az2XqrMIuARNXosTMH/wbK/V8tRcLgL5GbfW8x30QmypmnVhqmJoJ13Pdp512NGfz/SsnI5FHGN3ecS2BMez88hdYG2uNo+xp5pblhc3AHn/4RD82H/t2BqqcowfPvlN9H3r/i6Ok3TtNqk3STY8YlqGffIjxVzzGNL1IqCfgMLf93EBFo/BPtnQloihPwKjfuUvn76dmaW8OTme4m28MNW+BE1TdMqiI2lGX2bu9K3ubpoxt3IZE94ApevZWBR3w/qB6iJGdnpELkPzm9XCfPW9/MP4KIS5ZsjzDeb+muapmmKpa1amGPP/+DaBdUq7l6kJcDZTWpFwrtXG7yd/1jVr3nLe5AUCX3fu7f3rWA6QdY0rcZwtbfiwfZeBV+wsFY9QJsMUN/fiIEL228lzH+VY7RSibJvPzWDu7J6f2qaptUknZ9Wdcj7Z6nFTkojOw2O/gipVyHrBmTeUF+TLqs+zO0mFb+/e2vV/SN4KZjVqfhJgvdIJ8iaptU+9h7Q9lH1MBhUE/3zf+aXY8y7rRyjW/7ocj9wa6W7Y2iadn+yc1fdJo4tgb7vltxeM+M6/PwIXD6oaoet7MHSXn2tU0/d2XNtUfwxhFCjyDs+gebDbnWzqCZ0gqxpWu1mYgIebdSjx8uqHOPSvlsJ89a/qYeN663R5cZ9wc7N2JFrmqZVnW4vqNZsh7+D3m8WvV3KVVjyIMSfgXGLoOXo8g8uBDwCB+dU3kIl90AnyJqm3V8srMFvgHqAWtb0wg6VLIf/ASeWqefd/G8lzN5ddTmGpmm1m2sL8BsMO/4FyZehzztg73nnNtcvwg+jITUOJi6/97Zq9RrCWxH3doxKohNkTdPub/aed5VjhNwaXT44V9XlmVndWY7h2lKXY2iaVvuMmQM7P1OjyCeWQ5dnofvLUMcBrp6EH8dAbiY89nvFtYSrpoSUsvIOLsQQ4CvAFPhOSvlpIds8DPwdkECwlPLR4o4ZGBgojxw5UgnRapqm3SU77bZyjO0Qf0o9b+t2Z3eMcrQmEkIESSkDKzjiKqOvxZpWi12/CH9+rCY4WzlAxyfh8AI1WDB5Nbi1NHaEFaaoa3GljSALIUyBmcBAIAo4LIRYI6U8eds2fsA7QHcp5XUhRDka4GmaplUSCxvVx/NmL88bV251xgjfCid+AUdfePGocePUNE2rSPUawkPzVV3ytg9h9+dQrxE89pt67T5QmSUWnYBwKeUFACHEL8ADwMnbtpkGzJRSXgeQUsZVYjyapmn3xt4T2k1UD4MBYk9AeqKxo9I0TascHgEweRVEB6kEuaTuFrVIZSbI9YHLt30fBXS+a5umAEKIvagyjL9LKTfdfSAhxHRgOoC3t3elBKtpmlYmJibg2dbYUWiaplW++h2MHUGVM/YarGaAH9AHmADMF0I43L2RlHKelDJQShno4uJSxSFqmqZpmqZp95PKTJCjgQa3fe+V/9ztooA1UsocKWUEcBaVMGuapmmapmmaUVRmgnwY8BNCNBJCWADjgTV3bfMbavQYIYQzquTiQiXGpGmapmmapmnFqrQEWUqZC7wAbAZOAcullGFCiH8IIUblb7YZSBRCnAS2A29IKfWMF03TNE3TNM1oKnWhECnlBmDDXc+9f9ufJfBq/kPTNE3TNE3TjM7Yk/Q0TdM0TdM0rVrRCbKmaZqmaZqm3aZSl5quDEKIeOBSOXZ1BhIqOJzy0rEUTsdSUHWJA3QsRSlvLD5Syhrbt7KWXIsriz7H2qG2n2NtPz8o3TkWei2ucQlyeQkhjhS21rYx6FgKp2OpvnGAjqUo1SmWmuB++PvS51g71PZzrO3nB/d2jrrEQtM0TdM0TdNuoxNkTdM0TdM0TbvN/ZQgzzN2ALfRsRROx1JQdYkDdCxFqU6x1AT3w9+XPsfaobafY20/P7iHc7xvapA1TdM0TdM0rTTupxFkTdM0TdM0TSuRTpA1TdM0TdM07Tb3RYIshBgihDgjhAgXQrxtxDgWCiHihBChxoohP44GQojtQoiTQogwIcRLRozFSghxSAgRnB/Lh8aK5baYTIUQx4QQ64wcx0UhRIgQ4rgQ4oiRY3EQQqwQQpwWQpwSQnQ1UhzN8v8+bj5uCCFeNkYs+fG8kv9zGyqEWCqEsDJWLNVddbkOV6TCrulCCEchxFYhxLn8r/WMGeO9KurzojadZ1GfQ0KIRkKIg/k/s8uEEBbGjvVe3f35VtvOsbDPzfL+rNb6BFkIYQrMBIYCLYEJQoiWRgpnETDESO99u1zgNSllS6AL8LwR/06ygH5SygCgLTBECNHFSLHc9BJwysgx3NRXStm2GvSq/ArYJKVsDgRgpL8fKeWZ/L+PtkAHIB1YbYxYhBD1gReBQCmlP2AKjDdGLNVdNbsOV6RFFLymvw38IaX0A/7I/74mK+rzojadZ1GfQ/8G/ielbAJcB540YowV5e7Pt9p4jnd/bpbrZ7XWJ8hAJyBcSnlBSpkN/AI8YIxApJS7gGvGeO+74oiRUh7N/3MK6j9LfSPFIqWUqfnfmuc/jDZzVAjhBQwHvjNWDNWNEKIu0AtYACClzJZSJhk3KgD6A+ellOVZza2imAF1hBBmgDVwxYixVGfV5jpckYq4pj8ALM7/82JgdJUGVcGK+byoNedZzOdQP2BF/vM1+hyh4OebEEJQy86xCOX6Wb0fEuT6wOXbvo/CSMlgdSSEaAi0Aw4aMQZTIcRxIA7YKqU0WizAl8CbgMGIMdwkgS1CiCAhxHQjxtEIiAe+z781950QwsaI8dw0HlhqrDeXUkYD/wUigRggWUq5xVjxVHP303XYTUoZk//nWMDNmMFUpLs+L2rVed79OQScB5KklLn5m9SGn9m7P9+cqH3nWNjnZrl+Vu+HBFkrghDCFlgJvCylvF/uw0QAAAhPSURBVGGsOKSUefm3zL2ATkIIf2PEIYQYAcRJKYOM8f6F6CGlbI+6Lf28EKKXkeIwA9oDs6WU7YA0jHw7Nb9ObhTwqxFjqIcamWgEeAI2QohJxopHq36k6qNaK3qpFvd5URvO8+7PIaC5kUOqUNXw862yFPu5WZaf1fshQY4GGtz2vVf+c/c1IYQ56mL3k5RylbHjAci/bb8d49VpdwdGCSEuom4B9xNCLDFSLDdHKJFSxqHqbDsZKZQoIOq2kf0VqITZmIYCR6WUV40YwwAgQkoZL6XMAVYB3YwYT3V2P12HrwohPADyv8YZOZ57VsTnRa07T7jjc6gr4JBfPgU1/2e2wOcbam5JbTrHoj43y/Wzej8kyIcBv/yZmhao27JrjByTUeXXHS0ATkkpvzByLC5CCIf8P9cBBgKnjRGLlPIdKaWXlLIh6ufkTymlUUYEhRA2Qgi7m38GBgFG6X4ipYwFLgshmuU/1R84aYxYbjMBI5ZX5IsEugghrPP/T/Wn+kzurG7up+vwGuDx/D8/DvxuxFjuWTGfF7XmPIv4HDqFSpTH5m9Wo8+xiM+3idSicyzmc7NcP6tmJW9Ss0kpc4UQLwCbUbPMF0opw4wRixBiKdAHcBZCRAEfSCkXGCGU7sBkICS/5grgXSnlBiPE4gEszp/lbgIsl1Iatb1aNeEGrFafTZgBP0spNxkxnhnAT/nJzQXg/9u7+xi5qjqM49/HKhQtaYgaQhSzjbQhCqEaqTb4Qgi+E6GYUBBCUgjUxoKpRan+QfjDaIOQCMaYQEAbqRRSg6AhlYAKDVW6trbVFjEbW2NBEAWqFaxm+/jHOaOHdbav051t9/kkk7lz7r3nnpvs/u5vzj1zz7x+NaQGvg8C8/vVBgDbj0taCayn/NL/V0yMqVv323iKw73ULaYDS4F7JF0O/AG4oH8t7Imu1wuOrPPseh2StAVYIenLlP/vflyvD7VrOXLOset1U9IgB/C3mqmmIyIiIiIaE2GIRURERETEPkuCHBERERHRSIIcEREREdFIghwRERER0UiCHBERERHRSIIcY07Szvo+IOlTPa77SyM+r+ll/V2Od56k6/ayzfWSnpK0ob4+1qz7oqQhSU9K+nBT/pFaNiRpSVO+QtL0Q3M2ERG9IWm4iXkb2jjWg7oHJPXlufAxceQxbzHmJO20PUXSmcA1ts/Zj31f3cwbP2rdvWjnPrZnDfAJ23/ZwzbXAztt3zii/G2UyS5mUaYqfgiYUVf/jvKs3+2USRYusr1F0geAS2xf0etziYjolUMZiyUNAD+yfcqhqD8C0oMc/bUUeF/tXVgkaZKkr0kalLRJ0nwASWdKWi3pfuoMbpJ+IGmdpM2SrqxlS4Fjan3La1mnt1q17t9I+rWkuU3dP5O0UtJvJS2vM0chaamkLbUtN45svKQZwK5OcizpPkmX1uX5nTbswbnACtu7bG8FhijJ8ixgyPbvbf+LMi3ouXWf1cDZ+t/UoBERhw1J2yTdUOPwWkkn1fIBST+p8fZhSW+p5cdLulfSxvrqTOc+SdJt9RrwYJ0BD0lXN3F7RZ9OM44AuchGPy2h6UGuie4O26dLOhp4TNKDddt3AqfURBLgMtvP16A4KOn7tpdIWmh7ZpdjnQ/MBE4D3lD3ebSuewfwduBp4DHgDElPAHOAk21bdRrSEc6gzKLWcWVt81ZgMfCeZt3Cmjz/Elhs+wXgTcAvmm221zKAP44ofzeA7d2Shup5rOvSpoiI8eCYZuY9gK/avrsu77B9ao2JXwfOAb4BLLO9TNJlwC3AefX9Edtz6kx3U4DjgOmUO2tXSLoH+CRwJ+W6Ms32rlHidsQ+SQ9yjCcfAi6tQfVx4PWUIAiwtkmOAa6WtJGSYJ7YbDea9wJ32R62/SzwCHB6U/d227uBDcAAsAP4J3C7pPOBl7rUeQLwXOdDrfc6ytz2i20/X1d9C3grJUH/E3DTXtq6N3+mDMmIiBivXrY9s3nd3ay7q3mfXZdnA9+ry9+lxGyAsygxlBq/d9TyrbY7Cfg6StwG2AQsl3QJZQr4iAOSBDnGEwFXNQF1mu1OD/I//rtRGbt8NjDb9mmU+eMnH8RxdzXLw0BnnPMsYCWld2NVl/1e7nLcU4G/0iSwtp+tgX03cFutF+ApSnLf8eZaNlp5x+R67IiIw5FHWd4f/xe36/LHgW9S7joOZjhaHKgkyNFPfweObT7/GFgg6TVQxvhKel2X/aYCL9h+SdLJvHIow787+4+wGphbxzm/EXg/sHa0hkmaAky1/QCwiDKkYaQngJOafWYBH6UM2bhG0rRafkKzzxyg8+vr+4ELJR1dt51e2zQITJc0TdJRwIV1244ZTR0REYebuc37z+vyGkqsA7iYErMBHgYWANT4PXW0SiW9CjjR9k+BaynXijH70XYcWfLNKvppEzBch0p8B7iZcptsff2h3HOUMWgjrQI+XccJP8krx/HeCmyStN72xU35vZRbeBspPRZfsP1MTbC7ORa4T9JkSs/257ps8yhwU23rUZTe4Xm2n5a0GLhD0lnADZJm1uNuA+YD2N5cx85todwK/IztYQBJCylfGCYBd9jeXMuPp9y6fGaUdkdEjAcjxyCvst151NtxkjZReoEvqmVXAd+W9HlK7J9Xyz8L3CrpckpP8QLKULVuJgF31iRawC22X+zZGcWEkse8RRwESTcDP7T90BgdbxHwN9u3j8XxIiJ6SdI24F17ejRmxHiQIRYRB+crwGvH8HgvAsvG8HgRERETTnqQIyIiIiIa6UGOiIiIiGgkQY6IiIiIaCRBjoiIiIhoJEGOiIiIiGgkQY6IiIiIaPwHt6ANFePixF0AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "save_loss_comparison_gru(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, \"gru\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4ijaCzneAt"
      },
      "source": [
        "Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it (see follow-up questions in handout)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrNnz8W1nULf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c95b79de-d785-4290-e0c8-df36b3943fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay ariway ondingcay-ybay isway orysnimay\n"
          ]
        }
      ],
      "source": [
        "# best_encoder = rnn_encode_s  # Replace with rnn_encode_s or rnn_encode_l\n",
        "# best_decoder = rnn_decoder_s  # Replace with rnn_decoder_s or rnn_decoder_l\n",
        "# best_args = rnn_args_s     # Replace with rnn_args_s or rnn_args_l\n",
        "\n",
        "\n",
        "best_encoder = rnn_encode_l  # Replace with rnn_encode_s or rnn_encode_l\n",
        "best_decoder = rnn_decoder_l  # Replace with rnn_decoder_s or rnn_decoder_l\n",
        "best_args = rnn_args_l     # Replace with rnn_args_s or rnn_args_l\n",
        "\n",
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwA6OGqlaTq"
      },
      "source": [
        "# Part 2: Attention mechanisms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJSafHSAmu_w"
      },
      "source": [
        "## Step 1: Additive attention\n",
        "\n",
        "In the next cell, the [additive attention](https://paperswithcode.com/method/additive-attention) mechanism has been implemented for you. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdewEVSMo5jJ"
      },
      "outputs": [],
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # A two layer fully-connected network\n",
        "        # hidden_size * 2 --> hidden_size, ReLU, hidden_size --> 1\n",
        "        self.attention_network = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1),\n",
        "        )\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the additive attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.size(0)\n",
        "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(\n",
        "            keys\n",
        "        )\n",
        "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
        "        unnormalized_attention = self.attention_network(concat_inputs)\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = torch.bmm(attention_weights.transpose(2, 1), values)\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_p8d5EmvOJ"
      },
      "source": [
        "## Step 2: RNN + additive attention\n",
        "\n",
        "In the next cell, a modification of our `RNNDecoder` that makes use of an additive attention mechanism as been implemented for your. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJaABkXrpJSw"
      },
      "outputs": [],
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, attention_type=\"scaled_dot\"):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size * 2, hidden_size=hidden_size)\n",
        "        if attention_type == \"additive\":\n",
        "            self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
        "        elif attention_type == \"scaled_dot\":\n",
        "            self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
        "\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            embed_current = embed[\n",
        "                :, i, :\n",
        "            ]  # Get the current time step, across the whole batch\n",
        "            context, attention_weights = self.attention(\n",
        "                h_prev, annotations, annotations\n",
        "            )  # batch_size x 1 x hidden_size\n",
        "            embed_and_context = torch.cat(\n",
        "                [embed_current, context.squeeze(1)], dim=1\n",
        "            )  # batch_size x (2*hidden_size)\n",
        "            h_prev = self.rnn(embed_and_context, h_prev)  # batch_size x hidden_size\n",
        "\n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2)  # batch_size x seq_len x seq_len\n",
        "\n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYPae08Io1Fi"
      },
      "source": [
        "## Step 3: Training and analysis (with additive attention)\n",
        "\n",
        "Now, run the following cell to train our recurrent encoder-decoder model with additive attention. How does it perform compared to the recurrent encoder-decoder model without attention?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ke6t6rCezpZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3533afb1-2641-4964-9bf6-9fced59913d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                early_stopping_patience: 10                                     \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: rnn                                    \n",
            "                           decoder_type: rnn_attention                          \n",
            "                         attention_type: additive                               \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('injure', 'injureway')\n",
            "('revenge', 'evengeray')\n",
            "('resist', 'esistray')\n",
            "('robert', 'obertray')\n",
            "('sedateness', 'edatenesssay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.062 | Val loss: 1.721 | Gen: otay ay ingway ilway inggay\n",
            "Epoch:   1 | Train loss: 1.468 | Val loss: 1.461 | Gen: erthay away incingiongway isway ingway\n",
            "Epoch:   2 | Train loss: 1.143 | Val loss: 1.356 | Gen: ehthay iway oodincay-intiondinay isway oray-ingway\n",
            "Epoch:   3 | Train loss: 0.916 | Val loss: 1.244 | Gen: ethay away oocdingcinginginging isway oray-ingingway\n",
            "Epoch:   4 | Train loss: 0.768 | Val loss: 1.153 | Gen: ethay away ondithingway isway orway-ingway\n",
            "Epoch:   5 | Train loss: 0.662 | Val loss: 1.114 | Gen: ehthay aiway ondindindintingway isway ay-ingday\n",
            "Epoch:   6 | Train loss: 0.599 | Val loss: 0.981 | Gen: ehthay ariway onidinininininininin isway orkingingingingingin\n",
            "Epoch:   7 | Train loss: 0.459 | Val loss: 0.790 | Gen: ehthay airway ondictionday isway orkingnay\n",
            "Epoch:   8 | Train loss: 0.344 | Val loss: 0.708 | Gen: eththay airway ondintingcay isway orkingway\n",
            "Epoch:   9 | Train loss: 0.275 | Val loss: 0.665 | Gen: ethay-ehay airway onditionday isway orkingway\n",
            "Epoch:  10 | Train loss: 0.216 | Val loss: 0.622 | Gen: ethay airway onditingway isway orkingway\n",
            "Epoch:  11 | Train loss: 0.183 | Val loss: 0.564 | Gen: etthay airway onditingcay isway orkingway\n",
            "Epoch:  12 | Train loss: 0.130 | Val loss: 0.500 | Gen: ethay airway onditioningcay isway orkingingway\n",
            "Epoch:  13 | Train loss: 0.118 | Val loss: 0.606 | Gen: ethay airway onditioningcay isway orkikingway\n",
            "Epoch:  14 | Train loss: 0.097 | Val loss: 0.455 | Gen: ethay airway onditingoningcay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.119 | Val loss: 1.074 | Gen: ethay arway onditionday isway orkingway\n",
            "Epoch:  16 | Train loss: 0.315 | Val loss: 0.837 | Gen: ethay awrway onditingway isway orkingway\n",
            "Epoch:  17 | Train loss: 0.240 | Val loss: 0.627 | Gen: ethay airway onditingcay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.166 | Val loss: 0.532 | Gen: ethay airway onditiningcay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.109 | Val loss: 0.423 | Gen: ethay airway onditiningcay isway orkingway\n",
            "Epoch:  20 | Train loss: 0.080 | Val loss: 0.521 | Gen: ethay airway onditionioncay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.114 | Val loss: 0.393 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.057 | Val loss: 0.322 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  23 | Train loss: 0.028 | Val loss: 0.290 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.017 | Val loss: 0.253 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.011 | Val loss: 0.251 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.008 | Val loss: 0.247 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.007 | Val loss: 0.245 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.006 | Val loss: 0.244 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.005 | Val loss: 0.245 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.004 | Val loss: 0.246 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.004 | Val loss: 0.246 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.003 | Val loss: 0.247 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.003 | Val loss: 0.248 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.003 | Val loss: 0.248 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.002 | Val loss: 0.249 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.002 | Val loss: 0.250 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.002 | Val loss: 0.252 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.002 | Val loss: 0.252 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Validation loss has not improved in 10 epochs, stopping early\n",
            "Obtained lowest validation loss of: 0.24362294713216948\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "%timeit\n",
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "rnn_attn_args = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"early_stopping_patience\": 10,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 64,\n",
        "    \"encoder_type\": \"rnn\",            # options: rnn / transformer\n",
        "    \"decoder_type\": \"rnn_attention\",  # options: rnn / rnn_attention / transformer\n",
        "    \"attention_type\": \"additive\",     # options: additive / scaled_dot\n",
        "}\n",
        "rnn_attn_args.update(args_dict)\n",
        "\n",
        "print_opts(rnn_attn_args)\n",
        "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNVKbLc0ACj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a88c53-2d5f-4ca1-e651-96c1c88ab407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7nhsEio1w-"
      },
      "source": [
        "## Step 4: Implement scaled dot-product attention\n",
        "\n",
        "In the next cell, you will implement the [scaled dot-product attention](https://paperswithcode.com/method/scaled) mechanism. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_j3oY3hqsJQ"
      },
      "outputs": [],
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(\n",
        "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.shape[0]\n",
        "        k = self.K(keys)\n",
        "        q = self.Q(queries)\n",
        "        if len(q.size()) == 2: # 2D Tensor\n",
        "          q = q.unsqueeze(1).expand_as(k)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = (k @ q.transpose(1, 2)) * self.scaling_factor\n",
        "        attention_weights = self.softmax(unnormalized_attention)\n",
        "        context = attention_weights.transpose(1, 2) @ v\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unReAOrjo113"
      },
      "source": [
        "## Step 5: Implement causal dot-product Attention\n",
        "\n",
        "\n",
        "Now, implement the casual scaled dot-product attention mechanism. It will be very similar to your implementation for `ScaledDotAttention`. The additional step is to mask out the attention to future timesteps so this attention mechanism can be used in a decoder. See the assignment handouts for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovigzQffrKqj"
      },
      "outputs": [],
      "source": [
        "class CausalScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CausalScaledDotAttention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.neg_inf = torch.tensor(-1e7)\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(\n",
        "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
        "\n",
        "        Arguments:\n",
        "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
        "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            context: weighted average of the values (batch_size x k x hidden_size)\n",
        "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
        "\n",
        "            The output must be a softmax weighting over the seq_len annotations.\n",
        "        \"\"\"\n",
        "        batch_size = keys.shape[0]\n",
        "        k = self.K(keys)\n",
        "        q = self.Q(queries)\n",
        "        if len(q.size()) == 2:\n",
        "          q = q.unsqueeze(1).expand_as(k)\n",
        "        v = self.V(values)\n",
        "        unnormalized_attention = (k @ q.transpose(1, 2)) * self.scaling_factor\n",
        "        mask = torch.tril(unnormalized_attention, diagonal=-1)\n",
        "        masked_tensor = torch.where(mask != torch.zeros_like(self.neg_inf).to('cuda'), self.neg_inf.to('cuda'), mask) + unnormalized_attention\n",
        "        attention_weights = self.softmax(masked_tensor)\n",
        "        context = attention_weights.transpose(1, 2) @ v\n",
        "        return context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Attention encoder and decoder\n",
        "\n",
        "The following cells provide an implementation of an encoder and decoder that use a single `ScaledDotAttention` block. Please read through them to understand what they are doing."
      ],
      "metadata": {
        "id": "ZkjHbtvT6Qxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(AttentionEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.self_attention = ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "               \n",
        "        self.attention_mlp = nn.Sequential(\n",
        "                                nn.Linear(hidden_size, hidden_size),\n",
        "                                nn.ReLU(),\n",
        "                              )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder scaled dot attention.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        annotations = encoded\n",
        "        new_annotations, self_attention_weights = self.self_attention(\n",
        "            annotations, annotations, annotations\n",
        "        )  # batch_size x seq_len x hidden_size\n",
        "        residual_annotations = annotations + new_annotations\n",
        "        new_annotations = self.attention_mlp(residual_annotations)\n",
        "        annotations = residual_annotations + new_annotations\n",
        "\n",
        "        return annotations, None\n"
      ],
      "metadata": {
        "id": "yKGNqUaX6RLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.self_attention = CausalScaledDotAttention(\n",
        "                                hidden_size=hidden_size,\n",
        "                                )\n",
        "                \n",
        "        self.decoder_attention = ScaledDotAttention(\n",
        "                                  hidden_size=hidden_size,\n",
        "                                  )\n",
        "                \n",
        "        self.attention_mlp = nn.Sequential(\n",
        "                                nn.Linear(hidden_size, hidden_size),\n",
        "                                nn.ReLU(),\n",
        "                              )\n",
        "                \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        new_contexts, self_attention_weights = self.self_attention(\n",
        "            contexts, contexts, contexts\n",
        "        )  # batch_size x seq_len x hidden_size\n",
        "        residual_contexts = contexts + new_contexts\n",
        "        new_contexts, encoder_attention_weights = self.decoder_attention(\n",
        "            residual_contexts, annotations, annotations\n",
        "        )  # batch_size x seq_len x hidden_size\n",
        "        residual_contexts = residual_contexts + new_contexts\n",
        "        new_contexts = self.attention_mlp(residual_contexts)\n",
        "        contexts = residual_contexts + new_contexts\n",
        "\n",
        "        encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "        self_attention_weights_list.append(self_attention_weights)\n",
        "\n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "\n",
        "        return output, (encoder_attention_weights, self_attention_weights)"
      ],
      "metadata": {
        "id": "vDUvtOee7cMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Training and analysis (single scaled dot-product attention block)\n",
        "\n",
        "Now, train the following model, with an encoder and decoder each composed a single `ScaledDotAttention` block."
      ],
      "metadata": {
        "id": "B7gJLw5t_rnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "attention_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"attention\",\n",
        "    \"decoder_type\": \"attention\",  # options: rnn / rnn_attention / attention / transformer\n",
        "}\n",
        "attention_args_s.update(args_dict)\n",
        "print_opts(attention_args_s)\n",
        "\n",
        "attention_encoder_s, attention_decoder_s, attention_losses_s = train(attention_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, attention_encoder_s, attention_decoder_s, None, attention_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ],
      "metadata": {
        "id": "7MOkZonC8T3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c671f277-c9da-4454-98ec-966ca2036b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 100                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: attention                              \n",
            "                           decoder_type: attention                              \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('injure', 'injureway')\n",
            "('revenge', 'evengeray')\n",
            "('resist', 'esistray')\n",
            "('robert', 'obertray')\n",
            "('sedateness', 'edatenesssay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 3.000 | Val loss: 2.512 | Gen: ay iay iay iay iay  \n",
            "Epoch:   1 | Train loss: 2.361 | Val loss: 2.219 | Gen: ay iay inay iay inay\n",
            "Epoch:   2 | Train loss: 2.153 | Val loss: 2.081 | Gen: eay inay ingtingtingtingtingt iay ingtingtingtingtingt\n",
            "Epoch:   3 | Train loss: 2.027 | Val loss: 1.991 | Gen: eay inay ingtingtingtingtingt iay ingtingay\n",
            "Epoch:   4 | Train loss: 1.925 | Val loss: 1.917 | Gen: eay inay ingingtingtingtingri isay oooooooooooooooooooo\n",
            "Epoch:   5 | Train loss: 1.838 | Val loss: 1.854 | Gen: eay ingaray ingingongtingay isay oooooooooooooooooooo\n",
            "Epoch:   6 | Train loss: 1.766 | Val loss: 1.793 | Gen: eay ingaray oingingongay isay ongray\n",
            "Epoch:   7 | Train loss: 1.704 | Val loss: 1.741 | Gen: engay aray ongingingongongway isay ongray\n",
            "Epoch:   8 | Train loss: 1.651 | Val loss: 1.697 | Gen: engay aray ongingongongongongwa isway ongray\n",
            "Epoch:   9 | Train loss: 1.603 | Val loss: 1.660 | Gen: engay aray ongingongongongongwa isway ongray\n",
            "Epoch:  10 | Train loss: 1.560 | Val loss: 1.628 | Gen: ethay aray ongongongongongongon isway ongrkay\n",
            "Epoch:  11 | Train loss: 1.521 | Val loss: 1.601 | Gen: ehay aray ongongongongongongon isway ongrkay\n",
            "Epoch:  12 | Train loss: 1.486 | Val loss: 1.580 | Gen: ehay aray ongongongongongongon isway ongrkay\n",
            "Epoch:  13 | Train loss: 1.454 | Val loss: 1.561 | Gen: ehay aray ongongongongongongon isway ongrkay\n",
            "Epoch:  14 | Train loss: 1.425 | Val loss: 1.544 | Gen: ehay aray ongongongongongongon isway ongrkay\n",
            "Epoch:  15 | Train loss: 1.399 | Val loss: 1.526 | Gen: ehay aray ongongongongongongon isway ongrkay\n",
            "Epoch:  16 | Train loss: 1.375 | Val loss: 1.511 | Gen: ehay aray ongongongongongongon isway ongrkay\n",
            "Epoch:  17 | Train loss: 1.352 | Val loss: 1.495 | Gen: ehay aray ongongongongongongon isway ongrkay\n",
            "Epoch:  18 | Train loss: 1.331 | Val loss: 1.481 | Gen: ehthay aray ongongongongongongon isway ongrkay\n",
            "Epoch:  19 | Train loss: 1.311 | Val loss: 1.468 | Gen: ehthay aray ongongongongongongon isway igringray\n",
            "Epoch:  20 | Train loss: 1.292 | Val loss: 1.457 | Gen: ehthay aray ongongongongongongon isway igringray\n",
            "Epoch:  21 | Train loss: 1.274 | Val loss: 1.445 | Gen: ehthay aray ongongongongongongon isway igringray\n",
            "Epoch:  22 | Train loss: 1.257 | Val loss: 1.435 | Gen: ehthay aray ongongongongongongon isway igringray\n",
            "Epoch:  23 | Train loss: 1.241 | Val loss: 1.426 | Gen: ehthay aray ongongongongongongon isway igringray\n",
            "Epoch:  24 | Train loss: 1.226 | Val loss: 1.416 | Gen: ehthay aray onciongongongongongo isway igringray\n",
            "Epoch:  25 | Train loss: 1.211 | Val loss: 1.407 | Gen: ehthay aray onciongongongongongo isway igray\n",
            "Epoch:  26 | Train loss: 1.196 | Val loss: 1.397 | Gen: ehthay aray onciongongongongongo isway igray\n",
            "Epoch:  27 | Train loss: 1.182 | Val loss: 1.386 | Gen: ehthay aray onciongongongongongo isway igray\n",
            "Epoch:  28 | Train loss: 1.168 | Val loss: 1.375 | Gen: ehthay aray ingongongongongongon isway igray\n",
            "Epoch:  29 | Train loss: 1.155 | Val loss: 1.365 | Gen: ehthay aray ingongongongongongay isway igray\n",
            "Epoch:  30 | Train loss: 1.142 | Val loss: 1.354 | Gen: ehthay aray ingongongongondway isway igray\n",
            "Epoch:  31 | Train loss: 1.130 | Val loss: 1.346 | Gen: ehthay aray ingongongondway isway igray\n",
            "Epoch:  32 | Train loss: 1.118 | Val loss: 1.337 | Gen: ehthay aray ingongonday isway igray\n",
            "Epoch:  33 | Train loss: 1.107 | Val loss: 1.328 | Gen: ehthay aray ingongonday isway igray\n",
            "Epoch:  34 | Train loss: 1.096 | Val loss: 1.321 | Gen: ehthay aray ingonday isway igray\n",
            "Epoch:  35 | Train loss: 1.086 | Val loss: 1.313 | Gen: ehthay aray ingonday isway igray\n",
            "Epoch:  36 | Train loss: 1.076 | Val loss: 1.306 | Gen: ehthay aray ingonday isway igray\n",
            "Epoch:  37 | Train loss: 1.066 | Val loss: 1.299 | Gen: ehthay aray ingonday isway igray\n",
            "Epoch:  38 | Train loss: 1.057 | Val loss: 1.293 | Gen: ehthay aray ingonday isway igray\n",
            "Epoch:  39 | Train loss: 1.048 | Val loss: 1.287 | Gen: ehthay aray ingonday isway igray\n",
            "Epoch:  40 | Train loss: 1.040 | Val loss: 1.281 | Gen: ehthay aray ingonday isway igray\n",
            "Epoch:  41 | Train loss: 1.032 | Val loss: 1.276 | Gen: ehthay aray ingonday isway igray\n",
            "Epoch:  42 | Train loss: 1.024 | Val loss: 1.271 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  43 | Train loss: 1.017 | Val loss: 1.266 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  44 | Train loss: 1.010 | Val loss: 1.261 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  45 | Train loss: 1.003 | Val loss: 1.257 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  46 | Train loss: 0.996 | Val loss: 1.254 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  47 | Train loss: 0.990 | Val loss: 1.251 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  48 | Train loss: 0.983 | Val loss: 1.247 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  49 | Train loss: 0.977 | Val loss: 1.243 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  50 | Train loss: 0.972 | Val loss: 1.240 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  51 | Train loss: 0.966 | Val loss: 1.237 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  52 | Train loss: 0.961 | Val loss: 1.234 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  53 | Train loss: 0.955 | Val loss: 1.231 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  54 | Train loss: 0.950 | Val loss: 1.227 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  55 | Train loss: 0.945 | Val loss: 1.224 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  56 | Train loss: 0.940 | Val loss: 1.222 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  57 | Train loss: 0.935 | Val loss: 1.220 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  58 | Train loss: 0.931 | Val loss: 1.216 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  59 | Train loss: 0.926 | Val loss: 1.215 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  60 | Train loss: 0.922 | Val loss: 1.212 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  61 | Train loss: 0.918 | Val loss: 1.210 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  62 | Train loss: 0.913 | Val loss: 1.207 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  63 | Train loss: 0.909 | Val loss: 1.205 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  64 | Train loss: 0.905 | Val loss: 1.203 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  65 | Train loss: 0.901 | Val loss: 1.201 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  66 | Train loss: 0.897 | Val loss: 1.198 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  67 | Train loss: 0.894 | Val loss: 1.196 | Gen: ethay aray oncinday isway igray\n",
            "Epoch:  68 | Train loss: 0.890 | Val loss: 1.194 | Gen: ehthay aray oncinday isway igray\n",
            "Epoch:  69 | Train loss: 0.886 | Val loss: 1.192 | Gen: ethay aray oncinday isway igray\n",
            "Epoch:  70 | Train loss: 0.883 | Val loss: 1.191 | Gen: ethay aray oncinday isway igray\n",
            "Epoch:  71 | Train loss: 0.880 | Val loss: 1.188 | Gen: ethay aray oncinday isway igray\n",
            "Epoch:  72 | Train loss: 0.876 | Val loss: 1.186 | Gen: ethay aray oncintiongonday isway igray\n",
            "Epoch:  73 | Train loss: 0.873 | Val loss: 1.184 | Gen: ethay aray oncintiongonday isway igray\n",
            "Epoch:  74 | Train loss: 0.870 | Val loss: 1.183 | Gen: ethay aray oncintiongongonay isway igray\n",
            "Epoch:  75 | Train loss: 0.866 | Val loss: 1.182 | Gen: ethay aray oncintiongonday isway igray\n",
            "Epoch:  76 | Train loss: 0.863 | Val loss: 1.180 | Gen: ethay aray oncintiongonday isway igray\n",
            "Epoch:  77 | Train loss: 0.860 | Val loss: 1.178 | Gen: ethay aray oncintiongongonay isway igray\n",
            "Epoch:  78 | Train loss: 0.857 | Val loss: 1.177 | Gen: ethay aray oncintiongonday isway igray\n",
            "Epoch:  79 | Train loss: 0.854 | Val loss: 1.175 | Gen: ethay aray oncintiongonday isway igray\n",
            "Epoch:  80 | Train loss: 0.851 | Val loss: 1.174 | Gen: ethay aray oncintiongonday isway orkingway\n",
            "Epoch:  81 | Train loss: 0.849 | Val loss: 1.173 | Gen: ethay aray oncintiongonday isway orkingway\n",
            "Epoch:  82 | Train loss: 0.846 | Val loss: 1.171 | Gen: ethay aray oncintiongonday isway orkingway\n",
            "Epoch:  83 | Train loss: 0.843 | Val loss: 1.170 | Gen: ethay aray oncintiongonday isway orkingway\n",
            "Epoch:  84 | Train loss: 0.840 | Val loss: 1.168 | Gen: ethay aray oncintiongonday isway orkingway\n",
            "Epoch:  85 | Train loss: 0.838 | Val loss: 1.167 | Gen: ethay aray oncintiongonday isway orkingway\n",
            "Epoch:  86 | Train loss: 0.835 | Val loss: 1.166 | Gen: ethay aray oncintiongonday isway orkingway\n",
            "Epoch:  87 | Train loss: 0.832 | Val loss: 1.164 | Gen: ethay aray oncintiongonday isway orkingway\n",
            "Epoch:  88 | Train loss: 0.830 | Val loss: 1.163 | Gen: ethay aray oncintiongonday isway orkingway\n",
            "Epoch:  89 | Train loss: 0.828 | Val loss: 1.162 | Gen: ethay aray oncintiongonday isway orkingway\n",
            "Epoch:  90 | Train loss: 0.825 | Val loss: 1.161 | Gen: ethay aray oncintiongonday isway orkingway\n",
            "Epoch:  91 | Train loss: 0.823 | Val loss: 1.159 | Gen: ethay aray oncintiongonday isway orkingway\n",
            "Epoch:  92 | Train loss: 0.821 | Val loss: 1.159 | Gen: ehthay aray oncintiongonday isway orkingway\n",
            "Epoch:  93 | Train loss: 0.818 | Val loss: 1.157 | Gen: ehthay aray oncintiongonday isway orkingway\n",
            "Epoch:  94 | Train loss: 0.816 | Val loss: 1.157 | Gen: ehthay aray oncintiongonday isway orkingway\n",
            "Epoch:  95 | Train loss: 0.814 | Val loss: 1.155 | Gen: ehthay aray oncintiongonday isway orkingway\n",
            "Epoch:  96 | Train loss: 0.812 | Val loss: 1.154 | Gen: ehthay aray oncintiongonday isway orkingway\n",
            "Epoch:  97 | Train loss: 0.809 | Val loss: 1.154 | Gen: ehthay aray oncintiongonday isway orkingway\n",
            "Epoch:  98 | Train loss: 0.807 | Val loss: 1.152 | Gen: ehthay aray oncintiongonday isway orkingway\n",
            "Epoch:  99 | Train loss: 0.805 | Val loss: 1.152 | Gen: ehthay aray oncintiongonday isway orkingway\n",
            "Obtained lowest validation loss of: 1.1520590055279616\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tehthay aray oncintiongonday isway orkingway\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tcpUFKqo2Oi"
      },
      "source": [
        "## Step 8: Transformer encoder and decoder\n",
        "\n",
        "The following cells provide an implementation of the transformer encoder and decoder that use your `ScaledDotAttention` and `CausalScaledDotAttention`. Please read through them to understand what they are doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3B-fWsarlVk"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.self_attentions = nn.ModuleList(\n",
        "            [\n",
        "                ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.attention_mlps = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(hidden_size, hidden_size),\n",
        "                    nn.ReLU(),\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "            None: Used to conform to standard encoder return signature.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = inputs.size()\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
        "        encoded = encoded + self.positional_encodings[:seq_len]\n",
        "\n",
        "        annotations = encoded\n",
        "        for i in range(self.num_layers):\n",
        "            new_annotations, self_attention_weights = self.self_attentions[i](\n",
        "                annotations, annotations, annotations\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_annotations = annotations + new_annotations\n",
        "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
        "            annotations = residual_annotations + new_annotations\n",
        "\n",
        "        # Transformer encoder does not have a last hidden or cell layer.\n",
        "        return annotations, None\n",
        "        # return annotations, None, None\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
        "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        if self.opts.cuda:\n",
        "            pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyvTZFxtrvc6"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.self_attentions = nn.ModuleList(\n",
        "            [\n",
        "                CausalScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.encoder_attentions = nn.ModuleList(\n",
        "            [\n",
        "                ScaledDotAttention(\n",
        "                    hidden_size=hidden_size,\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.attention_mlps = nn.ModuleList(\n",
        "            [\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(hidden_size, hidden_size),\n",
        "                    nn.ReLU(),\n",
        "                )\n",
        "                for i in range(self.num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.positional_encodings = self.create_positional_encodings()\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: Not used in the transformer decoder\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "\n",
        "        embed = embed + self.positional_encodings[:seq_len]\n",
        "\n",
        "        encoder_attention_weights_list = []\n",
        "        self_attention_weights_list = []\n",
        "        contexts = embed\n",
        "        for i in range(self.num_layers):\n",
        "            new_contexts, self_attention_weights = self.self_attentions[i](\n",
        "                contexts, contexts, contexts\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = contexts + new_contexts\n",
        "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](\n",
        "                residual_contexts, annotations, annotations\n",
        "            )  # batch_size x seq_len x hidden_size\n",
        "            residual_contexts = residual_contexts + new_contexts\n",
        "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
        "            contexts = residual_contexts + new_contexts\n",
        "\n",
        "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
        "            self_attention_weights_list.append(self_attention_weights)\n",
        "\n",
        "        output = self.out(contexts)\n",
        "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
        "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
        "\n",
        "        return output, (encoder_attention_weights, self_attention_weights)\n",
        "\n",
        "    def create_positional_encodings(self, max_seq_len=1000):\n",
        "        \"\"\"Creates positional encodings for the inputs.\n",
        "\n",
        "        Arguments:\n",
        "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
        "\n",
        "        Returns:\n",
        "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
        "        \"\"\"\n",
        "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
        "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
        "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
        "        trig_args = pos_indices / (10000**exponents)\n",
        "        sin_terms = torch.sin(trig_args)\n",
        "        cos_terms = torch.cos(trig_args)\n",
        "\n",
        "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
        "        pos_encodings[:, 0::2] = sin_terms\n",
        "        pos_encodings[:, 1::2] = cos_terms\n",
        "\n",
        "        pos_encodings = pos_encodings.cuda()\n",
        "\n",
        "        return pos_encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ZjkXTNrUKb"
      },
      "source": [
        "\n",
        "## Step 9: Training and analysis (with scaled dot-product attention)\n",
        "\n",
        "Now we will train a (simplified) transformer encoder-decoder model.\n",
        "\n",
        "First, we train our smaller model on the small dataset. Use this model to answer Question 4 in the handout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk8e4KSnuZ8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e0796e8-a432-4368-f4f3-7643c0265530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 100                                    \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 4                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('injure', 'injureway')\n",
            "('revenge', 'evengeray')\n",
            "('resist', 'esistray')\n",
            "('robert', 'obertray')\n",
            "('sedateness', 'edatenesssay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.771 | Val loss: 2.176 | Gen: ay ay iiiiiay-ay isisisisisisisway iay\n",
            "Epoch:   1 | Train loss: 1.774 | Val loss: 1.777 | Gen: ehhhhhhhhhhhhhhhhhhh iiway indiiiindaiiiiiiiiii iiiiiiiiiiiiiiiiiiii inday-ioray\n",
            "Epoch:   2 | Train loss: 1.498 | Val loss: 1.764 | Gen: eay iay intincionciontinting isway oingway\n",
            "Epoch:   3 | Train loss: 1.340 | Val loss: 1.534 | Gen: ehay arway ongay-ionay isway ongay\n",
            "Epoch:   4 | Train loss: 1.188 | Val loss: 1.496 | Gen: etay ay oningiongsay isay ongay\n",
            "Epoch:   5 | Train loss: 1.069 | Val loss: 1.412 | Gen: exthay ay ongingionay isway oringway\n",
            "Epoch:   6 | Train loss: 0.985 | Val loss: 1.278 | Gen: eathay away ondiongiongionay iway oringway\n",
            "Epoch:   7 | Train loss: 0.883 | Val loss: 1.477 | Gen: eatay awawawaway ondionay-onay iswawway orway-ingway\n",
            "Epoch:   8 | Train loss: 0.771 | Val loss: 1.148 | Gen: etay arway ondingway-oniontiong isway oringway\n",
            "Epoch:   9 | Train loss: 0.654 | Val loss: 1.144 | Gen: eay away oooningway isway orway\n",
            "Epoch:  10 | Train loss: 0.575 | Val loss: 1.214 | Gen: etay arway ondiongay isway orkingway\n",
            "Epoch:  11 | Train loss: 0.549 | Val loss: 1.128 | Gen: ethay arway odingionay isway orkingway\n",
            "Epoch:  12 | Train loss: 0.500 | Val loss: 1.074 | Gen: etay away oninigiay isway orkingway\n",
            "Epoch:  13 | Train loss: 0.479 | Val loss: 1.283 | Gen: ethay away ooniintionay isway orkingway\n",
            "Epoch:  14 | Train loss: 0.456 | Val loss: 1.170 | Gen: etay away onditionay isway orkingway\n",
            "Epoch:  15 | Train loss: 0.384 | Val loss: 1.032 | Gen: etay arway onditionay isway orkingway\n",
            "Epoch:  16 | Train loss: 0.348 | Val loss: 1.180 | Gen: etay awwwway ondictiongway way orkingway\n",
            "Epoch:  17 | Train loss: 0.359 | Val loss: 0.959 | Gen: ehthay arway onditionay isway orkingway\n",
            "Epoch:  18 | Train loss: 0.329 | Val loss: 0.912 | Gen: ethay away ondintinay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.300 | Val loss: 0.854 | Gen: ethay away onditintionway isway orkingway\n",
            "Epoch:  20 | Train loss: 0.224 | Val loss: 0.765 | Gen: ethay ariway ondictiongway isway orkingway\n",
            "Epoch:  21 | Train loss: 0.169 | Val loss: 0.761 | Gen: ethay away ondidiontionay isway okingway\n",
            "Epoch:  22 | Train loss: 0.139 | Val loss: 0.752 | Gen: ethay arway ondictiongcay isway okkingway\n",
            "Epoch:  23 | Train loss: 0.117 | Val loss: 0.747 | Gen: ethay arway ondidiontiongay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.108 | Val loss: 0.769 | Gen: ethay ariway onditiniongcay isway okkingray\n",
            "Epoch:  25 | Train loss: 0.101 | Val loss: 0.756 | Gen: ethay arway onditioningcay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.095 | Val loss: 0.763 | Gen: ethay arwiway ondiditingcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.108 | Val loss: 0.942 | Gen: ethay ariway ooniditingcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.266 | Val loss: 2.128 | Gen: eththyyyyyyyyyyyyyyy wwwwwwwwwywywywyyywy onditcningcyway sywywywywywyyyyyyyyy orkningndway\n",
            "Epoch:  29 | Train loss: 0.575 | Val loss: 1.095 | Gen: etay away onditiongway iway orkingway\n",
            "Epoch:  30 | Train loss: 0.407 | Val loss: 0.932 | Gen: ehthay awayway ondiontiongcay isway okingway\n",
            "Epoch:  31 | Train loss: 0.198 | Val loss: 0.780 | Gen: ethay araway onditiongcay isay orkingway\n",
            "Epoch:  32 | Train loss: 0.142 | Val loss: 0.707 | Gen: ehthay arway onditiongcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.092 | Val loss: 0.653 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.071 | Val loss: 0.663 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.063 | Val loss: 0.884 | Gen: ethay arway onditcaniongcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.102 | Val loss: 0.805 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.068 | Val loss: 0.668 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.046 | Val loss: 0.697 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.038 | Val loss: 0.698 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.032 | Val loss: 0.726 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.028 | Val loss: 0.727 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.025 | Val loss: 0.755 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.026 | Val loss: 0.743 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.030 | Val loss: 0.793 | Gen: ethay arway onditintiongcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.037 | Val loss: 0.764 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.059 | Val loss: 0.861 | Gen: ethay away onditingcay way orkingway\n",
            "Epoch:  47 | Train loss: 0.065 | Val loss: 0.807 | Gen: ethay arirway onditiniongcay issway orkingway\n",
            "Epoch:  48 | Train loss: 0.105 | Val loss: 1.011 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.145 | Val loss: 0.793 | Gen: ethay away onditintiongcay isway orkingway\n",
            "Epoch:  50 | Train loss: 0.060 | Val loss: 0.749 | Gen: ethay away onditioningcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.040 | Val loss: 0.646 | Gen: ethay arway onditioniongcay isway orkingway\n",
            "Epoch:  52 | Train loss: 0.030 | Val loss: 0.696 | Gen: ethay arway onditioningcay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.038 | Val loss: 0.700 | Gen: ethay ariway onditiongcay isway orkingway\n",
            "Epoch:  54 | Train loss: 0.021 | Val loss: 0.684 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.016 | Val loss: 0.681 | Gen: ethay arway onditingcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.012 | Val loss: 0.698 | Gen: ethay away onditioningcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.010 | Val loss: 0.701 | Gen: ethay arway onditingcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.012 | Val loss: 0.725 | Gen: ethay arway onditioningcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.017 | Val loss: 0.701 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.034 | Val loss: 0.834 | Gen: ethay araway onditionicay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.044 | Val loss: 0.776 | Gen: ethay arway onditioningcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.033 | Val loss: 1.042 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.038 | Val loss: 0.805 | Gen: ethay awarway onditiongcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.033 | Val loss: 0.770 | Gen: ethay arway onditionicay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.027 | Val loss: 0.639 | Gen: ethay airway onditiongcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.015 | Val loss: 0.893 | Gen: eththay iwarway onditiongcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.037 | Val loss: 0.712 | Gen: ethay away onditioningcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.023 | Val loss: 0.704 | Gen: ethay ariaway onditionicay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.010 | Val loss: 0.675 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.011 | Val loss: 0.717 | Gen: ethay airway onditioniongcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.016 | Val loss: 0.708 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.019 | Val loss: 0.713 | Gen: ethay airway onditionicay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.016 | Val loss: 0.737 | Gen: ethay away onditiongcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.012 | Val loss: 0.705 | Gen: ethay arway onditioningcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.007 | Val loss: 0.701 | Gen: ethay arway onditiongcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.006 | Val loss: 0.810 | Gen: ethay arirway onditionicay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.005 | Val loss: 0.695 | Gen: ethay ariway onditiongcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.005 | Val loss: 0.786 | Gen: ethay ariway ondititingcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.005 | Val loss: 0.748 | Gen: ethay arrway onditingcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.006 | Val loss: 0.786 | Gen: ethay awirway onditioniocay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.005 | Val loss: 0.695 | Gen: ethay arirway onditiongcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.005 | Val loss: 0.762 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.002 | Val loss: 0.701 | Gen: ethay arirway onditiongcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.002 | Val loss: 0.784 | Gen: ethay ariway onditionicay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.002 | Val loss: 0.716 | Gen: ethay arirway onditiongcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.002 | Val loss: 0.788 | Gen: ethay ariway onditiongcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.002 | Val loss: 0.718 | Gen: ethay ariway onditionicay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.002 | Val loss: 0.811 | Gen: ethay ariway onditionicay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.002 | Val loss: 0.730 | Gen: ethay ariway onditiongcay isway orkingway\n",
            "Epoch:  90 | Train loss: 0.002 | Val loss: 0.809 | Gen: ethay ariway onditiongcay isway orkingway\n",
            "Epoch:  91 | Train loss: 0.002 | Val loss: 0.744 | Gen: ethay ariway onditiongcay isway orkingway\n",
            "Epoch:  92 | Train loss: 0.002 | Val loss: 0.827 | Gen: ethay awarway onditiongcay isway orkingway\n",
            "Epoch:  93 | Train loss: 0.002 | Val loss: 0.759 | Gen: ethay ariway onditionicay isway orkingway\n",
            "Epoch:  94 | Train loss: 0.002 | Val loss: 0.821 | Gen: ethay ariway onditioniongcay isway orkingway\n",
            "Epoch:  95 | Train loss: 0.001 | Val loss: 0.757 | Gen: ethay ariway onditiongcay isway orkingway\n",
            "Epoch:  96 | Train loss: 0.002 | Val loss: 0.854 | Gen: ethay awayway onditiongcay isway orkingway\n",
            "Epoch:  97 | Train loss: 0.001 | Val loss: 0.769 | Gen: ethay arirway onditiongcay isway orkingway\n",
            "Epoch:  98 | Train loss: 0.002 | Val loss: 0.857 | Gen: ethay ariway onditiongcay isway orkingway\n",
            "Epoch:  99 | Train loss: 0.008 | Val loss: 0.983 | Gen: ethay ariway onditionicay isway orkingway\n",
            "Obtained lowest validation loss of: 0.6385130280310788\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay ariway onditionicay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans32_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 100,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 64,\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 4,\n",
        "}\n",
        "trans32_args_s.update(args_dict)\n",
        "print_opts(trans32_args_s)\n",
        "\n",
        "trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l28mKuZxvaRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87d58a6b-9216-4712-e2ad-42ba2397d7be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay ariway onditionicay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L8EqLYFu48H"
      },
      "source": [
        "In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdZO69DozuUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "113a5c1e-8daa-47f9-d651-463a416ad4ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 10                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 32                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('empirical', 'empiricalway')\n",
            "('screams', 'eamsscray')\n",
            "('fate', 'atefay')\n",
            "('johnson', 'ohnsonjay')\n",
            "('hammond', 'ammondhay')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.875 | Val loss: 2.294 | Gen: ay-ay-ay ay ioay-oay yay oay\n",
            "Epoch:   1 | Train loss: 2.094 | Val loss: 2.031 | Gen: eay-ay iay ioioionay-iay isyyyyay oay-iay\n",
            "Epoch:   2 | Train loss: 1.851 | Val loss: 1.876 | Gen: eay ay onay-ionay-iay isyway oray\n",
            "Epoch:   3 | Train loss: 1.720 | Val loss: 1.780 | Gen: eay ay ononay-ionay isyway oray\n",
            "Epoch:   4 | Train loss: 1.612 | Val loss: 1.737 | Gen: eay ay ontionay isway oray\n",
            "Epoch:   5 | Train loss: 1.532 | Val loss: 1.675 | Gen: eay aaray onaytionaytiay isayway orayway\n",
            "Epoch:   6 | Train loss: 1.446 | Val loss: 1.643 | Gen: eay aaraaway ontingintinayday isay oraray\n",
            "Epoch:   7 | Train loss: 1.380 | Val loss: 1.507 | Gen: etay arisiay ontingintingsay isway oraray\n",
            "Epoch:   8 | Train loss: 1.309 | Val loss: 1.453 | Gen: etay aisisway ontingingingintingpa iway orway\n",
            "Epoch:   9 | Train loss: 1.257 | Val loss: 1.397 | Gen: ethay arisiway ontingingingintingpa isway orway\n",
            "Epoch:  10 | Train loss: 1.192 | Val loss: 1.414 | Gen: ethay arirway ontingingingpay iway orway\n",
            "Epoch:  11 | Train loss: 1.166 | Val loss: 1.331 | Gen: ethhay ariray ontingingingpay isway orway\n",
            "Epoch:  12 | Train loss: 1.115 | Val loss: 1.323 | Gen: ethhay arirway ontingpnaytingpway iway orway\n",
            "Epoch:  13 | Train loss: 1.084 | Val loss: 1.297 | Gen: ethhay ariway ontingintingcay isway orway\n",
            "Epoch:  14 | Train loss: 1.041 | Val loss: 1.272 | Gen: ethhay ariway ontingpay iway orway\n",
            "Epoch:  15 | Train loss: 0.989 | Val loss: 1.276 | Gen: eway arway ontinonatingcay iway oraway\n",
            "Epoch:  16 | Train loss: 0.958 | Val loss: 1.205 | Gen: ethhay ariway onontindinay isway orway\n",
            "Epoch:  17 | Train loss: 0.926 | Val loss: 1.277 | Gen: eway ariway ontinonatinday iway orway\n",
            "Epoch:  18 | Train loss: 0.931 | Val loss: 1.296 | Gen: ewhhay awarway onontincintinday iwaway orwaway\n",
            "Epoch:  19 | Train loss: 0.900 | Val loss: 1.123 | Gen: ethhay arway ontinoncay iway orway\n",
            "Epoch:  20 | Train loss: 0.842 | Val loss: 1.114 | Gen: ewhthay ariway ononitincincay iway owrway\n",
            "Epoch:  21 | Train loss: 0.813 | Val loss: 1.119 | Gen: ewhhhay ariway ontinioindinay iway orway\n",
            "Epoch:  22 | Train loss: 0.799 | Val loss: 1.070 | Gen: ethheway ariway ontinioninday iway orway\n",
            "Epoch:  23 | Train loss: 0.768 | Val loss: 1.110 | Gen: ewhhhay ariway ontioninday iway orway\n",
            "Epoch:  24 | Train loss: 0.762 | Val loss: 1.059 | Gen: ethhay ariway ontiniondinday isway orkingway\n",
            "Epoch:  25 | Train loss: 0.722 | Val loss: 1.040 | Gen: ewheway ariway ononitioninday isway okwringingway\n",
            "Epoch:  26 | Train loss: 0.703 | Val loss: 1.046 | Gen: ewhhhay ariway onctioninday isway orwngingway\n",
            "Epoch:  27 | Train loss: 0.681 | Val loss: 1.073 | Gen: ethheway ariway ononitindinday isway orkingway\n",
            "Epoch:  28 | Train loss: 0.683 | Val loss: 1.010 | Gen: etheway ariway ononitiondionday isway orkingingway\n",
            "Epoch:  29 | Train loss: 0.657 | Val loss: 0.983 | Gen: etheway ariway onditioningcay isway orkingway\n",
            "Epoch:  30 | Train loss: 0.635 | Val loss: 0.962 | Gen: etheway airway onontionday isway okwringway\n",
            "Epoch:  31 | Train loss: 0.633 | Val loss: 1.034 | Gen: etway airway onditininday iway okwringway\n",
            "Epoch:  32 | Train loss: 0.628 | Val loss: 0.971 | Gen: etheway airway onditioningcay isway okkingway\n",
            "Epoch:  33 | Train loss: 0.589 | Val loss: 0.993 | Gen: etheway airay ononitioninday isay orwningingway\n",
            "Epoch:  34 | Train loss: 0.570 | Val loss: 0.892 | Gen: ethway airway onditioningcay isway okkingway\n",
            "Epoch:  35 | Train loss: 0.543 | Val loss: 0.931 | Gen: etway iariway ononitioningcay isway okinggway\n",
            "Epoch:  36 | Train loss: 0.544 | Val loss: 0.882 | Gen: etway airway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.525 | Val loss: 0.899 | Gen: etway airway ononitiongcay isway okinggway\n",
            "Epoch:  38 | Train loss: 0.513 | Val loss: 0.858 | Gen: etway iwaray onditioningcay isway okingway\n",
            "Epoch:  39 | Train loss: 0.498 | Val loss: 0.849 | Gen: etway airway onditioningcay isway okkingway\n",
            "Epoch:  40 | Train loss: 0.487 | Val loss: 0.859 | Gen: ethay iway onditioningcay isway okingway\n",
            "Epoch:  41 | Train loss: 0.479 | Val loss: 0.826 | Gen: etway airway onditioningcay isway okkingway\n",
            "Epoch:  42 | Train loss: 0.465 | Val loss: 0.868 | Gen: ethay airway onditioningcay isway okingway\n",
            "Epoch:  43 | Train loss: 0.454 | Val loss: 0.830 | Gen: etway airway onditinonigcay isway okkingway\n",
            "Epoch:  44 | Train loss: 0.443 | Val loss: 0.827 | Gen: ethay airway ondiitioningcay isway okingway\n",
            "Epoch:  45 | Train loss: 0.429 | Val loss: 0.817 | Gen: ethay airway onditioningcay isway okingway\n",
            "Epoch:  46 | Train loss: 0.419 | Val loss: 0.805 | Gen: ethay airway onditioningcay isway okingway\n",
            "Epoch:  47 | Train loss: 0.408 | Val loss: 0.814 | Gen: ethay airway onditioningcay isway okingway\n",
            "Epoch:  48 | Train loss: 0.406 | Val loss: 0.787 | Gen: ethay airway onditioningcay isway okingwnay\n",
            "Epoch:  49 | Train loss: 0.390 | Val loss: 0.808 | Gen: ethay airway onditioningcay isway okingway\n",
            "Epoch:  50 | Train loss: 0.386 | Val loss: 0.792 | Gen: ethay airway ondiitioningcay isway orkingway\n",
            "Epoch:  51 | Train loss: 0.380 | Val loss: 0.821 | Gen: ethay airway onditingcantincway isway okringway\n",
            "Epoch:  52 | Train loss: 0.385 | Val loss: 0.832 | Gen: etheway airway onditioningcay isway orkingway\n",
            "Epoch:  53 | Train loss: 0.379 | Val loss: 0.780 | Gen: ethay airway onditioningcay isway okingwnay\n",
            "Epoch:  54 | Train loss: 0.364 | Val loss: 0.752 | Gen: etheway airway onditiningcay isway orkingway\n",
            "Epoch:  55 | Train loss: 0.342 | Val loss: 0.779 | Gen: ethay ariway onditiningcay isway orkingway\n",
            "Epoch:  56 | Train loss: 0.341 | Val loss: 0.737 | Gen: etheway airway onditioningcay isway orkingway\n",
            "Epoch:  57 | Train loss: 0.328 | Val loss: 0.775 | Gen: ethay airway onditiningcay isway orkingway\n",
            "Epoch:  58 | Train loss: 0.328 | Val loss: 0.717 | Gen: ethtay ariway ondiitioningcay isway orkingway\n",
            "Epoch:  59 | Train loss: 0.328 | Val loss: 0.741 | Gen: etheway airway onditiningcay isway orkingway\n",
            "Epoch:  60 | Train loss: 0.322 | Val loss: 0.724 | Gen: etheway ariway onditioningcay isway orkingway\n",
            "Epoch:  61 | Train loss: 0.296 | Val loss: 0.726 | Gen: etheway ariway onditioningcay isway orkingway\n",
            "Epoch:  62 | Train loss: 0.289 | Val loss: 0.716 | Gen: etheway ariway onditioningcay isway orkingway\n",
            "Epoch:  63 | Train loss: 0.282 | Val loss: 0.710 | Gen: etheway ariway onditioningcay isway orkingway\n",
            "Epoch:  64 | Train loss: 0.275 | Val loss: 0.714 | Gen: etheway ariway onditiningcay isway orkingway\n",
            "Epoch:  65 | Train loss: 0.268 | Val loss: 0.715 | Gen: etheway ariway onditioningcay isway orkingway\n",
            "Epoch:  66 | Train loss: 0.262 | Val loss: 0.700 | Gen: etheway ariway onditioningcay isway orkingway\n",
            "Epoch:  67 | Train loss: 0.255 | Val loss: 0.694 | Gen: etheway ariway onditioningcay isway orkingway\n",
            "Epoch:  68 | Train loss: 0.249 | Val loss: 0.701 | Gen: etheway ariway onditioningcay isway orkingway\n",
            "Epoch:  69 | Train loss: 0.246 | Val loss: 0.689 | Gen: etheway ariway onditioningcay isway orkingway\n",
            "Epoch:  70 | Train loss: 0.240 | Val loss: 0.690 | Gen: etheway ariway onditioningcay isway orkingway\n",
            "Epoch:  71 | Train loss: 0.236 | Val loss: 0.678 | Gen: etheway ariway onditioningcay isway orkingway\n",
            "Epoch:  72 | Train loss: 0.232 | Val loss: 0.706 | Gen: etheway airway onditioningcay isway orkingway\n",
            "Epoch:  73 | Train loss: 0.233 | Val loss: 0.713 | Gen: etheway ariway onditioningcay isway orkingway\n",
            "Epoch:  74 | Train loss: 0.254 | Val loss: 0.794 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  75 | Train loss: 0.292 | Val loss: 0.788 | Gen: ethetay ariway onditioningcay isway orkingway\n",
            "Epoch:  76 | Train loss: 0.306 | Val loss: 0.770 | Gen: ethay arway onditioningcay isway orkingway\n",
            "Epoch:  77 | Train loss: 0.298 | Val loss: 0.662 | Gen: ethetay arway onditioningcay isway orkingway\n",
            "Epoch:  78 | Train loss: 0.291 | Val loss: 0.677 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  79 | Train loss: 0.252 | Val loss: 0.649 | Gen: ethetay arway onditioningcay isway orkingway\n",
            "Epoch:  80 | Train loss: 0.222 | Val loss: 0.657 | Gen: ethetay arway onditioningcay isway orkingway\n",
            "Epoch:  81 | Train loss: 0.214 | Val loss: 0.666 | Gen: ethetay arway onditioningcay isway orkingway\n",
            "Epoch:  82 | Train loss: 0.210 | Val loss: 0.668 | Gen: ethetay ariway onditioningcay isway orkingway\n",
            "Epoch:  83 | Train loss: 0.205 | Val loss: 0.665 | Gen: ethetay ariway onditioningcay isway orkingway\n",
            "Epoch:  84 | Train loss: 0.202 | Val loss: 0.667 | Gen: ethetay ariway onditioningcay isway orkingway\n",
            "Epoch:  85 | Train loss: 0.198 | Val loss: 0.660 | Gen: ethetay ariway onditioningcay isway orkingway\n",
            "Epoch:  86 | Train loss: 0.196 | Val loss: 0.683 | Gen: ethetay ariway onditioningcay isway orkingway\n",
            "Epoch:  87 | Train loss: 0.195 | Val loss: 0.674 | Gen: ethetay ariway onditioningcay isway orkingway\n",
            "Epoch:  88 | Train loss: 0.201 | Val loss: 0.655 | Gen: ethetay ariway onditioningcay isway orkingway\n",
            "Epoch:  89 | Train loss: 0.199 | Val loss: 0.670 | Gen: ethetay ariway onditioningcay isway orkingway\n",
            "Validation loss has not improved in 10 epochs, stopping early\n",
            "Obtained lowest validation loss of: 0.6486733970688838\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethetay ariway onditioningcay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans32_args_l = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 100,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 10,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 512,\n",
        "    \"hidden_size\": 32,\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans32_args_l.update(args_dict)\n",
        "print_opts(trans32_args_l)\n",
        "\n",
        "trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmoTgrDcr_dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e5014cb-0287-43b6-a3d4-1ee5a9d302cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_small                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('injure', 'injureway')\n",
            "('revenge', 'evengeray')\n",
            "('resist', 'esistray')\n",
            "('robert', 'obertray')\n",
            "('sedateness', 'edatenesssay')\n",
            "Num unique word pairs: 3198\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.676 | Val loss: 1.995 | Gen: ay ay ingingiiiintintinay iiiiway ingay\n",
            "Epoch:   1 | Train loss: 1.748 | Val loss: 1.648 | Gen: etway away ongngingay iway ongngay\n",
            "Epoch:   2 | Train loss: 1.444 | Val loss: 1.472 | Gen: ethay away ongingingay isway onggay\n",
            "Epoch:   3 | Train loss: 1.236 | Val loss: 1.512 | Gen: ehay arway onicatay-ongay isway onggay\n",
            "Epoch:   4 | Train loss: 1.100 | Val loss: 1.308 | Gen: ethay arway ontingay iway orway\n",
            "Epoch:   5 | Train loss: 0.971 | Val loss: 1.267 | Gen: etay arway ontingay isway orway-ingray\n",
            "Epoch:   6 | Train loss: 0.885 | Val loss: 1.190 | Gen: etway iway onitingingay isway oriway-ingway\n",
            "Epoch:   7 | Train loss: 0.756 | Val loss: 1.205 | Gen: etway away oticingay iway owwagway\n",
            "Epoch:   8 | Train loss: 0.686 | Val loss: 1.105 | Gen: etway arway ondcay-oninay isway orway\n",
            "Epoch:   9 | Train loss: 0.592 | Val loss: 1.157 | Gen: etway away ondcingay isway owway\n",
            "Epoch:  10 | Train loss: 0.536 | Val loss: 1.229 | Gen: etway arway ondcay-ongay way owway\n",
            "Epoch:  11 | Train loss: 0.493 | Val loss: 1.009 | Gen: ehay arway ondcay isway okway\n",
            "Epoch:  12 | Train loss: 0.436 | Val loss: 0.867 | Gen: ehay ariway ondcintingay isway okwngway\n",
            "Epoch:  13 | Train loss: 0.381 | Val loss: 0.968 | Gen: ethay ariway ondcatingingway isway orway\n",
            "Epoch:  14 | Train loss: 0.339 | Val loss: 0.982 | Gen: ethay airway ondcayingingay isway owway\n",
            "Epoch:  15 | Train loss: 0.312 | Val loss: 1.066 | Gen: ethay airway ooonitiongingay isway ooway\n",
            "Epoch:  16 | Train loss: 0.331 | Val loss: 0.879 | Gen: ethay away onditiongway iay orway\n",
            "Epoch:  17 | Train loss: 0.333 | Val loss: 1.104 | Gen: ethay arway ondittingay isay orway\n",
            "Epoch:  18 | Train loss: 0.331 | Val loss: 0.798 | Gen: ethay ariway cncidingingay isway orkingray\n",
            "Epoch:  19 | Train loss: 0.297 | Val loss: 0.714 | Gen: ethay arway onditiongingday isay orkingggway\n",
            "Epoch:  20 | Train loss: 0.209 | Val loss: 0.597 | Gen: ethay ariway onditiongingcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.175 | Val loss: 0.610 | Gen: ethay arway onditioningcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.155 | Val loss: 0.658 | Gen: ethay arirway onditiongingcay isway oringway\n",
            "Epoch:  23 | Train loss: 0.137 | Val loss: 0.562 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.102 | Val loss: 0.527 | Gen: ethay ariway onditiongingcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.087 | Val loss: 0.518 | Gen: ethay ariway onditionginay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.078 | Val loss: 0.577 | Gen: ethay ariway onditiongingcay isway orkingway\n",
            "Epoch:  27 | Train loss: 0.083 | Val loss: 0.709 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.107 | Val loss: 0.581 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  29 | Train loss: 0.204 | Val loss: 0.926 | Gen: ethay airway ondcitiniongcay isway owwngway\n",
            "Epoch:  30 | Train loss: 0.227 | Val loss: 0.668 | Gen: ehay ariway onditioningcay isway orkingway\n",
            "Epoch:  31 | Train loss: 0.124 | Val loss: 0.544 | Gen: ethay ariway onditiongingcay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.073 | Val loss: 0.499 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.053 | Val loss: 0.484 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  34 | Train loss: 0.042 | Val loss: 0.530 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.037 | Val loss: 0.486 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.033 | Val loss: 0.566 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  37 | Train loss: 0.036 | Val loss: 0.507 | Gen: ethay ariway onditiongingcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.072 | Val loss: 0.546 | Gen: ethay arway onditiningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.050 | Val loss: 0.496 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.033 | Val loss: 0.602 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.029 | Val loss: 0.481 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.025 | Val loss: 0.672 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.040 | Val loss: 0.516 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.033 | Val loss: 0.504 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.030 | Val loss: 0.573 | Gen: ethay ariway onditiongcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.054 | Val loss: 0.504 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.038 | Val loss: 0.476 | Gen: ethay ariway onditioningcay isway okwakngway\n",
            "Epoch:  48 | Train loss: 0.029 | Val loss: 0.569 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.020 | Val loss: 0.461 | Gen: ethay ariway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.46060669720144487\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay ariway onditioningcay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans64_args_s = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_small\",\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 64,\n",
        "    \"hidden_size\": 64,  # Increased model size\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans64_args_s.update(args_dict)\n",
        "print_opts(trans64_args_s)\n",
        "\n",
        "trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dardK4RWvUWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb99c63d-96c6-4925-9b91-35b8395c7532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                         data_file_name: pig_latin_large                        \n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 50                                     \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.0005                                 \n",
            "                early_stopping_patience: 20                                     \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 512                                    \n",
            "                            hidden_size: 64                                     \n",
            "                           encoder_type: transformer                            \n",
            "                           decoder_type: transformer                            \n",
            "                 num_transformer_layers: 3                                      \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('empirical', 'empiricalway')\n",
            "('screams', 'eamsscray')\n",
            "('fate', 'atefay')\n",
            "('johnson', 'ohnsonjay')\n",
            "('hammond', 'ammondhay')\n",
            "Num unique word pairs: 22402\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.495 | Val loss: 1.989 | Gen: etay-ay-ay-ay iray ointinintintintionay issssay oray\n",
            "Epoch:   1 | Train loss: 1.731 | Val loss: 1.705 | Gen: etay-ay-ay away oiointintintintionay isay orway\n",
            "Epoch:   2 | Train loss: 1.468 | Val loss: 1.539 | Gen: etetetay away oningingngngay isway orgway\n",
            "Epoch:   3 | Train loss: 1.343 | Val loss: 1.613 | Gen: etetetay arway-iray onintingay-ingay iway orway-inway\n",
            "Epoch:   4 | Train loss: 1.236 | Val loss: 1.518 | Gen: ettetay-ay arirawayway ondingatingangay isilisay orlway-ingrgsway\n",
            "Epoch:   5 | Train loss: 1.071 | Val loss: 1.436 | Gen: etetetay araiwawawawawaway oonintiongationgay isway orlway-ingway\n",
            "Epoch:   6 | Train loss: 0.967 | Val loss: 1.173 | Gen: ethay araaaaway oniontiongay isway orgaaay\n",
            "Epoch:   7 | Train loss: 0.852 | Val loss: 1.114 | Gen: etetway araiway ondintingingay isway orkway\n",
            "Epoch:   8 | Train loss: 0.728 | Val loss: 1.192 | Gen: ethtway arway onditiontingway isway orkwangway\n",
            "Epoch:   9 | Train loss: 0.705 | Val loss: 1.029 | Gen: ethay araiway onditintingway isway orkwangway\n",
            "Epoch:  10 | Train loss: 0.610 | Val loss: 0.913 | Gen: ethway arway ondiotiongangway isway orkwangway\n",
            "Epoch:  11 | Train loss: 0.573 | Val loss: 1.152 | Gen: ethay airway oditintingngday isway okingwangway\n",
            "Epoch:  12 | Train loss: 0.591 | Val loss: 1.008 | Gen: ethtway arway onditiontingway isway orkingway\n",
            "Epoch:  13 | Train loss: 0.538 | Val loss: 0.940 | Gen: ethay awiway onditioningway isway okingway\n",
            "Epoch:  14 | Train loss: 0.474 | Val loss: 0.801 | Gen: ethay airway onditioningway isway okingway\n",
            "Epoch:  15 | Train loss: 0.460 | Val loss: 0.786 | Gen: ethay airway onditioningngway isway oway-ingway\n",
            "Epoch:  16 | Train loss: 0.413 | Val loss: 0.785 | Gen: ethay airay onditioniningway isway okingkway\n",
            "Epoch:  17 | Train loss: 0.382 | Val loss: 0.681 | Gen: ethay airway onditioningingway isway orkingway\n",
            "Epoch:  18 | Train loss: 0.320 | Val loss: 0.698 | Gen: ethay airway onditioningscay isway orkingway\n",
            "Epoch:  19 | Train loss: 0.281 | Val loss: 0.621 | Gen: ethay airway onditioningway isway okrkingway\n",
            "Epoch:  20 | Train loss: 0.258 | Val loss: 0.569 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  21 | Train loss: 0.221 | Val loss: 0.524 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  22 | Train loss: 0.190 | Val loss: 0.498 | Gen: ethay airway onditioningway isway orkingway\n",
            "Epoch:  23 | Train loss: 0.168 | Val loss: 0.477 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  24 | Train loss: 0.151 | Val loss: 0.493 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  25 | Train loss: 0.151 | Val loss: 0.598 | Gen: ethay arway onditiontincay isway orkingway\n",
            "Epoch:  26 | Train loss: 0.164 | Val loss: 0.704 | Gen: ethay aaaiway onditininingway isway owkingway\n",
            "Epoch:  27 | Train loss: 0.171 | Val loss: 0.698 | Gen: ethay awwway onditioningnacay isway orkingway\n",
            "Epoch:  28 | Train loss: 0.250 | Val loss: 0.986 | Gen: ethay awawwiway onditiontioncay isway owrkingway\n",
            "Epoch:  29 | Train loss: 0.286 | Val loss: 0.640 | Gen: ehetay airway onditininingway isway orkingway\n",
            "Epoch:  30 | Train loss: 0.167 | Val loss: 0.472 | Gen: ethay airway onditioniongway isway orkingway\n",
            "Epoch:  31 | Train loss: 0.130 | Val loss: 0.412 | Gen: ethay airway onditiontincay isway orkingway\n",
            "Epoch:  32 | Train loss: 0.107 | Val loss: 0.498 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  33 | Train loss: 0.126 | Val loss: 0.678 | Gen: thay arway onditiongincay isay orkingway\n",
            "Epoch:  34 | Train loss: 0.225 | Val loss: 0.402 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  35 | Train loss: 0.103 | Val loss: 0.331 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  36 | Train loss: 0.094 | Val loss: 0.496 | Gen: ethay airway onditiongnay isay orkingway\n",
            "Epoch:  37 | Train loss: 0.084 | Val loss: 0.319 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  38 | Train loss: 0.073 | Val loss: 0.310 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  39 | Train loss: 0.065 | Val loss: 0.295 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  40 | Train loss: 0.049 | Val loss: 0.299 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  41 | Train loss: 0.043 | Val loss: 0.297 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  42 | Train loss: 0.036 | Val loss: 0.302 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  43 | Train loss: 0.032 | Val loss: 0.294 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  44 | Train loss: 0.030 | Val loss: 0.293 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  45 | Train loss: 0.026 | Val loss: 0.289 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  46 | Train loss: 0.023 | Val loss: 0.287 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  47 | Train loss: 0.021 | Val loss: 0.287 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  48 | Train loss: 0.020 | Val loss: 0.287 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Epoch:  49 | Train loss: 0.018 | Val loss: 0.288 | Gen: ethay airway onditioningcay isway orkingway\n",
            "Obtained lowest validation loss of: 0.28719999951620895\n",
            "source:\t\tthe air conditioning is working \n",
            "translated:\tethay airway onditioningcay isway orkingway\n"
          ]
        }
      ],
      "source": [
        "TEST_SENTENCE = \"the air conditioning is working\"\n",
        "\n",
        "trans64_args_l = AttrDict()\n",
        "args_dict = {\n",
        "    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n",
        "    \"cuda\": True,\n",
        "    \"nepochs\": 50,\n",
        "    \"checkpoint_dir\": \"checkpoints\",\n",
        "    \"learning_rate\": 5e-4,\n",
        "    \"early_stopping_patience\": 20,\n",
        "    \"lr_decay\": 0.99,\n",
        "    \"batch_size\": 512,\n",
        "    \"hidden_size\": 64,  # Increased model size\n",
        "    \"encoder_type\": \"transformer\",\n",
        "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
        "    \"num_transformer_layers\": 3,\n",
        "}\n",
        "trans64_args_l.update(args_dict)\n",
        "print_opts(trans64_args_l)\n",
        "\n",
        "trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n",
        "\n",
        "translated = translate_sentence(\n",
        "    TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l\n",
        ")\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSSyiG39vVlN"
      },
      "source": [
        "The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ql0pxrEvVP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "11de00f1-62d9-4234-ce26-b29a236838d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'NoneType'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "save_loss_comparison_by_dataset(\n",
        "    trans32_losses_s,\n",
        "    trans32_losses_l,\n",
        "    trans64_losses_s,\n",
        "    trans64_losses_l,\n",
        "    trans32_args_s,\n",
        "    trans32_args_l,\n",
        "    trans64_args_s,\n",
        "    trans64_args_l,\n",
        "    \"trans_by_dataset\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_loss_comparison_by_hidden(\n",
        "    trans32_losses_s,\n",
        "    trans32_losses_l,\n",
        "    trans64_losses_s,\n",
        "    trans64_losses_l,\n",
        "    trans32_args_s,\n",
        "    trans32_args_l,\n",
        "    trans64_args_s,\n",
        "    trans64_args_l,\n",
        "    \"trans_by_hidden\",\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I2vHh_gVJMuM",
        "outputId": "dc9b77df-8586-45de-f0e7-174c04470695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B3VR1LnjMhk7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TjPTaRB4mpCd"
      ],
      "name": "Copy of nmt.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}